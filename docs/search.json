[
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html",
    "href": "notebooks/05-exploring-hiking-network-options.html",
    "title": "Exploring hiking network options",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-08-07\nInitial post.\nPer the previous notebook we’ll make a small network (lower resolution) so we can explore the possibilities of this graph representation.\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\nbase_folder &lt;-    str_glue(\"{here()}/_data/testing\")\ninput_folder &lt;-   str_glue(\"{base_folder}/input\")\noutput_folder &lt;-  str_glue(\"{base_folder}/output\")\nbasename &lt;-       \"dry-valleys-10m\"\ndem_file &lt;-       str_glue(\"{input_folder}/{basename}.tif\")\nimagery_file &lt;-   str_glue(\"{input_folder}/{basename}-imagery.tif\")\nlandcover_file &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")\nextent_file &lt;-    str_glue(\"{input_folder}/{basename}-extent.gpkg\")\norigins_file &lt;-   str_glue(\"{input_folder}/{basename}-origins.gpkg\")\n\nterrain &lt;- rast(dem_file)\n# and for visualization\nshade &lt;- get_hillshade(terrain)\nimagery &lt;- rast(imagery_file)\nextent &lt;- st_read(extent_file)\n\n# make a focal area for convenience of plotting in some situations\ncentre_of_extent &lt;- extent |&gt; \n  st_centroid() # we use this later...\nc_vec &lt;- centre_of_extent |&gt;\n  st_coordinates() |&gt;\n  as.vector()\nfocal_area &lt;- ((extent$geom[1] - c_vec) * diag(0.025, 2, 2) + c_vec) |&gt;\n  st_sfc(crs = st_crs(extent)) |&gt;\n  st_as_sf() |&gt;\n  rename(geom = x)\n\nresolution &lt;- res(terrain)[1] * 20 \n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- 2 * resolution / sqrt(2 * sqrt(3))\n\nhexgrid_file &lt;- str_glue(\"{input_folder}/{basename}-hex-grid-{resolution}.gpkg\")\ngraph_file &lt;-   str_glue(\"{output_folder}/{basename}-graph-hex-{resolution}.txt\") \n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    st_set_crs(st_crs(extent))\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n# make pts and remove any NAs that might result from interpolation\n# NAs are around the edges of the study area\npts &lt;- xy |&gt;\n  mutate(z = z[, 2]) |&gt;\n  filter(!is.na(z))\n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\ncover &lt;- st_read(landcover_file)\n\nxyz &lt;- extract_xyz_from_points(pts)\n\ncosts &lt;- pts |&gt;\n  st_join(cover, .predicate = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  select(cost)\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz, impedances = costs$cost)",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html#now-we-have-a-graph",
    "href": "notebooks/05-exploring-hiking-network-options.html#now-we-have-a-graph",
    "title": "Exploring hiking network options",
    "section": "Now we have a graph",
    "text": "Now we have a graph\nSo let’s take a look…\n\n# make a hillshade 'basemap'\nhillshade_basemap &lt;- ggplot() +\n  geom_raster(data = shade |&gt; as.data.frame(xy = TRUE), \n              aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  theme_void()\n\nhillshade_basemap +\n  geom_sf(data = G |&gt; get_graph_as_line_layer(), \n          aes(colour = cost), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"Spectral\") +\n  theme_void()\n\n\n\n\n\n\n\n\nAnd just for assurance, here’s the estimated travel time map.\n\norigin_i &lt;- centre_of_extent |&gt;\n  st_nearest_feature(pts)\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), weights = edge_attr(G, \"cost\")) |&gt; t()\n\n# assemble results into a DF\ndf &lt;- data.frame(x = V(G)$x, y = V(G)$y, z = V(G)$z, \n                 time_hrs = V(G)$time_hrs, \n                 cost = costs$cost)\n# write this out to a shapefile, which Whitebox Tools needs\nshp_fname &lt;- str_glue(\"{here()}/_temp/{basename}-{format(origin_i, scientific = FALSE)}-hex.shp\")\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# do the interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\nwbt_natural_neighbour_interpolation(shp_fname, field = \"time_hrs\",\n                                    output = tif_fname, \n                                    base = str_glue(dem_file))\n\nReading the exported raster layer made by whitebox::wbt_natural_neighbour_interpolation back in we get a map of estimated travel times.\n\nrast(tif_fname) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_distiller(palette = \"Spectral\", name = \"Est. time hrs\") +\n  geom_contour(aes(z = travel_time), linewidth = 0.5) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nIt may be worth noting that this map made with a 200m resolution network doesn’t differ greatly from the one at 50m resolution in the previous notebook.",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html#ok-so-what-can-we-do-with-these-things",
    "href": "notebooks/05-exploring-hiking-network-options.html#ok-so-what-can-we-do-with-these-things",
    "title": "Exploring hiking network options",
    "section": "OK… so what can we do with these things?!",
    "text": "OK… so what can we do with these things?!\nThe network is stored as an igraph object G. This admits many different network analysis methods.\n\nMinimum spanning tree\nThis is the shortest total path length (expressed here in the estimated traversal time variable cost) to reach all graph nodes.\n\nMST &lt;- G |&gt; \n  mst(weights = E(G)$cost) |&gt;\n  get_graph_as_line_layer() \n\nWe can see what this looks like\n\nhillshade_basemap + \n  geom_sf(data = MST, linewidth = 0.1)\n\n\n\n\n\n\n\n\nThe hillshade basemap highlights how the total shortest path prioritises paths that follow contours since the lowest cost way in general to get to a particular vertex is from a neighbouring vertex at similar elevation.\n\n\nShortest path tree\nStarting from a given vertex we can make a tree graph which shows all the shortest paths from that site to every other site in the vertex. This is not a built-in igraph function. A function in raster-to-graph-functions.R builds one for us.\n\nSPT &lt;- G |&gt;\n  get_shortest_path_tree(origin_i) |&gt;\n  get_graph_as_line_layer()\n\n\nhillshade_basemap + \n  geom_sf(data = SPT, linewidth = 0.1) +\n  geom_sf(data = pts |&gt; slice(origin_i), pch = 4)\n\n\n\n\n\n\n\n\nThis shows all the potential most efficient pathways out from the root vertex to every other vertex.\n\n\nEdge betweenness\nIt turns out we can count all the appearances of graph edges and/or vertices in all the shortest paths among all the vertices in a network. This are graph centrality measures termed edge betweenness and vertex betweenness (the latter is often simply betweenness).\n\n# scale relative to betweenness if all edges are equal cost to offset edge effects\nbase_eb &lt;- edge_betweenness(G)\ncost_eb &lt;- edge_betweenness(G, weights = E(G)$cost)\ndiff_eb &lt;- (cost_eb - base_eb) / (cost_eb + base_eb)\nrel_eb &lt;- cost_eb / base_eb\nlog_rel_eb &lt;- log(rel_eb, 10)\nGsp &lt;- G |&gt; \n  set_edge_attr(\"base_eb\", value = base_eb) |&gt;\n  set_edge_attr(\"cost_eb\", value = cost_eb) |&gt;\n  set_edge_attr(\"diff_eb\", value = diff_eb) |&gt;\n  set_edge_attr(\"rel_eb\", value = rel_eb) |&gt;\n  set_edge_attr(\"log_rel_eb\", value = log_rel_eb)\n\n# add 1 to these to make it easier to scale them if needed because a raw value 0 is possible\nbase_vb &lt;- betweenness(G) + 1\ncost_vb &lt;- betweenness(G, weights = E(G)$cost) + 1\ndiff_vb &lt;- (cost_vb - base_vb) / (cost_vb + base_vb)\nrel_vb &lt;- cost_vb / base_vb\nlog_rel_vb &lt;- log(rel_vb)\nGsp &lt;- Gsp |&gt; \n  set_vertex_attr(\"base_vb\", value = base_vb) |&gt;\n  set_vertex_attr(\"cost_vb\", value = cost_vb) |&gt;\n  set_vertex_attr(\"diff_vb\", value = diff_vb) |&gt;\n  set_vertex_attr(\"rel_vb\", value = rel_vb) |&gt;\n  set_vertex_attr(\"log_rel_vb\", value = log_rel_vb)\n\nMaps of these…\nFirst vertex centrality, where it is relatively simple to colour vertices by their centrality.\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer() |&gt; filter(log_rel_vb &gt; 0), \n          aes(colour = rel_vb), size = 0.5) +\n  scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\n\nEdge centrality is fiddlier to map (because of how the linewidth aesthetic works).\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt; filter(log_rel_eb &gt; 0) |&gt; mutate(lwd = rel_eb / 100),\n          aes(linewidth = lwd)) +\n  scale_linewidth_identity(breaks = 0:5 * 50, labels = format(0:5 * 50, scientific = FALSE),\n                           guide = \"legend\", name = \"Edge betweenness\")\n\n\n\n\n\n\n\n\nFrom which we deduce… there is a ‘shortcut’ around the eastern (to the left…) end of the northernmost (bottom…) ridge which the shortest path function is finding.\nTODO: Check if this is an appropriate study area…\n\n\nRestricting the length of shortest paths\nIt is possible to restrict shortest paths to a maximum length in terms of the cost attribute used to calculate path lengths, which in our case is hours. Intuitively if the longest hike ‘segments’ undertaken are (say) 3 hours, then restricting shortest paths in this way might make sense. So…\n\nvb &lt;- betweenness(G, weights = E(G)$cost, cutoff = 3) + 1\nGsp &lt;- G |&gt;\n  set_vertex_attr(\"betweenness\", value = vb)\n\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(), aes(colour = betweenness), size = 0.5) +\n  scale_colour_distiller(palette = \"Spectral\", guide = \"none\")\n\n\n\n\n\n\n\n\nThis more local focus identifies more vertices as being on shortest paths",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html",
    "href": "notebooks/06-a-hiking-network-with-better-data.html",
    "title": "A hiking network with better data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-03\nAdded tentative conclusion pointing to need to explore performance when cutoff is introduced to betweennness calculation.\n\n\n2024-09-01\nCleaned up to show the workflow.\n\n\n2024-08-29\nInitial post.\nThe primary purpose of this notebook is to show the overall workflow for making a map of potential impacts from scientists moving around in the Dry Valleys, based on slope, landcover, and hiking function inputs. The workflow is shown with respect to the 5th largest area of contiguous rocky (rocks and/or gravels) cover — an area of about 320 sq.km, at a resolution of 100m.\nMuch of this code is in previous notebooks in similar form, but pointing here to different data resources, and once final inputs (primarily landcover costs and a hiking function) are determined, can be automated to run on several zones across all of the Dry Valleys.\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\nlibrary(ggspatial)\nlibrary(scales)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#set-up-folders-and-some-base-parameters",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#set-up-folders-and-some-base-parameters",
    "title": "A hiking network with better data",
    "section": "Set up folders and some base parameters",
    "text": "Set up folders and some base parameters\nThe set up of data subfolders under the top level _data folder is:\n\n\n\n\n\n\n\nFolder\nContents\n\n\n\n\ndry-valleys/output\nOutputs - primarily the graph and elevation data (as points) with relative movement costs\n\n\ncontiguous-geologies\nGPKG files dry-valleys-extent-??.gpkg with a single contiguous geology polygon per file\n\n\ndry-valleys/common-inputs\nVarious input layers as needed. Primarily relative landcover costs *-cover-costs.gpkg\n\n\ndem/10m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-10m-dem-clipped.tif\n\n\ndem/32m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-32m-dem-clipped.tif\n\n\n\nAdditionally, the basename for all files is dry-valleys, with DEM resolution used (32m or 10m) and the nominal hiking network resolution (without the m), which is likely to be 100 included in various output files as appropriate.\n\ndem_resolution     &lt;- \"32m\" # 10m also available, but probably not critical\nresolution         &lt;- 200   # 100 is high res for rapid iterating: use 200 while testing\nimpassable_cost    &lt;- -999  # in the cover-costs file. hat-tip to ESRI\n\ndata_folder        &lt;- str_glue(\"{here()}/_data\")\nbase_folder        &lt;- str_glue(\"{data_folder}/dry-valleys\")\ninput_folder       &lt;- str_glue(\"{base_folder}/common-inputs\")\noutput_folder      &lt;- str_glue(\"{base_folder}/output\")\ndem_folder         &lt;- str_glue(\"{data_folder}/dem/{dem_resolution}\")\n\nbasename           &lt;- str_glue(\"dry-valleys\")\ndem_file           &lt;- str_glue(\"{dem_folder}/{basename}-combined-{dem_resolution}-dem-clipped.tif\")\nlandcover_file     &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#load-the-terrain-and-crop-to-the-extent",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#load-the-terrain-and-crop-to-the-extent",
    "title": "A hiking network with better data",
    "section": "Load the terrain and crop to the extent",
    "text": "Load the terrain and crop to the extent\nThe extent in each case is a single area of ‘contiguous geology’ derived by dissolving all polygons in the GNS geomap data. These files are named *01.gpkg, *02.gpkg, etc., where the sequence number is from the largest to the smallest by area.\n\ncontiguous_geology &lt;- \"05\"  # small one for testing\nextent_file        &lt;- str_glue(\"{base_folder}/contiguous-geologies/{basename}-extent-{contiguous_geology}.gpkg\")\nextent             &lt;- st_read(extent_file)\n\nterrain &lt;- rast(dem_file) |&gt;\n  crop(extent |&gt; as(\"SpatVector\")) |&gt;\n  mask(extent |&gt; as(\"SpatVector\"))\n# and for visualization\nshade &lt;- get_hillshade(terrain) |&gt;\n  as.data.frame(xy = TRUE) # this is for plotting in ggplot\n\nIt’s handy to make a couple of basemaps here\n\nhillshade_basemap &lt;- ggplot(shade) +\n  geom_raster(data = shade, aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()\n\n# this is a bit goofy, but seems to work\ncontours &lt;- function(terrain) {\n  stat_contour(data = terrain |&gt; as.data.frame(xy = TRUE) |&gt; rename(z = names(terrain)),\n               aes(x = x, y = y, z = z), binwidth = 100, linewidth = 0.2, colour = \"#ddaa44\")\n}\n\nhillshade_basemap + \n  contours(terrain) +\n  coord_equal()",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#make-the-graph",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#make-the-graph",
    "title": "A hiking network with better data",
    "section": "Make the graph",
    "text": "Make the graph\nNodes in the graph are centres of a hexagonal grid across the extent. The cell size of the hexagonal grid is set so that hexagons are the same area as square raster cells of side length given by resolution. The area of these is \\(r^2\\). The area of a hexagon with ‘radius’ \\(R\\) is \\(3\\sqrt{3}R^2/2\\). Cell size in sf::st_make_grid is specified by the ‘face-to-face distance’ of the hexagons, this distance is given by \\(D=\\sqrt{3}R\\), so \\(R=D/\\sqrt{3}\\). Hence \\[\n\\begin{array}{rcrcl}\nA & = & \\frac{3\\sqrt{3}}{2}\\left(\\frac{D}{\\sqrt{3}}\\right)^2 &=& r^2 \\\\\n& & D^2 & = & \\frac{2}{\\sqrt{3}}r^2 \\\\\n& & & = & r\\sqrt{\\frac{2}{\\sqrt{3}}}\n\\end{array}\n\\] So make a hex grid of points with this spacing.\n\n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- resolution * sqrt(2 / sqrt(3))\n\n# check if we have already made this dataset\nhexgrid_file &lt;- \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-hex-grid-{resolution}.gpkg\")\n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    rename(geometry = x) |&gt;   # shouldsee https://github.com/r-spatial/sf/issues/2429\n    st_set_crs(st_crs(extent))\n  coords &lt;- xy |&gt; st_coordinates()\n  xy &lt;- xy |&gt;\n    mutate(x = coords[, 1], y = coords[, 2])\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nReading layer `dry-valleys-05-32m-hex-grid-200' from data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/dry-valleys/output/dry-valleys-05-32m-hex-grid-200.gpkg' \n  using driver `GPKG'\nSimple feature collection with 20777 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 424601.4 ymin: -1262612 xmax: 452755.2 ymax: -1233391\nProjected CRS: WGS 84 / Antarctic Polar Stereographic\n\n\nAttach elevation values to it from the terrain data.\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n\npts &lt;- xy |&gt;\n  mutate(z = z[, 2])\n\nAlso attach land cover relative movement costs, removing any NAs that might have been picked up along the way. These will predominantly be due to minor mismatches between the extent of the raster terrain layer and the coverage of the hexagonal grid of points. We also remove any points whose cost is equal to the sentinel value (-999) for impassable terrain (water and ice).\n\n# remove any NAs that might result from interpolation or impassable terrain\n# NAs tend to occur around the edge of study area\ncover &lt;- st_read(landcover_file)\n\npts &lt;- pts |&gt;\n  st_join(cover) |&gt;\n  filter(!is.na(z), cost != impassable_cost)\n\nMapping it we can see that more difficult terrain is associated with the higher elevations (which are rocky).\n\nhillshade_basemap +\n  geom_sf(data = pts, aes(colour = cost), size = 0.05) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  contours(terrain) +\n  theme_void()\n\n\n\n\n\n\n\n\nNow we make the graph by connecting pairs of points within range of one another (note 1.1 factor to catch floating-point near misses).\n\ngraph_file &lt;-   \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-graph-hex-{resolution}.txt\") \n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz = pts, impedances = pts$cost)",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#now-we-have-a-graph",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#now-we-have-a-graph",
    "title": "A hiking network with better data",
    "section": "Now we have a graph",
    "text": "Now we have a graph\nSo let’s take a look… first, at a map of the network for reassurance - noting that since costs are different in each direction there’s no easy way to show this unambiguously\n\nhillshade_basemap +\n  geom_sf(data = G |&gt; get_graph_as_line_layer(), \n          aes(colour = log(cost)), linewidth = 0.05) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  theme_void()\n\n\n\n\n\n\n\n\nAnd just for assurance, here’s an estimated travel time map. These are actually tricky to make because\n\n# pick a random point as origin point for some examples\norigin_i &lt;- sample(seq_along(pts$geom), 1)[1]\norigin &lt;- c(pts$x[origin_i], pts$y[origin_i])\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), \n                           weights = edge_attr(G, \"cost\")) |&gt; t()\ndf &lt;- vertex.attributes(G) |&gt; as.data.frame()\n\nhillshade_basemap +\n  geom_point(data = df, aes(x = x, y = y, colour = time_hrs), size = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  annotate(\"point\", x = origin[1], y = origin[2], pch = 4) +\n  coord_equal() \n\n\n\n\n\n\n\n\nNote the following, if we want the estimated travel times as a raster layer rather than a point dataset, where we use Sibley’s natural neighbours interpolation method from whitebox_tools to interpolate to a raster from the hex grid distributed points.\n[NOT RUN]\n# filenames for (temporary) shapefile and TIF outputs that we need \nshp_fname &lt;- str_glue(\"{here()}/_temp/test-{format(origin_i, scientific = FALSE)}-hex.shp\")\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\n\n# write the df out to a shapefile, which Whitebox Tools needs\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# do the interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\nwbt_natural_neighbour_interpolation(\n  shp_fname, output = tif_fname, field = \"time_hrs\", base = str_glue(dem_file))\n\nrast(tif_fname) |&gt;\n  mask(extent |&gt; as(\"SpatVector\")) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_viridis_c(option = \"A\", direction = -1, name = \"Est. time hrs\") +\n  coord_equal() +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#shortest-path-tree",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#shortest-path-tree",
    "title": "A hiking network with better data",
    "section": "Shortest path tree",
    "text": "Shortest path tree\nThis is more for fun that useful – although given a set of heavily used sites (base camps?) their shortest path trees might usefully be combined to give a general idea of most at-risk / heavily travelled areas. Note that shortest_path_tree is a function in raster-graph-functions.R not an igraph built-in.\n\nSPT &lt;- G |&gt;\n  get_shortest_path_tree(origin_i) |&gt;\n  get_graph_as_line_layer()\n\n\nggplot() + \n  contours(terrain) +\n  geom_sf(data = SPT, linewidth = 0.1) +\n  geom_sf(data = pts |&gt; slice(origin_i), pch = 4) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows all the potential most efficient pathways out from the root vertex to every other vertex. There is a strong tendency for these paths to follow contours.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#betweenness-centralities",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#betweenness-centralities",
    "title": "A hiking network with better data",
    "section": "Betweenness centralities",
    "text": "Betweenness centralities\nIt turns out we can count all the appearances of graph edges and/or vertices in all the shortest paths among all the vertices in a network. These are graph centrality measures termed edge betweenness and vertex betweenness (the latter is often simply betweenness).\nIf we use these in their ‘raw’ form then the most geographically central spots in a study area may have the highest values by virtue of their location in the middle of the area. This might not be wrong — not exactly anyway — but it might lead to ignoring relatively vulnerable areas that may be heavily travelled in spite of more peripheral locations because they are at bottlenecks between parts of the study area. For this reason we calculate a base- and cost- betweenness scores. The former is the betweenness if the cost of traversing every edge in the graph was equal’ the latter is when slope and terrain are taken into account.\n\n# vertices\n# add 1 to these to make it easier to scale them if needed because a raw value 0 is possible\nbase_vb &lt;- betweenness(G)\ncost_vb &lt;- betweenness(G, weights = E(G)$cost)\nbase_vb &lt;- rescale(base_vb, to = c(1, 100))\ncost_vb &lt;- rescale(cost_vb, to = c(1, 100))\ndiff_vb &lt;- (cost_vb - base_vb) / (cost_vb + base_vb)\nrel_vb &lt;- cost_vb / base_vb\nlog_rel_vb &lt;- log(rel_vb)\n\n\n# edges\nbase_eb &lt;- edge_betweenness(G)\ncost_eb &lt;- edge_betweenness(G, weights = E(G)$cost)\nbase_eb &lt;- rescale(base_eb, to = c(1, 100))\ncost_eb &lt;- rescale(cost_eb, to = c(1, 100))\ndiff_eb &lt;- (cost_eb - base_eb) / (cost_eb + base_eb)\nrel_eb &lt;- cost_eb / base_eb\nlog_rel_eb &lt;- log(rel_eb, 10)\n\nAdd all these results to the relevant graph attributes.\n\nGsp &lt;- G |&gt; \n  set_vertex_attr(\"base_vb\", value = base_vb) |&gt;\n  set_vertex_attr(\"cost_vb\", value = cost_vb) |&gt;\n  set_vertex_attr(\"diff_vb\", value = diff_vb) |&gt;\n  set_vertex_attr(\"rel_vb\", value = rel_vb) |&gt;\n  set_vertex_attr(\"log_rel_vb\", value = log_rel_vb)|&gt; \n  set_edge_attr(\"base_eb\", value = base_eb) |&gt;\n  set_edge_attr(\"cost_eb\", value = cost_eb) |&gt;\n  set_edge_attr(\"diff_eb\", value = diff_eb) |&gt;\n  set_edge_attr(\"rel_eb\", value = rel_eb) |&gt;\n  set_edge_attr(\"log_rel_eb\", value = log_rel_eb)",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#maps-of-betweenness-centrality",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#maps-of-betweenness-centrality",
    "title": "A hiking network with better data",
    "section": "Maps of betweenness centrality",
    "text": "Maps of betweenness centrality\nFirst vertex centrality, where it is relatively simple to colour vertices by their centrality. It is useful to map all the various calculated betweenness measures to get a feel for which may be most useful.\n\nm1 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = base_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm2 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = cost_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm3 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = diff_vb), size = 0.02) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm4 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = rel_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm5 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = log_rel_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n  \nggarrange(plotlist = list(m1, m2, m3, m4, m5), ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nExamining these results, it appears that the the simple cost_vb betweenness may actually be the most useful in revealing routes likely to be well trafficked. The diff_vb output also appears likely to be useful.\nEdge centrality is trickier to map (because of how the linewidth aesthetic works).\n\nm1 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt;\n            mutate(lwd = base_eb / max(E(Gsp)$base_eb)),\n          aes(colour = base_eb, linewidth = lwd)) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  scale_linewidth_identity()\n\nm2 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt;\n            mutate(lwd = cost_eb / max(E(Gsp)$cost_eb)),\n          aes(colour = cost_eb, linewidth = lwd)) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  scale_linewidth_identity()\n\nm3 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = diff_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm4 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = rel_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm5 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = log_rel_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n  \nggarrange(plotlist = list(m1, m2, m3, m4, m5), ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nHere, it seems clear that the cost_eb is likely to be the most useful output.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html",
    "href": "notebooks/03-estimating-a-hiking-function.html",
    "title": "Estimating a hiking function",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-22\nAdded variables to hex bin filter method.\n\n\n2024-07-19\nAdded nlsLM estimation of models.\n\n\n2024-07-17\nInitial post.\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(here)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(minpack.lm)\nsrc_folder &lt;- str_glue(\"{here()}/_data/GPS-1516Season-MiersValley\")\ntgt_folder &lt;- str_glue(\"{here()}/_data/cleaned-gps-data\")\nall_traces &lt;- st_read(str_glue(\"{tgt_folder}/all-gps-traces.gpkg\"))\n\nReading layer `all-gps-traces' from data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' \n  using driver `GPKG'\nSimple feature collection with 112216 features and 26 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 352183.3 ymin: -1254631 xmax: 364874.4 ymax: -1240066\nProjected CRS: WGS 84 / Antarctic Polar Stereographic",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#now-look-at-the-slope-speed-plots",
    "href": "notebooks/03-estimating-a-hiking-function.html#now-look-at-the-slope-speed-plots",
    "title": "Estimating a hiking function",
    "section": "Now look at the slope-speed plots",
    "text": "Now look at the slope-speed plots\nIf we first make a crude estimate using geom_smooth on all the data we can start to appreciate the challenge\n\nggplot(all_traces, aes(x = slope_h, y = speed_km_h)) +\n  geom_point(alpha = 0.25, size = 0.5) +\n  geom_smooth() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nWhile the result is somewhat bell-shaped as we might hope, there are a lot of fixes that seem unlikely to be related to movement from place to place, rather they are ‘milling about’ in place - such as at basecamp, at experiment sites, at rest stops, etc.\nWe can of course fit a curve of desired functional form. In the relevant literature, there are essentially two functional forms, exponential, in Waldo Tobler’s original formulation1, given by \\[\nv=6e^{-3.5\\|{s+0.05}\\|}\n\\] which gives hiking speed \\(v\\) in km/h for a slope (given as rise over run) \\(s\\). This function has subsequently modified by others.2\nOther work3 yields speeds essentially Gaussian with respect to slope centred at slight downward slopes, i.e.\n\\[\nv = ae^{-b(s+c)^2}\n\\]\nWe can explicitly fit such functional forms to the data using the nlsLM function from the minpack.lm package:\n\n# Tobler-like\nmod_tobler &lt;- nlsLM(speed_km_h ~ a * exp(-b * abs(slope_h + c)), data = all_traces,\n                    start = c(a = 5, b = 3, c = 0.05))\n# Gaussian\nmod_gauss &lt;- nlsLM(speed_km_h ~ a * dnorm(slope_h, m, s), data = all_traces,\n                   start = c(a = 5, m = 0, s = 0.5))\n# Student's t (for heavier tails)\nmod_students_t &lt;- nlsLM(speed_km_h ~ a * dt((slope_h + m), d), data = all_traces,\n                   start = c(a = 5, m = 0, d = 0.001))\n\nNote that we use the probability density functions dnorm and dt here for convenience of parameterisation. It is easier to revert to base R plotting to get a feel for what these look like:\n\n# a non spatial version of the data for plotting - which it is convenient to \ngps_data &lt;- all_traces |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\ns &lt;- -200:200 / 100\nslopes &lt;- data.frame(slope_h = s)\nplot(gps_data, col = \"lightgray\", cex = 0.5)\nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nThat’s the process… but we need to apply it to filtered data to get closer to a reasonable outcome. Wrap the model-building in convenience functions:\n\nget_tobler_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * exp(-b * abs(slope_h + c)), data = df,\n        start = c(a = 5, b = 3, c = 0.05))  \n}\n\nget_gaussian_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dnorm(slope_h, m, s), data = df,\n        start = c(a = 5, m = 0, s = 0.5))\n}\n\nget_students_t_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dt((slope_h + m), d), data = df,\n        start = c(a = 5, m = 0, d = 0.001))\n}",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#cleaning-the-data-to-get-a-better-hiking-function",
    "href": "notebooks/03-estimating-a-hiking-function.html#cleaning-the-data-to-get-a-better-hiking-function",
    "title": "Estimating a hiking function",
    "section": "Cleaning the data to get a better hiking function",
    "text": "Cleaning the data to get a better hiking function\nThere are a number of features in the data that might allow us to exclude such data from consideration. We examine three below:\n\nOnly consider an ‘upper bound’ on the distribution, say the 90th percentile of speed_km_h relative to a given slope\nRemove fixes where the density of fixes is high since these may relate to base camp sites, etc.\nRemove fixes with high turn angles, since these may relate to where the data suggest a lot of ‘milling about’\n\nWe look at each of these below.\n\nUpper bound of the distribution\nIf we only consider speed estimates at or close to the upper bound of the data, then we are removing all the milling about data:\n\nd90 &lt;- gps_data |&gt;\n  mutate(slope_h = round(slope_h, 2)) |&gt;\n  group_by(slope_h) |&gt;\n  summarise(speed_km_h = quantile(speed_km_h, 0.9)) |&gt;\n  ungroup()\n\nmod_tobler &lt;- get_tobler_hiking_function(d90)\nmod_gauss &lt;- get_gaussian_hiking_function(d90)\nmod_students_t &lt;- get_students_t_hiking_function(d90)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from 90th percentile speeds\")\npoints(d90, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nBased on residual sum-of-squares, the Tobler-like function is the best fit in this case (although the differences are small). In this particular case, the paramterisation is strikingly similar to Tobler’s\n\nsummary(mod_tobler)\n\n\nFormula: speed_km_h ~ a * exp(-b * abs(slope_h + c))\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 5.950260   0.141452  42.065  &lt; 2e-16 ***\nb 3.399181   0.114934  29.575  &lt; 2e-16 ***\nc 0.014756   0.004926   2.995  0.00302 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5403 on 249 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.49e-08\n\n\nGiving us \\(v = 5.95e^{-3.39\\|s+0.0147\\|}\\), which only differs much from Tobler’s proposed formula by the offset from 0 slope, which is reduced from 0.05 to 0.0147.\nNote that by choosing a different quantile than the 90th percentile we can vary the speeds predicted by a model produced in this way.\n\n\nFiltering by densely travelled locations\nThe idea here is that densely travelled locations are near base camps and similar sites, and not where scientists are ‘on the move’. The approach below uses hex binning to find densely trafficked areas and exclude fixes in those areas.\n\nhexes &lt;- all_traces |&gt;\n  st_make_grid(cellsize = 60, square = FALSE, what = \"polygons\") |&gt;\n  st_as_sf(crs = st_crs(all_traces)) |&gt;\n  rename(geom = x) |&gt;\n  st_join(all_traces |&gt; mutate(id = row_number()), left = FALSE) |&gt;\n  group_by(geom) |&gt;\n  summarise(n = n(), n_persons = n_distinct(name)) |&gt;\n  select(n, n_persons) |&gt; \n  ungroup() |&gt;\n  mutate(density = n / n_persons)\n\nMap with tmap (greater flexibility on the classification scheme)\n\ntm_shape(hexes |&gt; pivot_longer(cols = c(n, n_persons, density))) +\n  tm_fill(col = \"value\", style = \"quantile\", n = 10, palette = \"-magma\") +\n  tm_facets(by = \"name\", free.scales = TRUE)\n\n\n\n\n\n\n\n\nBut where to cut off the data? Here are boxplots (note log scales) of the number of fixes n, the number of persons, n_persons, and the number of fixes per person, density, associated with each hex.\n\nhexes |&gt; \n  pivot_longer(cols = c(n, n_persons, density)) |&gt;\n  ggplot() +\n  geom_boxplot(aes(y = value)) +\n  scale_y_log10() +\n  facet_wrap( ~ name, scales = \"free_y\", ncol = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis doesn’t really help! … and it’s not entirely clear that this yields better results!\n\nd_hexes &lt;- all_traces |&gt;\n  st_join(hexes) |&gt; \n  filter(n &lt; 26) |&gt;\n  filter(density &lt; 7.5) |&gt; \n  filter(n_persons &lt; 5) |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\nmod_tobler &lt;- get_tobler_hiking_function(d_hexes)\nmod_gauss &lt;- get_gaussian_hiking_function(d_hexes)\nmod_students_t &lt;- get_students_t_hiking_function(d_hexes)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from less densely trafficked areas\")\npoints(d_hexes, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\n\n\nRemoving fixes with large turn angles\nAgain, this is a way to potentially remove data where there is a lot of milling about. As with the density approach, it’s hard to know what is a sensible cutoff. A map of the fixes coloured by turn angle is not necessarily much help:\n\nggplot(all_traces) +\n  geom_sf(aes(colour = turn_angle), size = 0.25)\n\n\n\n\n\n\n\n\nIn fact it seems that a rather aggressive seeming filter is needed before base camp and similar areas ‘show through’:\n\nggplot(all_traces |&gt; filter(turn_angle &lt; 5)) +\n  geom_sf(size = 0.2)\n\n\n\n\n\n\n\n\n\nd_low_turn_angle &lt;- all_traces |&gt;\n  filter(turn_angle &lt; 5) |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\nmod_tobler &lt;- get_tobler_hiking_function(d_low_turn_angle)\nmod_gauss &lt;- get_gaussian_hiking_function(d_low_turn_angle)\nmod_students_t &lt;- get_students_t_hiking_function(d_low_turn_angle)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from fixes with low turn angles\")\npoints(d_low_turn_angle, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#conclusion",
    "href": "notebooks/03-estimating-a-hiking-function.html#conclusion",
    "title": "Estimating a hiking function",
    "section": "Conclusion",
    "text": "Conclusion\nThis is still a work in progress…",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#footnotes",
    "href": "notebooks/03-estimating-a-hiking-function.html#footnotes",
    "title": "Estimating a hiking function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTobler WR. 1993. Three Presentations on Geographical Analysis and Modeling: Non-Isotropic Geographic Modeling; Speculations on the Geometry of Geography; and Global Spatial Analysis. Technical Report 93–1. NCGIA Technical Reports. Santa Barbara, CA: National Center for Geographic Information and Analysis.↩︎\nSee, for example, Márquez-Pérez J, I Vallejo-Villalta and JI Álvarez-Francoso. 2017. Estimated travel time for walking trails in natural areas. Geografisk Tidsskrift-Danish Journal of Geography 117(1) 53–62. doi: 10.1080/00167223.2017.1316212↩︎\nIrmischer IJ and KC Clarke. 2018. Measuring and modeling the speed of human navigation. Cartography and Geographic Information Science 45(2). 177–186. doi: 10.1080/15230406.2017.1292150↩︎",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/00-notes.html",
    "href": "notebooks/00-notes.html",
    "title": "Overview / plan",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-18\nReprioritised items.\n\n\n2024-07-18\nInitial post.\nWill keep this page updated as we go.",
    "crumbs": [
      "Notebooks",
      "Overview / plan"
    ]
  },
  {
    "objectID": "notebooks/00-notes.html#data-sources",
    "href": "notebooks/00-notes.html#data-sources",
    "title": "Overview / plan",
    "section": "Data sources",
    "text": "Data sources\n\nElevation data\nHowat, Ian, et al., 2022, “The Reference Elevation Model of Antarctica – Mosaics, Version 2”, https://doi.org/10.7910/DVN/EBW8UC, Harvard Dataverse, V1, Accessed: 28-29 August 2024. Downloaded from ftp.data.pgc.umn.edu at 10 and 32m resolutions.\n\n\nRock outcrops\nGerrish, L. (2020). Automatically extracted rock outcrop dataset for Antarctica (7.3) [Data set]. UK Polar Data Centre, Natural Environment Research Council, UK Research & Innovation. https://doi.org/10.5285/178ec50d-1ffb-42a4-a4a3-1145419da2bb\n\n\nGeology\nFrom GNS GeoMAP:\nCox SC, B Smith Lyttle, S Elkind, CS Smith Siddoway, P Morin, G Capponi, T Abu-Alam, M Ballinger, L Bamber, B Kitchener, L Lelli, JF Mawson, A Millikin, N Dal Seno, L Whitburn, T White, A Burton-Johnson, L Crispini, D Elliot, S Elvevold, JW Goodge, JA Halpin, J Jacobs, E Mikhalsky, AP Martin, F Morgan, J Smellie, P Scadden, and GS Wilson. 2023. The GeoMAP (v.2022-08) continent-wide detailed geological dataset of Antarctica.PANGAEA. doi: 10.1594/PANGAEA.951482.\nDownloaded from: https://download.pangaea.de/dataset/951482/files/ATA_SCAR_GeoMAP_v2022_08_QGIS.zip\n\n\nLakes\nGerrish, L., Fretwell, P., & Cooper, P. (2020). High resolution Antarctic lakes dataset (7.3) [Data set]. UK Polar Data Centre, Natural Environment Research Council, UK Research & Innovation. https://doi.org/10.5285/6a27ab9e-1258-49b1-bd2c-fcbed310ab45",
    "crumbs": [
      "Notebooks",
      "Overview / plan"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Science impacts on Antarctica",
    "section": "",
    "text": "A simple website to share ongoing project work on possible impacts of science in Antarctica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview / plan\n\n\nAntarctica Dry Valleys vulnerability to human impacts\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExploring GPS data\n\n\nChecking over GPS data for accuracy of speed, etc.\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning GPS data\n\n\nAssembling provided GPS data into cleaned files, and a single large dataset\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating a hiking function\n\n\nA work in progress…\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a hiking network\n\n\nAlso a work in progress…\n\n\n\n\n\n\n\n\nAug 6, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExploring hiking network options\n\n\nAlso a work in progress…\n\n\n\n\n\n\n\n\nAug 7, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA hiking network with better data\n\n\nHoming in on the most useful graph analysis outputs\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html",
    "href": "notebooks/04-building-a-hiking-network.html",
    "title": "Building a hiking network",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-08-06\nInitial post.\nThis notebook explores creation of hiking networks across a terrain where movement rates may depend on the direction of movement (because going uphill is different than going downhill…).\nFirst we set up some folders and filenames.\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\nbase_folder &lt;-    str_glue(\"{here()}/_data/testing\")\ninput_folder &lt;-   str_glue(\"{base_folder}/input\")\noutput_folder &lt;-  str_glue(\"{base_folder}/output\")\nbasename &lt;-       \"dry-valleys-10m\"\ndem_file &lt;-       str_glue(\"{input_folder}/{basename}.tif\")\nimagery_file &lt;-   str_glue(\"{input_folder}/{basename}-imagery.tif\")\nlandcover_file &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")\nextent_file &lt;-    str_glue(\"{input_folder}/{basename}-extent.gpkg\")\norigins_file &lt;-   str_glue(\"{input_folder}/{basename}-origins.gpkg\")\nNow read in the DEM and an extent, and imagery (which we might not use…).\nterrain &lt;- rast(dem_file)\n# and for visualization\nshade &lt;- get_hillshade(terrain)\nimagery &lt;- rast(imagery_file)\nextent &lt;- st_read(extent_file)\n\n# make a focal area for convenience of plotting in some situations\ncentre_of_extent &lt;- extent |&gt; \n  st_centroid() # we use this later...\nc_vec &lt;- centre_of_extent |&gt;\n  st_coordinates() |&gt;\n  as.vector()\nfocal_area &lt;- ((extent$geom[1] - c_vec) * diag(0.025, 2, 2) + c_vec) |&gt;\n  st_sfc(crs = st_crs(extent)) |&gt;\n  st_as_sf() |&gt;\n  rename(geom = x)\nSetup a resolution for use in building networks (i.e. determining which cells are adjacent to one another in vector GIS space). Although we have a DEM at 10m resolution, at least for now we’ll use a coarser grain for construction of the network. It is in any case unclear what an appropriate resolution might be given the various approximations in play.\n# assume square pixels: 50m on a side\nresolution &lt;- res(terrain)[1] * 5 \n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- 2 * resolution / sqrt(2 * sqrt(3))\n\nhexgrid_file &lt;- str_glue(\"{input_folder}/{basename}-hex-grid-{resolution}.gpkg\")\ngraph_file &lt;-   str_glue(\"{output_folder}/{basename}-graph-hex-{resolution}.txt\")\nhex_resolution is a hexagon spacing for use in st_make_grid that yields the same area cell as square cells with edge length given by resolution",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#making-a-hex-grid-across-the-terrain",
    "href": "notebooks/04-building-a-hiking-network.html#making-a-hex-grid-across-the-terrain",
    "title": "Building a hiking network",
    "section": "Making a hex grid across the terrain",
    "text": "Making a hex grid across the terrain\nWe build the network based on hex spaced points, not a square grid. This is helpful in making all edges the same length, as opposed to in a square grid where diagonal neighbour cell centres are further apart than edge neighbouring cells.\nWe load a hex grid file if one has already been made, otherwise build it.\nIMPORTANT NOTE: Don’t forget to remake this grid if changing any of the code prior to this point in the workflow in ways that affect the spacing of the hex grid.\n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    st_set_crs(st_crs(extent))\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nMake a plot to make sure things are the right scale, in particular that the hexes we get are the same area as the simple square grid cells would be (this isn’t critical, but it’s nice as a standardisation of what ‘resolution’ means for a hex grid).\n\nsquare_grid &lt;- focal_area |&gt;\n  st_make_grid(cellsize = resolution)\n\nggplot(square_grid) +\n  geom_sf(colour = \"white\") +\n  geom_sf(data = xy |&gt; st_filter(focal_area), colour = \"red\", size = 3) +\n  theme_void()\n\n\n\n\n\n\n\n\nBy visual inspection these are similar. We can confirm that the hexagon area in a grid at this spacing is\n\nfocal_area |&gt; \n  st_make_grid(cellsize = hex_cell_spacing, square = FALSE) |&gt; \n  head(1) |&gt; \n  st_area()\n\n2500 [m^2]\n\n\nwhich is the same as the square grid\n\nsquare_grid |&gt; head(1) |&gt; st_area()\n\n2500 [m^2]",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#retrieve-elevations-from-the-terrain",
    "href": "notebooks/04-building-a-hiking-network.html#retrieve-elevations-from-the-terrain",
    "title": "Building a hiking network",
    "section": "Retrieve elevations from the terrain",
    "text": "Retrieve elevations from the terrain\nAlthough we do the modelling of hiking functions using elevations in the GPS data, we don’t have complete elevation data for the area on that basis, so we retrieve these from the DEM.\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n# make pts and remove any NAs that might result from interpolation\n# NAs are around the edges of the study area\npts &lt;- xy |&gt;\n  mutate(z = z[, 2]) |&gt;\n  filter(!is.na(z))\n\nAgain a map is helpful, although again we use only the focal area so we can see what’s going on—and for speed of plotting.\n\nggplot(pts |&gt; st_filter(focal_area)) +\n  geom_sf(aes(colour = z), size = 20) +\n  scale_colour_distiller(palette = \"Oranges\") +\n  theme_void()",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#make-a-graph-i.e.-network-connecting-these-elevation-points",
    "href": "notebooks/04-building-a-hiking-network.html#make-a-graph-i.e.-network-connecting-these-elevation-points",
    "title": "Building a hiking network",
    "section": "Make a graph (i.e. network) connecting these elevation points",
    "text": "Make a graph (i.e. network) connecting these elevation points\nThe graph_from_points function in raster-to-graph-functions.R creates an igraph::graph object from an (x, y) point set. IMPORTANT NOTE: Don’t forget to remake the graph if messing with any of the code above here that might affect the spacing of the hex grid.\n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\nBefore we can visualize this easily, we have to attach various data to its elements (at the moment it is just a set of vertices without any attributes). The required data include a landcover layer with relative impedances. Note that for now this cover layer is entirely made up – pending consultation with someone more knowledgeable about conditions on the ground. As far as I can tell from imagery the high cost areas are ‘ice’ which I’ve arbitrarily assigned a relative cost of 5.\n\ncover &lt;- st_read(landcover_file)\n\nggplot() +\n  geom_sf(data = cover, aes(fill = as.factor(cost)), linewidth = 0) +\n  scale_fill_manual(values = c(\"white\", \"red\"), name = \"Relative cost\") +\n  # some ggnewscale shenanigans required to plot a second layer with fill aesthetic\n  new_scale_fill() +\n  geom_raster(data = shade |&gt; as.data.frame(xy = TRUE), \n              aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\nNext add attributes to the graph using the assignment_movement_variables_to_graph function in raster-to-graph-functions.R\n\nxyz &lt;- extract_xyz_from_points(pts)\n\ncosts &lt;- pts |&gt;\n  st_join(cover, .predicate = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  select(cost)\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz, impedances = costs$cost)\n\nNow we have a graph with coordinates assigned to its vertices, we can map it (after transforming to a sf dataset)\n\nggplot(G |&gt; get_graph_as_line_layer() |&gt; \n  st_filter(focal_area)) +\n  geom_sf(aes(colour = cost)) +\n  theme_void()\n\n\n\n\n\n\n\n\nIt’s not clear above that this is a bidirectional network, but we can see by inspection, e.g. the first edge in the graph is between vertices 1 and 166. We can access these (in igraph’s rather abstruse way), as follows:\n\nG |&gt; \n  edge_attr(index = get.edge.ids(G, c(1, 166, 166, 1))) |&gt;\n  as_tibble()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlength_xyz\nlength_xy\nz_diff\ngradient\nimpedance\ntobler_speed\ncost\n\n\n\n\n53.72997\n53.7285\n-0.3975143\n-0.0073986\n1\n3101.323\n0.0173249\n\n\n53.72997\n53.7285\n0.3975143\n0.0073986\n1\n2944.794\n0.0182457\n\n\n\n\n\n\nThe lengths of the edges are identical, their height differences and gradients are opposite and the speed (metres per hour) and cost (time to traverse in hours) are different due to the slope.",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#make-an-estimated-travel-time-map",
    "href": "notebooks/04-building-a-hiking-network.html#make-an-estimated-travel-time-map",
    "title": "Building a hiking network",
    "section": "Make an estimated travel time map",
    "text": "Make an estimated travel time map\nUsing the graph representation we can estimate travel times from any start location to every other location. For the sake of illustration we’ll make such a map from the graph vertex closest to the centre of the study area.\n\norigin_i &lt;- centre_of_extent |&gt;\n  st_nearest_feature(pts)\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), weights = edge_attr(G, \"cost\")) |&gt; t()\n\n# assemble results into a DF\ndf &lt;- data.frame(x = V(G)$x, y = V(G)$y, z = V(G)$z, \n                 time_hrs = V(G)$time_hrs, \n                 cost = costs$cost)\n# write this out to a shapefile, which Whitebox Tools needs\nshp_fname &lt;- str_glue(\"{here()}/_temp/{basename}-{format(origin_i, scientific = FALSE)}-hex.shp\")\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# note that interpolation is required because we are using hexagonal grid cells\n# do interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\nwbt_natural_neighbour_interpolation(shp_fname, field = \"time_hrs\",\n                                    output = tif_fname, \n                                    base = str_glue(dem_file))\n\nReading the exported raster layer made by whitebox::wbt_natural_neighbour_interpolation back in we get a map of estimated travel times.\n\nrast(tif_fname) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_distiller(palette = \"Spectral\", name = \"Est. time hrs\") +\n  geom_contour(aes(z = travel_time), linewidth = 0.5) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nIn practice we’ll use other graph functionalities than one origin to all destination shortest paths, but this gives the general idea.",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html",
    "href": "notebooks/01-exploring-gps-data.html",
    "title": "Exploring GPS data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-19\nUpdated to add turn angle calculation and source gps cleaning function from gps-data-util.R.\n\n\n2024-07-16\nInitial post.\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(here)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(zoo) # useful for serial data\nlibrary(R.utils)\n\nsource(str_glue(\"{here()}/scripts/gps-data-utils.R\"))\nThese notes explore the usefulness, accuracy etc. of the provided GPS data.\nOf particular interest is the asymmetry or not of movement speeds with respect to slope, given the salience of this for ‘hiking functions’. It is possible in a largely ‘unpathed’ environment that the asymmetry is limited (see e.g. Rees 20041)\nWe use just one of the datasets to explore general characteristics. Remove a meaningless column x and do a little bit of cleanup on the rcr and date and time columns.\ngps_data &lt;- get_gps_data_as_sf(str_glue(\n  \"{here()}/_data/GPS-1516Season-MiersValley/Fraser-FINAL.csv\")) |&gt;\n    select(-x) |&gt;\n    mutate(rcr = as.logical(rcr),\n           date_time = ymd_hms(paste(date, time)))",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#exploration-of-the-data",
    "href": "notebooks/01-exploring-gps-data.html#exploration-of-the-data",
    "title": "Exploring GPS data",
    "section": "Exploration of the data",
    "text": "Exploration of the data\nA summary of the data suggests that there are some data rows that should be ignored.\n\nsummary(gps_data)\n\n     index        rcr              date               time          \n Min.   :   1   Mode:logical   Length:7323        Length:7323       \n 1st Qu.:1832   TRUE:7304      Class :character   Class :character  \n Median :3662   NA's:19        Mode  :character   Mode  :character  \n Mean   :3662                                                       \n 3rd Qu.:5492                                                       \n Max.   :7323                                                       \n    valid              latitude          n_s              longitude    \n Length:7323        Min.   :-78.12   Length:7323        Min.   :163.7  \n Class :character   1st Qu.:-78.11   Class :character   1st Qu.:163.8  \n Mode  :character   Median :-78.10   Mode  :character   Median :163.8  \n                    Mean   :-78.10                      Mean   :163.9  \n                    3rd Qu.:-78.10                      3rd Qu.:163.9  \n                    Max.   :-78.09                      Max.   :164.2  \n     e_w               height_m       speed_km_h           pdop      \n Length:7323        Min.   :-23.1   Min.   :  0.000   Min.   : 0.98  \n Class :character   1st Qu.:102.1   1st Qu.:  0.255   1st Qu.: 1.14  \n Mode  :character   Median :110.8   Median :  0.496   Median : 1.19  \n                    Mean   :180.1   Mean   :  1.604   Mean   : 1.27  \n                    3rd Qu.:165.7   3rd Qu.:  2.241   3rd Qu.: 1.24  \n                    Max.   :636.9   Max.   :209.555   Max.   :13.76  \n      hdop              vdop        nsat_used_view       distance_m      \n Min.   : 0.6100   Min.   :0.7600   Length:7323        Min.   :    0.00  \n 1st Qu.: 0.7400   1st Qu.:0.8600   Class :character   1st Qu.:    1.38  \n Median : 0.7900   Median :0.8900   Mode  :character   Median :    2.78  \n Mean   : 0.7977   Mean   :0.9756                      Mean   :   21.73  \n 3rd Qu.: 0.8400   3rd Qu.:0.9200                      3rd Qu.:   14.99  \n Max.   :13.7200   Max.   :4.7100                      Max.   :74486.45  \n          geometry      date_time                     \n POINT        :7323   Min.   :2016-01-13 16:28:31.07  \n epsg:3031    :   0   1st Qu.:2016-01-15 16:43:27.00  \n +proj=ster...:   0   Median :2016-01-19 12:46:32.00  \n                      Mean   :2016-01-19 01:37:06.72  \n                      3rd Qu.:2016-01-22 08:39:45.00  \n                      Max.   :2016-01-23 15:16:47.00  \n\n\nA speed of &gt; 200km/h was clearly not achieved on foot! Similarly an elapsed distance between 1/30Hz fixes of 74km was not on foot either.\nThe GPS dilution of precision measures for horizontal and vertical position (hdop and vdop) are mostly within the ‘good’ range (&lt;1) with some outliers. The 3D positions (pdop) are not as good, but are also generally considered reliable (&lt;5). Inspection of a quick map of the data suggests helicopter movement or similar for some ‘singleton’ isolated observations:\n\nggplot(gps_data) +\n  geom_sf(size = 0.25) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis being the case it seems reasonable to remove observations with clearly unreasonable speed estimates, assuming that these result from use of other forms of transport. However, before doing so, we augment the data with additional estimates of speed, distance, turn angle, etc. calculated from consecutive fixes (it is not advisable to remove any fixes before making these estimates).",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#sanity-checking-the-gps-estimates-of-speed-distance-and-height",
    "href": "notebooks/01-exploring-gps-data.html#sanity-checking-the-gps-estimates-of-speed-distance-and-height",
    "title": "Exploring GPS data",
    "section": "Sanity checking the GPS estimates of speed, distance, and height",
    "text": "Sanity checking the GPS estimates of speed, distance, and height\nBefore applying that potentially simplistic cleaning of the data, it is also reasonable to sanity check the speed, distance, and height estimates in the GPS data. First we obtain height data from a 10m DEM.\n\nxy &lt;- gps_data |&gt;\n  st_coordinates() |&gt;\n  as_tibble()\n\ndem &lt;- rast(str_glue(\"{here()}/_data/input/dry-valleys-10m.tif\"))\n# and useful for later will be a hillshade\nslope &lt;- dem |&gt; terra::terrain(\"slope\", unit = \"radians\")\naspect &lt;- dem |&gt; terra::terrain(\"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect)\nTRI &lt;- dem |&gt; terra::terrain(\"TRI\")\n\nheights_from_dem &lt;- dem |&gt;\n  terra::extract(xy, method = \"simple\") |&gt;\n  select(2)\n\nslope_from_dem &lt;- slope |&gt;\n  terra::extract(xy, method = \"simple\") |&gt;\n  select(2)\n\nTRI_from_dem &lt;- TRI |&gt;\n  terra::extract(xy, method = \"simple\") |&gt;\n  select(2)\n\nxy &lt;- xy |&gt;\n  bind_cols(heights_from_dem) |&gt;\n  rename(Z = `dry-valleys-10m`) |&gt;\n  bind_cols(slope_from_dem) |&gt;\n  mutate(dem_slope = tan(slope)) |&gt;\n  select(-slope) |&gt;\n  bind_cols(TRI_from_dem)\n\ngps_data_plus &lt;- gps_data |&gt;\n  bind_cols(xy)\n\nNow calculate estimates of distance, change in elevation, and slope between observations, from these numbers and from the GPS data.\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  mutate(dx1 = as.numeric(diff(as.zoo(X), na.pad = TRUE)), \n         dy1 = as.numeric(diff(as.zoo(Y), na.pad = TRUE)),\n         dx2 = as.numeric(diff(as.zoo(X), lag = -1, na.pad = TRUE)),\n         dy2 = as.numeric(diff(as.zoo(Y), lag = -1, na.pad = TRUE)),\n         turn_angle = get_turn_angle(dx1, dy1, dx2, dy2),\n         dz = as.numeric(diff(as.zoo(Z), na.pad = TRUE)),\n         dh = as.numeric(diff(as.zoo(height_m), na.pad = TRUE)),\n         dt = as.numeric(diff(as.zoo(date_time), na.pad = TRUE)),\n         dxy = sqrt(dx1^ 2 + dy1 ^ 2),\n         dxyz = sqrt(dx1 ^ 2 + dy1 ^ 2 + dz ^ 2),\n         dxyh = sqrt(distance_m ^ 2 + dh ^ 2),\n         slope_h = dh / distance_m,\n         slope_z = dz / dxy,\n         speedxy = dxy / dt * 3600 / 1000, speedxyz = dxyz / dt * 3600 / 1000) \n\nNow some comparisons.\n\nDistance estimates\nThe distance_m estimates in the provided data are a little higher than the manually calculated point to point straight line distances between fixes. However, examination of sequences of data show the two time series are well aligned (below). Differences may be due to filtering applied by the GPS unit, although this is not well documented.\nIn any case, the GPS distance_m variable seems safe for use in further analysis.\n\ngps_data_plus |&gt; \n  filter(dt == 30) |&gt;\n  slice(2:151) |&gt;\n  pivot_longer(cols = c(distance_m, dxy, dxyz)) |&gt;\n  ggplot() + \n  geom_line(aes(x = date_time, y = value, colour = name, group = name), alpha = 0.5) +\n  xlab(\"Date/time\") + ylab(\"Estimated distance\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSpeed estimates\nDifferences in the speed estimates are more pronounced:\n\ngps_data_plus |&gt; \n  filter(dt == 30) |&gt;\n  slice(2:151) |&gt;\n  pivot_longer(cols = c(speed_km_h, speedxy, speedxyz)) |&gt;\n  ggplot() + \n  geom_line(aes(x = date_time, y = value, colour = name, group = name), alpha = 0.5) +\n  xlab(\"Date/time\") + ylab(\"Estimated speed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe provided speed estimates are generally higher although there is some alignment between the time series, insofar as extended periods of low speeds tend to line up.\nA likely reason for the differences is that in a 30 second period (the usual interval between fixes) an individual may move in many directions so that the direct point to point distance between fixes is less than might be expected based on the recorded speed. So, for example, in the above chart between 16:30 and 16:40 the person was moving not in straight lines, where in the following 10 minutes they were moving in relatively straight lines. We can test this theory, at least qualitatively, by calculating a sinuosity estimate between fixes.\n\nsinuosity &lt;- gps_data_plus |&gt;\n  filter(dt == 30) |&gt;\n  slice(2:61) |&gt;\n  # select(date_time, speed_km_h, dxy, speedxy) |&gt;\n  mutate(sinuosity = speed_km_h / 120 / distance_m,\n         date_time_start = lag(date_time)) \n\nspeeds &lt;- sinuosity |&gt;\n  pivot_longer(cols = c(speed_km_h, speedxy))\n\nggplot(speeds) +\n  geom_step(aes(x = date_time, y = value, group = name, linetype = name), \n            direction = \"vh\", lwd = 0.5) +\n  geom_segment(data = sinuosity, \n               aes(x = date_time_start, xend = date_time, \n                   y = speed_km_h, colour = rank(sinuosity)), lwd = 2) +\n  scale_colour_distiller(palette = \"Reds\", direction = 1) +\n  xlab(\"Date/time\") + ylab(\"Estimated speed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSince the estimated sinuosity here is based only on the GPS estimated speed and point to point distances, that it is generally highest when the difference between the GPS reported speed speed_km_h and the manually estimated speed speedxy is largest supports the assumption that the GPS is reporting speed using satellite carrier signal doppler effects.\nBased on this check, the GPS speed_km_h variable seems reasonable to use for hiking function estimation.\n\n\nGPS height and elevation from the DEM\nAgain, there are differences between heights recorded by the GPS units, and heights extracted from the 10m DEM. Again, it is difficult to know why these might occur, although there is a clear correlation between the two. There is no obvious correlation with surface roughness (TRI), for example, or with the vertical dilution of precision (vdop).\n\nggplot(gps_data_plus |&gt; \n         filter(dt == 30, distance_m &lt; 60, height_m &gt; 0, Z &gt; 0)) +\n  geom_point(aes(x = TRI, y = height_m / Z, colour = vdop), alpha = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPragmatically, in the next section, I find that the relationship between slope and speed based on the GPS readings is much better behaved than that derived from the speed and elevation extracted from the DEM. The reason it can be used to estimate slope in that context is divergences between height_m (from the GPS) and Z (from the DEM) are serially correlated, so that estimates of slope derived from consecutive data points are useable for that purpose.\nSome reassurance that the GPS height estimates are suitably aligned is obtained by plotting each days set of observations of the two as time series. On days where much of the activity occurred at the same elevation or across a very limited elevation range there are obvious deviations, but these are relatively small in magnitude (a few metres); on days when more elevation chance is observed, the overall profile of the day’s activity matches well.\n\ngps_data_plus |&gt; \n  filter(dt == 30) |&gt;\n  pivot_longer(cols = c(height_m, Z)) |&gt;\n  ggplot() + \n  geom_line(aes(x = date_time, y = value, colour = name, group = name), alpha = 0.5) +\n  xlab(\"Date/time\") + ylab(\"Estimated elevation\") + \n  facet_wrap(~ date, scales = \"free\", ncol = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAgain, it seems reasonable to use the GPS unit reported heights for estimation of hiking functions. The main impact on the analysis in the next section is to introduce noise into the slope at low values.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#a-hiking-function",
    "href": "notebooks/01-exploring-gps-data.html#a-hiking-function",
    "title": "Exploring GPS data",
    "section": "A hiking function",
    "text": "A hiking function\nA hiking function relates slope of the land to speed at which it is traversed. Using the GPS unit reported speed and slope derived from GPS unit reported distance and height, locally estimated scatter plot smoothing yields an approximately Gaussian curve with the peak speed at or close to 0 slope. This gives some confidence that we can use these data to estimate hiking functions. The approach implied in this plot is insufficient, as the data contain many fixes that don’t give much information about ‘hiking’ as such. The preponderance of lower turn angle fixes in the more ‘mobile’ parts of the data (i.e. higher speeds) suggests that it may be possible to filter out those data points.\nBetter estimation of a hiking function is left to a later notebook (see Estimating a hiking function).\n\ngps_data_plus |&gt;\n  # here's a possible filter to apply to the data to improve the estimation\n  filter(dt == 30, speed_km_h &lt; 10, !is.na(slope_h), distance_m &gt; 2.5, turn_angle &lt; 150) |&gt;\n  ggplot(aes(x = slope_h, y = speed_km_h, colour = turn_angle)) +\n  geom_point(size = 0.25) +\n  geom_smooth(aes(x = slope_h, y = speed_km_h))\n\n\n\n\n\n\n\nmodel &lt;- loess(speed_km_h ~ slope_h, data = gps_data_plus)\n\nMany data in the above plot are probably from ‘milling about’ in breaks, at camp, or around experiments, rather than while on the move. The best way to remove these sections is probably based on turn angles.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#conclusion-a-reasonable-gps-file-cleaning-function",
    "href": "notebooks/01-exploring-gps-data.html#conclusion-a-reasonable-gps-file-cleaning-function",
    "title": "Exploring GPS data",
    "section": "Conclusion: a reasonable GPS file cleaning function",
    "text": "Conclusion: a reasonable GPS file cleaning function\nBased on this exploration, GPS data need to be processed as follows (note that no initial conversion to a spatial format is required at this stage, since none of the added spatial data introduced above is required).\n\nRead raw data from CSV.\nClean names with janitor::clean_names.\nRemove meaningless variable x.\nAdd a name reflecting the id of the person whose data is.\nAugment data with dt (time interval between fixes), dh (height change between fixes), slope_h (estimated slope between fixes), and turn_angle (change in direction at this fix).\nRemove data outside expected latitudinal range (&gt; -78 latitude).\nRemove fixes with high estimated speeds (&gt;~ 10km/h).\nRemove fixes with long estimated distances (&gt;~ 60m).\nIdentify breaks in the series where dt != 30 and tag subsets of the data accordingly.\nCount number of fixes in each subset – consider removing small ones…\nDrop NAs and any rows where dt != 30.\n\nThis is the basis for a function get_cleaned_gps_data() in the script gps-data-utils.R used for cleaning up / augmenting the raw CSV files.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#footnotes",
    "href": "notebooks/01-exploring-gps-data.html#footnotes",
    "title": "Exploring GPS data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRees WG. 2004. Least-cost paths in mountainous terrain. Computers & Geosciences 30(3) 203–209. doi: 10.1016/j.cageo.2003.11.001.↩︎",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/02-collating-gps-data.html",
    "href": "notebooks/02-collating-gps-data.html",
    "title": "Cleaning GPS data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-19\nRerun to add turn angles.\n\n\n2024-07-17\nInitial post.\n\n\n\nRun this code once only to make clean and aggregated GPS datasets.\n\n\n\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(here)\nlibrary(sf)\n\nsource(str_glue(\"{here()}/scripts/gps-data-utils.R\"))\n\n\nRead raw files, clean and write any that still have data remaining\n\nsrc_folder &lt;- str_glue(\"{here()}/_data/GPS-1516Season-MiersValley\")\ntgt_folder &lt;- str_glue(\"{here()}/_data/cleaned-gps-data\")\n\nif (!file.exists(tgt_folder)) {\n  dir.create(tgt_folder)\n}\npersons &lt;- dir(src_folder, pattern = \".csv\") |&gt;\n  lapply(str_split_i, \"-\", i = 1) |&gt;\n  unlist()\ndfs &lt;- list()\ni &lt;- 0\nfor (p in persons) {\n  i &lt;- i + 1\n  df &lt;- get_cleaned_gps_data(str_glue(\"{src_folder}/{p}-FINAL.csv\"), p) \n  if (nrow(df) &gt; 0) {\n    df |&gt; write.csv(str_glue(\"{tgt_folder}/{p}-cleaned.csv\"))\n    dfs[[i]] &lt;- df\n  }\n}\nbind_rows(dfs) |&gt;\n  write.csv(str_glue(\"{tgt_folder}/all-gps-traces.csv\"))\n\n\n\nWrite to individual GPKG files and a single all traces file\n\nfiles &lt;- str_c(tgt_folder, \"/\", dir(path = tgt_folder, pattern = \".csv\"))\nfor (f in files) {\n  get_gps_data_as_sf(f) |&gt; \n    st_write(str_replace(f, \"csv\", \"gpkg\"), delete_dsn = TRUE)\n}\n\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' using driver `GPKG'\nWriting layer `all-gps-traces' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' using driver `GPKG'\nWriting 112216 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/AVC-cleaned.gpkg' using driver `GPKG'\nWriting layer `AVC-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/AVC-cleaned.gpkg' using driver `GPKG'\nWriting 5069 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Ben-cleaned.gpkg' using driver `GPKG'\nWriting layer `Ben-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Ben-cleaned.gpkg' using driver `GPKG'\nWriting 6918 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Charlie-cleaned.gpkg' using driver `GPKG'\nWriting layer `Charlie-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Charlie-cleaned.gpkg' using driver `GPKG'\nWriting 5851 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Craig-cleaned.gpkg' using driver `GPKG'\nWriting layer `Craig-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Craig-cleaned.gpkg' using driver `GPKG'\nWriting 1668 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Fraser-cleaned.gpkg' using driver `GPKG'\nWriting layer `Fraser-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Fraser-cleaned.gpkg' using driver `GPKG'\nWriting 7216 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Gemma-cleaned.gpkg' using driver `GPKG'\nWriting layer `Gemma-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Gemma-cleaned.gpkg' using driver `GPKG'\nWriting 8428 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Georgia-cleaned.gpkg' using driver `GPKG'\nWriting layer `Georgia-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Georgia-cleaned.gpkg' using driver `GPKG'\nWriting 4685 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanHawes-cleaned.gpkg' using driver `GPKG'\nWriting layer `IanHawes-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanHawes-cleaned.gpkg' using driver `GPKG'\nWriting 6593 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanMcD-cleaned.gpkg' using driver `GPKG'\nWriting layer `IanMcD-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanMcD-cleaned.gpkg' using driver `GPKG'\nWriting 8861 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Jayne-cleaned.gpkg' using driver `GPKG'\nWriting layer `Jayne-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Jayne-cleaned.gpkg' using driver `GPKG'\nWriting 6448 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Kurt-cleaned.gpkg' using driver `GPKG'\nWriting layer `Kurt-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Kurt-cleaned.gpkg' using driver `GPKG'\nWriting 6178 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Marwan-cleaned.gpkg' using driver `GPKG'\nWriting layer `Marwan-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Marwan-cleaned.gpkg' using driver `GPKG'\nWriting 10385 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Paul-cleaned.gpkg' using driver `GPKG'\nWriting layer `Paul-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Paul-cleaned.gpkg' using driver `GPKG'\nWriting 7320 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Peyman-cleaned.gpkg' using driver `GPKG'\nWriting layer `Peyman-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Peyman-cleaned.gpkg' using driver `GPKG'\nWriting 8390 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Pierre-cleaned.gpkg' using driver `GPKG'\nWriting layer `Pierre-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Pierre-cleaned.gpkg' using driver `GPKG'\nWriting 8615 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Tim-cleaned.gpkg' using driver `GPKG'\nWriting layer `Tim-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Tim-cleaned.gpkg' using driver `GPKG'\nWriting 9591 features with 26 fields and geometry type Point.",
    "crumbs": [
      "Notebooks",
      "Cleaning GPS data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#conclusions",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#conclusions",
    "title": "A hiking network with better data",
    "section": "Conclusions",
    "text": "Conclusions\nTentatively, I conclude from the above that the most useful outputs are the vertex cost-based betweenness and difference outputs. The edge betweenness results are similar, harder to map, and double the computation times without adding much.\nThe next step is to investigate the impact on computation times of imposing a ‘cutoff’ time on vertex betweenness measures. Some initial exploration suggests that this greatly speeds up computation, again, without necessarily leading to very different conclusions concerning likely areas of high impact.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  }
]