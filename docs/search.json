[
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html",
    "href": "notebooks/12-betweenness-for-specified-ODs.html",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-19\nDetailed the commentary.\n\n\n2024-09-16\nInitial post.\nThis notebook explores developing the betweenness centrality networks/risk maps based on a known set of most often used origins and destinations. The idea would be to supply a list of locations with associated weightings, where for example base camp might be (say) 25, some once-visited sampling locations might be ‘1’, repeat-visit experimental sites might be 5. Betweenness is then based on all shortest paths among these sites weighted by those numbers.\nAs an extension it might be possible to use outputs from this to propose a low impact network of routes to use for an expedition.\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(ggspatial)\nlibrary(hrbrthemes)\nlibrary(spatstat)\nlibrary(purrr)\nlibrary(igraph)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html#setting-up-the-data-sets-and-folders",
    "href": "notebooks/12-betweenness-for-specified-ODs.html#setting-up-the-data-sets-and-folders",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "Setting up the data sets and folders",
    "text": "Setting up the data sets and folders\nThis is similar to many previous notebooks.\n\n\nCode\ndem_resolution     &lt;- \"32m\" # 10m also available, but probably not critical\nimpassable_geology &lt;- \"ice\"\ndata_folder        &lt;- str_glue(\"{here()}/_data\")\nbase_folder        &lt;- str_glue(\"{data_folder}/dry-valleys\")\ninput_folder       &lt;- str_glue(\"{base_folder}/common-inputs\")\noutput_folder      &lt;- str_glue(\"{base_folder}/output\")\nbasename           &lt;- str_glue(\"dry-valleys\")\ncontiguous_geology &lt;- \"05\"  # use small one '05' for testing\n\ndem_folder         &lt;- str_glue(\"{data_folder}/dem/{dem_resolution}\")\ndem_file           &lt;- str_glue(\"{dem_folder}/{basename}-combined-{dem_resolution}-dem-clipped.tif\")\nextent_file        &lt;- str_glue(\"{base_folder}/contiguous-geologies/{basename}-extent-{contiguous_geology}.gpkg\")\nextent             &lt;- st_read(extent_file)\n\nterrain &lt;- rast(dem_file) |&gt;\n  crop(extent |&gt; as(\"SpatVector\")) |&gt;\n  mask(extent |&gt; as(\"SpatVector\"))\nshade &lt;- get_hillshade(terrain) |&gt;\n  as.data.frame(xy = TRUE) # this is for plotting in ggplot\n\nhillshade_basemap  &lt;- ggplot(shade) +\n  geom_raster(data = shade, aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()\n\ngeologies_file     &lt;- str_glue(\"{data_folder}/ata-scar-geomap-geology-v2022-08-clipped.gpkg\")\n\n\nAlso similar is building or retrieving the graph representation of the terrain.\n\n\nCode\nresolution &lt;- 150\n\nhex_cell_spacing &lt;- resolution * sqrt(2 / sqrt(3))\n# check if we have already made this dataset\nhexgrid_file &lt;- str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-hex-grid-{resolution}.gpkg\")\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    rename(geometry = x) |&gt;\n    st_set_crs(st_crs(extent))\n  coords &lt;- xy |&gt; st_coordinates()\n  xy &lt;- xy |&gt;\n    mutate(x = coords[, 1], y = coords[, 2])\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n\npts &lt;- xy |&gt;\n  mutate(z = z[, 2])\n\n# remove any NAs that might result from interpolation or impassable terrain\n# NAs tend to occur around the edge of study area\ncover &lt;- st_read(geologies_file) |&gt;\n  dplyr::select(POLYGTYPE) |&gt; \n  rename(terrain = POLYGTYPE)\n\npts &lt;- pts |&gt;\n  st_join(cover) |&gt;\n  filter(!is.na(z), terrain != impassable_geology)\n\ngraph_file &lt;-   \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-graph-hex-{resolution}.txt\") \nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\nG &lt;- assign_movement_variables_to_graph_2(G, xyz = pts, terrain = pts$terrain)\nV(G)$id &lt;- 1:length(G)\ncost_cutoff &lt;- 0.5 * resolution / 100\nG &lt;- delete_edges(G, E(G)[which(E(G)$cost &gt; cost_cutoff)]) |&gt;\n  largest_component(mode = \"strong\")\n\nremaining_nodes &lt;- V(G)$id\npts &lt;- pts |&gt; slice(remaining_nodes)",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html#make-a-bunch-of-origin-locations",
    "href": "notebooks/12-betweenness-for-specified-ODs.html#make-a-bunch-of-origin-locations",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "Make a bunch of origin locations",
    "text": "Make a bunch of origin locations\nUse spatstat to make a set of random locations that are at least somewhat spaced apart.\n\n\nCode\norigin_pts &lt;- rSSI(2000, n = 20, win = as.owin(extent)) |&gt; \n  st_as_sf() |&gt; \n  st_set_crs(st_crs(extent)) |&gt; \n  slice(-1) |&gt; \n  dplyr::select(-label)\n# get the sequence numbers of thes in the pts data\norigin_is &lt;- origin_pts |&gt;\n  st_nearest_feature(pts)\n# make up a point dataset of these origins with randomly assigned importance\norigin_vs &lt;- pts |&gt;\n  mutate(id = row_number()) |&gt;\n  slice(origin_is) |&gt;\n  mutate(importance = sample(1:25, n(), replace = TRUE, prob = 25:1))",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html#connect-them-all-in-various-ways",
    "href": "notebooks/12-betweenness-for-specified-ODs.html#connect-them-all-in-various-ways",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "Connect them all in various ways",
    "text": "Connect them all in various ways\nWhile doing so, we accumulate how often they appear on shortest paths.\nThere are a couple of key issues to note here:\n\nThe weights parameter referenced if using all all_shortest_paths MUST NOT be equal across all edges. If it is, then given the uniformity of the underlying lattice there is likely to be a combinatorial explosion as all the many equal shortest paths between every pair of sites are calculated.\nThe first version (NOT USED) considers potential multiple shortest paths between sites and is much slower because of the nested iteration and relatively complicated join process along with the down-weighting of vertices when more than one path is found between two sites. This is almost certainly an unnecessary detail.\n\n\n\nCode\nresult &lt;- data.frame(v = V(G) |&gt; \n                       as.vector() |&gt; \n                       as.character(), \n                     betweenness = 0)\nfor (i in 1:nrow(origin_vs)) {\n  for (j in 1:nrow(origin_vs)) {\n    if (i != j) {\n      ASP &lt;- all_shortest_paths(G, origin_vs$id[i], origin_vs$id[j], \n                                weights = edge_attr(G, \"cost\"))\n      n_shortest_paths &lt;- length(ASP$vpaths)\n      result &lt;- result |&gt; \n        left_join(ASP$vpaths |&gt;\n                    lapply(as.vector) |&gt;\n                    reduce(c) |&gt;\n                    table() |&gt;\n                    as.data.frame() |&gt;\n                    mutate(v = as.character(Var1)) |&gt;\n                    dplyr::select(-Var1), by = \"v\") |&gt;\n        mutate(Freq = replace_na(Freq, 0)) |&gt;\n        mutate(betweenness = \n                 betweenness + Freq / n_shortest_paths *\n                 origin_vs$importance[i]) |&gt;\n        dplyr::select(-Freq)\n    }\n  }\n}\nG &lt;- G |&gt; set_vertex_attr(\"sp_betweenness\", value = result$betweenness)\n\n\n\nThe second version is much quicker, but is an approximation to the extent that it ignores competing equal shortest paths, but instead chooses one at random (or by whatever means igraph makes this choice). Note that unless two paths are exactly the same total cost then the much slower unused version of the code will still, in effect, pick the shorter path, when in fact ‘on the ground’ either might be just as likely to be selected. First up, we use it calculate all sites to all sites shortest paths and overlay those.\n\n\n\nCode\n# put all vertices on a shortest path to begin\nvs_on_shortest_paths &lt;- V(G) |&gt; \n  as.vector()\n\n# make a data frame we can join the results into\nfor (i in seq_along(origin_vs$id)) {\n  # one entry for each appearance on a shortest path\n  SP &lt;- shortest_paths(G, origin_vs$id[i], origin_vs$id[-i], weights = edge_attr(G, \"cost\"))\n  new_shortest_paths &lt;- SP$vpath |&gt;\n    lapply(as.vector) |&gt;\n    reduce(c) |&gt;\n    rep(origin_vs$importance[i])\n  vs_on_shortest_paths &lt;- vs_on_shortest_paths |&gt;\n    c(new_shortest_paths)\n}\nshortest_path_counts &lt;- vs_on_shortest_paths |&gt;\n  table() |&gt;\n  as.vector()\nG &lt;- set_vertex_attr(G, \"sp_betweenness\", value = shortest_path_counts - 1)\n\n\nAnd map it\n\n\nCode\nspb &lt;- G |&gt; igraph::as_data_frame(what = \"vertices\")\nhillshade_basemap + \n  geom_point(data = spb |&gt; filter(sp_betweenness &gt; 0), \n             aes(x = x, y = y, colour = sp_betweenness),size = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  geom_sf(data = origin_vs, aes(size = importance), colour = \"black\", pch = 1) +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\nA naive attempt to determine a minimal network\nHere, we calculate the distance matrix and determine the greatest minimum distance to a near site across all sites, then construct a network consisting only of connections shorter than this.\n\n\nCode\ndist_m &lt;- distances(G, origin_vs$id, origin_vs$id, weights = edge_attr(G, \"cost\"))\ndiag(dist_m) &lt;- max(dist_m)\nrow_max_min &lt;- dist_m |&gt; apply(MARGIN = 1, FUN = min) |&gt; max()\ncol_max_min &lt;- dist_m |&gt; apply(MARGIN = 2, FUN = min) |&gt; max()\ncutoff &lt;- max(row_max_min, col_max_min)\n\n# hence the connections to be made are given by\nwhich_ijs &lt;- which(dist_m &lt;= cutoff, arr.ind = TRUE)\n# and proceed as before\nvs_on_shortest_paths &lt;- V(G) |&gt; \n  as.vector()\n# make a data frame we can join the results into\nfor (k in 1:nrow(which_ijs)) {\n  i &lt;- which_ijs[k, 1]\n  j &lt;- which_ijs[k, 2]\n  # one entry for each appearance on a shortest path\n  SP &lt;- shortest_paths(G, origin_vs$id[i], origin_vs$id[j], weights = edge_attr(G, \"cost\"))\n  new_shortest_paths &lt;- SP$vpath[[1]] |&gt;\n    as.vector() |&gt;\n    rep(origin_vs$importance[i])\n  vs_on_shortest_paths &lt;- vs_on_shortest_paths |&gt;\n    c(new_shortest_paths)\n}\nshortest_path_counts &lt;- vs_on_shortest_paths |&gt;\n  table() |&gt;\n  as.vector()\nG &lt;- set_vertex_attr(G, \"sp_betweenness\", value = shortest_path_counts - 1)\n\n\nAnd map it\n\n\nCode\nspb &lt;- G |&gt; igraph::as_data_frame(what = \"vertices\")\nhillshade_basemap + \n  geom_point(data = spb |&gt; filter(sp_betweenness &gt; 0), \n             aes(x = x, y = y, colour = sp_betweenness),size = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  geom_sf(data = origin_vs, aes(size = importance), colour = \"black\", pch = 1) +\n  theme_ipsum_rc()",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html#a-minimal-network-connecting-all-sites",
    "href": "notebooks/12-betweenness-for-specified-ODs.html#a-minimal-network-connecting-all-sites",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "A minimal network connecting all sites",
    "text": "A minimal network connecting all sites\nWe can use a minimum spanning tree to make a network instead… with one reservation: the minimum spanning tree of a directed graph is not found using algorithms for MSTs on undirected graphs, such as that offered by igraph::mst.\nThe directed graph equivalent to an MST is called a minimum-cost arborescence or optimal branching.1 The networkx package in python supports the required algorithms, but igraph does not.\nSince the asymmetry in our networks is not extreme, we can approximate a solution in igraph in this case by round tripping the network via an undirected network and back to a directed one.\nFirst the incorrect arborescence…\n\n\nCode\nxs &lt;- vertex_attr(G, \"x\", V(G)[origin_vs$id])\nys &lt;- vertex_attr(G, \"y\", V(G)[origin_vs$id])\napprox_min_arborescence &lt;- dist_m |&gt; \n  graph_from_adjacency_matrix(mode = \"directed\", weighted = TRUE) |&gt;\n  set_vertex_attr(\"x\", value = xs) |&gt;\n  set_vertex_attr(\"y\", value = ys) |&gt;\n  mst() # this applies algorithms appropriate to an undirected graph\nplot(approx_min_arborescence, \n     vertex.size = 20, edge.arrow.size = .35, \n     xlim = range(xs), ylim = range(ys), rescale = FALSE, asp = 1)\n\n\n\n\n\n\n\n\n\nNow round-trip it via an undirected graph. For some reason, I can’t get igraph::as.undirected() to do this, so use matrix representation.\n\n\nCode\ndir_m &lt;- approx_min_arborescence |&gt; as_adjacency_matrix(sparse = FALSE)\nundir_m &lt;- ((dir_m + t(dir_m)) &gt; 0)\nwhich_ijs &lt;- which(undir_m, arr.ind = TRUE)\n\n# put all vertices on a shortest path to begin\nvs_on_shortest_paths &lt;- V(G) |&gt; \n  as.vector()\n\n# make a data frame we can join the results into\nfor (k in 1:nrow(which_ijs)) {\n  i &lt;- which_ijs[k, 1]\n  j &lt;- which_ijs[k, 2]\n  # one entry for each appearance on a shortest path\n  SP &lt;- shortest_paths(G, origin_vs$id[i], origin_vs$id[j], weights = edge_attr(G, \"cost\"))\n  new_shortest_paths &lt;- SP$vpath[[1]] |&gt;\n    as.vector() |&gt;\n    rep(origin_vs$importance[i])\n  vs_on_shortest_paths &lt;- vs_on_shortest_paths |&gt;\n    c(new_shortest_paths)\n}\nshortest_path_counts &lt;- vs_on_shortest_paths |&gt;\n  table() |&gt;\n  as.vector()\nG &lt;- set_vertex_attr(G, \"sp_betweenness\", value = shortest_path_counts - 1)\n\n\nAnd once more… map it\n\n\nCode\nspb &lt;- G |&gt; igraph::as_data_frame(what = \"vertices\")\nhillshade_basemap + \n  geom_point(data = spb |&gt; filter(sp_betweenness &gt; 0), \n             aes(x = x, y = y, colour = sp_betweenness),size = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  geom_sf(data = origin_vs, aes(size = importance), colour = \"black\", pch = 1) +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nIt is interesting to note the total cost of this network and the cost of its longest connection between sites:\n\n\nCode\nedges &lt;- which_ijs |&gt;\n  as.data.frame() |&gt;\n  mutate(from = origin_vs$id[row], to = origin_vs$id[col]) |&gt;\n  mutate(cost = 0) |&gt; \n  dplyr::select(-row, -col)\nvertices &lt;- data.frame(\n  id = origin_vs$id,\n  x = vertex_attr(G, \"x\", origin_vs$id),\n  y = vertex_attr(G, \"y\", origin_vs$id))\nfor (k in 1:nrow(edges)) {\n  i &lt;- edges$from[k]\n  j &lt;- edges$to[k]\n  costs = distances(G, i, j, weights = edge_attr(G, \"cost\"))\n  edges$cost[k] &lt;- costs |&gt; as.vector()\n}\nprint(str_glue(\"Total cost: {sum(edges$cost)}\\nHighest cost: {max(edges$cost)}\"))\n\n\nTotal cost: 53.1451751235408\nHighest cost: 3.06211977831383",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/12-betweenness-for-specified-ODs.html#footnotes",
    "href": "notebooks/12-betweenness-for-specified-ODs.html#footnotes",
    "title": "Betweenness centrality based on a limited set of locations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this and Korte B and J Vygen. 2018. Spanning Trees and Arborescences. In Combinatorial Optimization: Theory and Algorithms, ed. B Korte and J Vygen, 133–157. Berlin, Heidelberg: Springer. doi:10.1007/978-3-662-56039-6_6.↩︎",
    "crumbs": [
      "Notebooks",
      "Betweenness centrality based on a limited set of locations"
    ]
  },
  {
    "objectID": "notebooks/02-collating-gps-data.html",
    "href": "notebooks/02-collating-gps-data.html",
    "title": "Cleaning GPS data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-19\nRerun to add turn angles.\n\n\n2024-07-17\nInitial post.\n\n\n\nRun this code once only to make clean and aggregated GPS datasets.\n\n\n\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(here)\nlibrary(sf)\n\nsource(str_glue(\"{here()}/scripts/gps-data-utils.R\"))\n\n\nRead raw files, clean and write any that still have data remaining\n\nsrc_folder &lt;- str_glue(\"{here()}/_data/GPS-1516Season-MiersValley\")\ntgt_folder &lt;- str_glue(\"{here()}/_data/cleaned-gps-data\")\n\nif (!file.exists(tgt_folder)) {\n  dir.create(tgt_folder)\n}\npersons &lt;- dir(src_folder, pattern = \".csv\") |&gt;\n  lapply(str_split_i, \"-\", i = 1) |&gt;\n  unlist()\ndfs &lt;- list()\ni &lt;- 0\nfor (p in persons) {\n  i &lt;- i + 1\n  df &lt;- get_cleaned_gps_data(str_glue(\"{src_folder}/{p}-FINAL.csv\"), p) \n  if (nrow(df) &gt; 0) {\n    df |&gt; write.csv(str_glue(\"{tgt_folder}/{p}-cleaned.csv\"))\n    dfs[[i]] &lt;- df\n  }\n}\nbind_rows(dfs) |&gt;\n  write.csv(str_glue(\"{tgt_folder}/all-gps-traces.csv\"))\n\n\n\nWrite to individual GPKG files and a single all traces file\n\nfiles &lt;- str_c(tgt_folder, \"/\", dir(path = tgt_folder, pattern = \".csv\"))\nfor (f in files) {\n  get_gps_data_as_sf(f) |&gt; \n    st_write(str_replace(f, \"csv\", \"gpkg\"), delete_dsn = TRUE)\n}\n\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' using driver `GPKG'\nWriting layer `all-gps-traces' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' using driver `GPKG'\nWriting 112216 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/AVC-cleaned.gpkg' using driver `GPKG'\nWriting layer `AVC-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/AVC-cleaned.gpkg' using driver `GPKG'\nWriting 5069 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Ben-cleaned.gpkg' using driver `GPKG'\nWriting layer `Ben-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Ben-cleaned.gpkg' using driver `GPKG'\nWriting 6918 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Charlie-cleaned.gpkg' using driver `GPKG'\nWriting layer `Charlie-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Charlie-cleaned.gpkg' using driver `GPKG'\nWriting 5851 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Craig-cleaned.gpkg' using driver `GPKG'\nWriting layer `Craig-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Craig-cleaned.gpkg' using driver `GPKG'\nWriting 1668 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Fraser-cleaned.gpkg' using driver `GPKG'\nWriting layer `Fraser-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Fraser-cleaned.gpkg' using driver `GPKG'\nWriting 7216 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Gemma-cleaned.gpkg' using driver `GPKG'\nWriting layer `Gemma-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Gemma-cleaned.gpkg' using driver `GPKG'\nWriting 8428 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Georgia-cleaned.gpkg' using driver `GPKG'\nWriting layer `Georgia-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Georgia-cleaned.gpkg' using driver `GPKG'\nWriting 4685 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanHawes-cleaned.gpkg' using driver `GPKG'\nWriting layer `IanHawes-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanHawes-cleaned.gpkg' using driver `GPKG'\nWriting 6593 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanMcD-cleaned.gpkg' using driver `GPKG'\nWriting layer `IanMcD-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/IanMcD-cleaned.gpkg' using driver `GPKG'\nWriting 8861 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Jayne-cleaned.gpkg' using driver `GPKG'\nWriting layer `Jayne-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Jayne-cleaned.gpkg' using driver `GPKG'\nWriting 6448 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Kurt-cleaned.gpkg' using driver `GPKG'\nWriting layer `Kurt-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Kurt-cleaned.gpkg' using driver `GPKG'\nWriting 6178 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Marwan-cleaned.gpkg' using driver `GPKG'\nWriting layer `Marwan-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Marwan-cleaned.gpkg' using driver `GPKG'\nWriting 10385 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Paul-cleaned.gpkg' using driver `GPKG'\nWriting layer `Paul-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Paul-cleaned.gpkg' using driver `GPKG'\nWriting 7320 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Peyman-cleaned.gpkg' using driver `GPKG'\nWriting layer `Peyman-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Peyman-cleaned.gpkg' using driver `GPKG'\nWriting 8390 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Pierre-cleaned.gpkg' using driver `GPKG'\nWriting layer `Pierre-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Pierre-cleaned.gpkg' using driver `GPKG'\nWriting 8615 features with 26 fields and geometry type Point.\nDeleting source `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Tim-cleaned.gpkg' using driver `GPKG'\nWriting layer `Tim-cleaned' to data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/Tim-cleaned.gpkg' using driver `GPKG'\nWriting 9591 features with 26 fields and geometry type Point.",
    "crumbs": [
      "Notebooks",
      "Cleaning GPS data"
    ]
  },
  {
    "objectID": "notebooks/08-taking-stock.html",
    "href": "notebooks/08-taking-stock.html",
    "title": "Taking stock",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded some notes on future work.\n\n\n2024-08-29\nInitial post.\nThis notebook sets out progress to date (end of August 2024).\nSince it was assembled, some refinements to the process have been developed:\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\nlibrary(ggspatial)\nlibrary(tmap)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\nbase_folder &lt;- here()\ndata_folder &lt;- str_glue(\"{base_folder}/_data\")\ndem_folder  &lt;- str_glue(\"{base_folder}/_data/dem\")",
    "crumbs": [
      "Notebooks",
      "Taking stock"
    ]
  },
  {
    "objectID": "notebooks/08-taking-stock.html#definition-of-the-study-area",
    "href": "notebooks/08-taking-stock.html#definition-of-the-study-area",
    "title": "Taking stock",
    "section": "Definition of the study area",
    "text": "Definition of the study area\nI’ve settled on two ‘basins’, Skelton and Dry Valleys, as defined by the NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) project, downloadable from here.1\n\n\nCode\ncoastline &lt;- st_read(str_glue(\"{data_folder}/coastline.gpkg\"))\nbasins &lt;- st_read(str_glue(\"{dem_folder}/antarctic-basins.gpkg\"))\nstudy_basins &lt;- st_read(str_glue(\"{dem_folder}/dry-valleys-and-skelton.gpkg\"))\nstudy_area &lt;- st_read(str_glue(\"{dem_folder}/study-area-extent.gpkg\"))\ngps_data &lt;- st_read(str_glue(\"{data_folder}/cleaned-gps-data/all-gps-traces.gpkg\"))\ntiles &lt;- st_read(str_glue(\"{dem_folder}/REMA-index-included-tiles.gpkg\"))\nshade &lt;- rast(str_glue(\"{dem_folder}/32m/dry-valleys-combined-32m-browse.tif\"))\nshade8 &lt;- shade |&gt;\n  terra::aggregate(8) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  rename(shade = `15_35_32m_v2.0_browse`)\ngeology &lt;- st_read(str_glue(\"{data_folder}/ata-scar-geomap-geology-v2022-08-clipped.gpkg\"))\ncontiguous_geologies &lt;- st_read(str_glue(\"{data_folder}/dry-valleys/common-inputs/contiguous-geologies.gpkg\"))\n\n\n\n\nCode\nbb &lt;- st_bbox(study_area)\nggplot() +\n  # geom_sf(data = basins) + \n  geom_sf(data = coastline, colour = \"#5588aa\", fill = \"#00000000\") +\n  geom_sf(data = study_area, fill = \"#666666\", colour = \"white\") +\n  annotate(\"rect\", xmin = bb[1], ymin = bb[2], xmax = bb[3], ymax = bb[4], fill = \"#00000000\", colour = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"study-area-locator-map.png\", bg = \"white\")\n\n\nLooking more closely, Skelton was included in the study area extent because the provided GPS data from the 2015-16 season (shown in red below) extended into this basin in addition the Dry Valleys basin.\n\n\nCode\nggplot() +\n  geom_sf(data = coastline) +\n  geom_sf(data = study_basins, fill = \"grey\", colour = \"white\") +\n  geom_sf(data = gps_data, colour = \"red\", size = 0.025, name = \"GPS data\") +\n  geom_sf_text(data = study_basins, aes(label = NAME)) +\n  coord_sf(xlim = bb[c(1, 3)], ylim = bb[c(2, 4)]) +\n  annotation_scale(height = unit(0.1, \"cm\"), location = \"tr\") +\n  theme_minimal() +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"study-area-closeup-map.png\", bg = \"white\")\n\n\n\nDEM data\nBased on this study area DEM data was assembled from the REMA project2 downloaded from ftp.data.pgc.umn.edu, using at least to begin the 32m resolution product, extending across tiles as shown below.\n\n\nCode\nggplot() +\n  geom_raster(data = shade8, aes(x = x, y = y, fill = shade), alpha = 0.35) +\n  scale_fill_distiller(palette = \"Blues\", guide = \"none\") +\n  geom_sf(data = study_area, fill = \"#00000000\", colour = \"red\") +\n  geom_sf(data = tiles, fill = \"#00000000\", colour = \"darkgrey\") +\n  geom_sf_text(data = tiles, aes(label = tile), size = 2.5) +\n  theme_void()",
    "crumbs": [
      "Notebooks",
      "Taking stock"
    ]
  },
  {
    "objectID": "notebooks/08-taking-stock.html#definition-of-areas-of-interest",
    "href": "notebooks/08-taking-stock.html#definition-of-areas-of-interest",
    "title": "Taking stock",
    "section": "Definition of areas of interest",
    "text": "Definition of areas of interest\nMuch of the area delineated above is covered in permanent snow and ice. To delineate areas for closer study we use areas of classified geology obtained from the GNS GeoMAP3 and downloaded from here\nThis layer is clipped to the study area extent. Colours in the map below are randomly signed and indicative only.\n\n\nCode\nggplot() + \n  geom_sf(data = geology, aes(fill = SIMPDESC), linewidth = 0) +\n  guides(fill = \"none\") +\n  geom_sf(data = study_area, fill = \"#00000000\", colour = \"black\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThe geology layer serves two purposes:\n\nDelineation of areas of interest: large regions of contiguous geology are chosen as study areas\nEstimation of relative movement costs for building the hiking network\n\n\nAreas of interest\nThe five largest regions of contiguous geology in the study area have been chosen for closer study. These range in extent from almost 2600 to around 320 sq.km.\n\n\nCode\ntop5 &lt;- contiguous_geologies |&gt; arrange(desc(area)) |&gt; slice(1:5)\nbb &lt;- st_bbox(top5)\nggplot() +\n  geom_sf(data = study_area, fill = \"#00000000\", colour = \"grey\") +\n  geom_sf(data = geology, aes(fill = SIMPDESC), linewidth = 0, alpha = 0.35) +\n  geom_sf(data = contiguous_geologies, fill = \"#00000000\", colour = \"darkgrey\") +\n  geom_sf(data = top5, fill = \"#00000000\", colour = \"black\", linewidth = 0.35) +\n  coord_sf(xlim = bb[c(1, 3)], ylim = bb[c(2, 4)]) +\n  guides(fill = \"none\") +\n  annotation_scale(height = unit(0.1, \"cm\")) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nRelative movement cost\nFor the time being a very crude relative movement rate classification assigning any geology including the term till a cost 1.5, any containing the term rock a cost 2, and all other areas (water and ice) a null value -999 has been used. Better choices might be 1.19 (or 1.67) for till and 2.5 for rock per values cited in the documentation of the R movecost package, as follows:\n\n\n\n\n\n\n\nCover\nCost\n\n\n\n\nBlacktop roads, improved dirt paths, cement\n1.00\n\n\nLawn grass\n1.03\n\n\nLoose beach sand\n1.19\n\n\nDisturbed ground (former stone quarry)\n1.24\n\n\nHorse riding path, flat trails and meadows\n1.25\n\n\nTall grassland (with thistle and nettles)\n1.35\n\n\nOpen space above the treeline (i.e., 2000 m asl)\n1.50\n\n\nBad trails, stony outcrops and river beds\n1.67\n\n\nOff-paths\n1.67\n\n\nBog\n1.79\n\n\nOff-path areas below the treeline (pastures, forests, heathland)\n2.00\n\n\nRock\n2.50\n\n\nSwamp, water course\n5.00\n\n\n\nThese choices remain open for discussion and adjustment, in combination with determination of localised hiking functions from the provided GPS data.",
    "crumbs": [
      "Notebooks",
      "Taking stock"
    ]
  },
  {
    "objectID": "notebooks/08-taking-stock.html#current-results-an-example",
    "href": "notebooks/08-taking-stock.html#current-results-an-example",
    "title": "Taking stock",
    "section": "Current results: an example",
    "text": "Current results: an example\nAn example output for the largest area of interest is shown below.\n\nr1 &lt;- st_read(str_glue(\"{data_folder}/dry-valleys/output/dry-valleys-01-32m-betweenness-100-2.gpkg\"))\nr2 &lt;- st_read(str_glue(\"{data_folder}/dry-valleys/output/dry-valleys-01-32m-betweenness-200-2.gpkg\"))\next &lt;- st_read(str_glue(\"{data_folder}/broken-connection-extent.gpkg\"))\n\n\nggplot() +\n  geom_sf(data = r1, aes(colour = cost_vb), size = 0.025) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis result was obtained at nominal resolution of the hiking network 100m. A coarser grained network, nominal resolution 200m yields a similar result\n\nggplot() +\n  geom_sf(data = r2, aes(colour = cost_vb), size = 0.05) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  geom_sf(data = ext, alpha = 0.35) +\n  theme_void()\n\n\n\n\n\n\n\n\nBoth results were obtained with a betweenness centrality calculation cutoff of 2 hours estimated hiking time. The resulting beweenness measures have been rescaled in both cases to a range between 1 and 100 to enable direct comparison. The distribution of the results in each case is similar, although the finer resolution network yields more strongly concentrated outcomes.\n\n\nCode\ndf &lt;- bind_rows(r1, r2) |&gt;\n  mutate(resolution = as.factor(resolution))\n\nggplot() +\n  geom_density(data = df, aes(x = cost_vb, fill = resolution, group = resolution), linewidth = 0, alpha = 0.5) +\n  xlab(\"Rescaled vertex betweenness\") +\n  ylab(\"Relative frequency\")\n\n\n\n\n\n\n\n\n\nHowever, it is worth noting that there are differences on closer inspection. The area highlighted in the second map above is shown below screenshotted from QGIS.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are actually two spots here where the image on the right (200m resolution network) is disconnected relative to the 100m resolution case. Note how the 100m resolution network finds two paths through this region, where the 200m resolution network does not. The second ‘missing’ node is towards the top right of the image.\nIt would not be difficult to fix this by buffering the geology as required, but it may be that such narrow connections are not ‘really’ connections at all…",
    "crumbs": [
      "Notebooks",
      "Taking stock"
    ]
  },
  {
    "objectID": "notebooks/08-taking-stock.html#footnotes",
    "href": "notebooks/08-taking-stock.html#footnotes",
    "title": "Taking stock",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMouginot, J., Scheuchl, B. & Rignot, E. MEaSUREs Antarctic Boundaries for IPY 2007-2009 from Satellite Radar, Version 2. 2017, Distributed by NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/AXE4121732AD. Date Accessed 09-03-2024.↩︎\nHowat, Ian, et al., 2022, “The Reference Elevation Model of Antarctica – Mosaics, Version 2”, https://doi.org/10.7910/DVN/EBW8UC, Harvard Dataverse, V1, Accessed: 28-29 August 2024.↩︎\nCox SC, B Smith Lyttle, S Elkind, CS Smith Siddoway, P Morin, G Capponi, T Abu-Alam, M Ballinger, L Bamber, B Kitchener, L Lelli, JF Mawson, A Millikin, N Dal Seno, L Whitburn, T White, A Burton-Johnson, L Crispini, D Elliot, S Elvevold, JW Goodge, JA Halpin, J Jacobs, E Mikhalsky, AP Martin, F Morgan, J Smellie, P Scadden, and GS Wilson. 2023. The GeoMAP (v.2022-08) continent-wide detailed geological dataset of Antarctica.PANGAEA. doi: 10.1594/PANGAEA.951482.↩︎",
    "crumbs": [
      "Notebooks",
      "Taking stock"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html",
    "href": "notebooks/06-a-hiking-network-with-better-data.html",
    "title": "A hiking network with better data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-09-03\nAdded tentative conclusion pointing to need to explore performance when cutoff is introduced to betweennness calculation.\n\n\n2024-09-01\nCleaned up to show the workflow.\n\n\n2024-08-29\nInitial post.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#tldr-executive-summary",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#tldr-executive-summary",
    "title": "A hiking network with better data",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings progressing the overall project are:\n\nVertex betweenness and edge betweenness maps where it seems clear that the most informative measure is the raw betweenness not any normalised derivative from it.\nImposing cutoff costs on the betweenness calculation is a refinement left for a later notebook, where it seems clear for both theoretical reasons (route planning is performed hour by hour in the field), and practical ones (calculation is MUCH faster) betweenness centrality with imposed cost cutoff settings are the most useful graph analytic output for this project.\n\n\nThe primary purpose of this notebook is to show the overall workflow for making a map of potential impacts from scientists moving around in the Dry Valleys, based on slope, landcover, and hiking function inputs. The workflow is shown with respect to the 5th largest area of contiguous rocky (rocks and/or gravels) cover — an area of about 320 sq.km, at a resolution of 100m.\nMuch of this code is in previous notebooks in similar form, but pointing here to different data resources, and once final inputs (primarily landcover costs and a hiking function) are determined, can be automated to run on several zones across all of the Dry Valleys.\n\n\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\nlibrary(ggspatial)\nlibrary(scales)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#set-up-folders-and-some-base-parameters",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#set-up-folders-and-some-base-parameters",
    "title": "A hiking network with better data",
    "section": "Set up folders and some base parameters",
    "text": "Set up folders and some base parameters\nThe set up of data subfolders under the top level _data folder is:\n\n\n\n\n\n\n\nFolder\nContents\n\n\n\n\ndry-valleys/output\nOutputs - primarily the graph and elevation data (as points) with relative movement costs\n\n\ncontiguous-geologies\nGPKG files dry-valleys-extent-??.gpkg with a single contiguous geology polygon per file\n\n\ndry-valleys/common-inputs\nVarious input layers as needed. Primarily relative landcover costs *-cover-costs.gpkg\n\n\ndem/10m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-10m-dem-clipped.tif\n\n\ndem/32m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-32m-dem-clipped.tif\n\n\n\nAdditionally, the basename for all files is dry-valleys, with DEM resolution used (32m or 10m) and the nominal hiking network resolution (without the m), which is likely to be 100 included in various output files as appropriate.\n\ndem_resolution     &lt;- \"32m\" # 10m also available, but probably not critical\nresolution         &lt;- 200   # 100 is high res for rapid iterating: use 200 while testing\nimpassable_cost    &lt;- -999  # in the cover-costs file. hat-tip to ESRI\n\ndata_folder        &lt;- str_glue(\"{here()}/_data\")\nbase_folder        &lt;- str_glue(\"{data_folder}/dry-valleys\")\ninput_folder       &lt;- str_glue(\"{base_folder}/common-inputs\")\noutput_folder      &lt;- str_glue(\"{base_folder}/output\")\ndem_folder         &lt;- str_glue(\"{data_folder}/dem/{dem_resolution}\")\n\nbasename           &lt;- str_glue(\"dry-valleys\")\ndem_file           &lt;- str_glue(\"{dem_folder}/{basename}-combined-{dem_resolution}-dem-clipped.tif\")\nlandcover_file     &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#load-the-terrain-and-crop-to-the-extent",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#load-the-terrain-and-crop-to-the-extent",
    "title": "A hiking network with better data",
    "section": "Load the terrain and crop to the extent",
    "text": "Load the terrain and crop to the extent\nThe extent in each case is a single area of ‘contiguous geology’ derived by dissolving all polygons in the GNS geomap data. These files are named *01.gpkg, *02.gpkg, etc., where the sequence number is from the largest to the smallest by area.\n\ncontiguous_geology &lt;- \"05\"  # small one for testing\nextent_file        &lt;- str_glue(\"{base_folder}/contiguous-geologies/{basename}-extent-{contiguous_geology}.gpkg\")\nextent             &lt;- st_read(extent_file)\n\nterrain &lt;- rast(dem_file) |&gt;\n  crop(extent |&gt; as(\"SpatVector\")) |&gt;\n  mask(extent |&gt; as(\"SpatVector\"))\n# and for visualization\nshade &lt;- get_hillshade(terrain) |&gt;\n  as.data.frame(xy = TRUE) # this is for plotting in ggplot\n\nIt’s handy to make a couple of basemaps here\n\nhillshade_basemap &lt;- ggplot(shade) +\n  geom_raster(data = shade, aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()\n\n# this is a bit goofy, but seems to work\ncontours &lt;- function(terrain) {\n  stat_contour(data = terrain |&gt; as.data.frame(xy = TRUE) |&gt; rename(z = names(terrain)),\n               aes(x = x, y = y, z = z), binwidth = 100, linewidth = 0.2, colour = \"#ddaa44\")\n}\n\nhillshade_basemap + \n  contours(terrain) +\n  coord_equal()",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#make-the-graph",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#make-the-graph",
    "title": "A hiking network with better data",
    "section": "Make the graph",
    "text": "Make the graph\nNodes in the graph are centres of a hexagonal grid across the extent. The cell size of the hexagonal grid is set so that hexagons are the same area as square raster cells of side length given by resolution. The area of these is \\(r^2\\). The area of a hexagon with ‘radius’ \\(R\\) is \\(3\\sqrt{3}R^2/2\\). Cell size in sf::st_make_grid is specified by the ‘face-to-face distance’ of the hexagons, this distance is given by \\(D=\\sqrt{3}R\\), so \\(R=D/\\sqrt{3}\\). Hence \\[\n\\begin{array}{rcrcl}\nA & = & \\frac{3\\sqrt{3}}{2}\\left(\\frac{D}{\\sqrt{3}}\\right)^2 &=& r^2 \\\\\n& & D^2 & = & \\frac{2}{\\sqrt{3}}r^2 \\\\\n& & & = & r\\sqrt{\\frac{2}{\\sqrt{3}}}\n\\end{array}\n\\] So make a hex grid of points with this spacing.\n\n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- resolution * sqrt(2 / sqrt(3))\n\n# check if we have already made this dataset\nhexgrid_file &lt;- \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-hex-grid-{resolution}.gpkg\")\n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    rename(geometry = x) |&gt;   # shouldsee https://github.com/r-spatial/sf/issues/2429\n    st_set_crs(st_crs(extent))\n  coords &lt;- xy |&gt; st_coordinates()\n  xy &lt;- xy |&gt;\n    mutate(x = coords[, 1], y = coords[, 2])\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nReading layer `dry-valleys-05-32m-hex-grid-200' from data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/dry-valleys/output/dry-valleys-05-32m-hex-grid-200.gpkg' \n  using driver `GPKG'\nSimple feature collection with 20777 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 424601.4 ymin: -1262612 xmax: 452755.2 ymax: -1233391\nProjected CRS: WGS 84 / Antarctic Polar Stereographic\n\n\nAttach elevation values to it from the terrain data.\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n\npts &lt;- xy |&gt;\n  mutate(z = z[, 2])\n\nAlso attach land cover relative movement costs, removing any NAs that might have been picked up along the way. These will predominantly be due to minor mismatches between the extent of the raster terrain layer and the coverage of the hexagonal grid of points. We also remove any points whose cost is equal to the sentinel value (-999) for impassable terrain (water and ice).\n\n# remove any NAs that might result from interpolation or impassable terrain\n# NAs tend to occur around the edge of study area\ncover &lt;- st_read(landcover_file)\n\npts &lt;- pts |&gt;\n  st_join(cover) |&gt;\n  filter(!is.na(z), cost != impassable_cost)\n\nMapping it we can see that more difficult terrain is associated with the higher elevations (which are rocky).\n\nhillshade_basemap +\n  geom_sf(data = pts, aes(colour = cost), size = 0.05) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  contours(terrain) +\n  theme_void()\n\n\n\n\n\n\n\n\nNow we make the graph by connecting pairs of points within range of one another (note 1.1 factor to catch floating-point near misses).\n\ngraph_file &lt;-   \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-graph-hex-{resolution}.txt\") \n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz = pts, impedances = pts$cost)",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#now-we-have-a-graph",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#now-we-have-a-graph",
    "title": "A hiking network with better data",
    "section": "Now we have a graph",
    "text": "Now we have a graph\nSo let’s take a look… first, at a map of the network for reassurance - noting that since costs are different in each direction there’s no easy way to show this unambiguously\n\nhillshade_basemap +\n  geom_sf(data = G |&gt; get_graph_as_line_layer(), \n          aes(colour = log(cost)), linewidth = 0.05) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  theme_void()\n\n\n\n\n\n\n\n\nAnd just for assurance, here’s an estimated travel time map. These are actually tricky to make because\n\n# pick a random point as origin point for some examples\norigin_i &lt;- sample(seq_along(pts$geom), 1)[1]\norigin &lt;- c(pts$x[origin_i], pts$y[origin_i])\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), \n                           weights = edge_attr(G, \"cost\")) |&gt; t()\ndf &lt;- vertex.attributes(G) |&gt; as.data.frame()\n\nhillshade_basemap +\n  geom_point(data = df, aes(x = x, y = y, colour = time_hrs), size = 0.25) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  annotate(\"point\", x = origin[1], y = origin[2], pch = 4) +\n  coord_equal() \n\n\n\n\n\n\n\n\nNote the following, if we want the estimated travel times as a raster layer rather than a point dataset, where we use Sibley’s natural neighbours interpolation method from whitebox_tools to interpolate to a raster from the hex grid distributed points.\n[NOT RUN]\n# filenames for (temporary) shapefile and TIF outputs that we need \nshp_fname &lt;- str_glue(\"{here()}/_temp/test-{format(origin_i, scientific = FALSE)}-hex.shp\")\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\n\n# write the df out to a shapefile, which Whitebox Tools needs\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# do the interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\nwbt_natural_neighbour_interpolation(\n  shp_fname, output = tif_fname, field = \"time_hrs\", base = str_glue(dem_file))\n\nrast(tif_fname) |&gt;\n  mask(extent |&gt; as(\"SpatVector\")) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_viridis_c(option = \"A\", direction = -1, name = \"Est. time hrs\") +\n  coord_equal() +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#shortest-path-tree",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#shortest-path-tree",
    "title": "A hiking network with better data",
    "section": "Shortest path tree",
    "text": "Shortest path tree\nThis is more for fun that useful – although given a set of heavily used sites (base camps?) their shortest path trees might usefully be combined to give a general idea of most at-risk / heavily travelled areas. Note that shortest_path_tree is a function in raster-graph-functions.R not an igraph built-in.\n\nSPT &lt;- G |&gt;\n  get_shortest_path_tree(origin_i) |&gt;\n  get_graph_as_line_layer()\n\n\nggplot() + \n  contours(terrain) +\n  geom_sf(data = SPT, linewidth = 0.1) +\n  geom_sf(data = pts |&gt; slice(origin_i), pch = 4) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows all the potential most efficient pathways out from the root vertex to every other vertex. There is a strong tendency for these paths to follow contours.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#betweenness-centralities",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#betweenness-centralities",
    "title": "A hiking network with better data",
    "section": "Betweenness centralities",
    "text": "Betweenness centralities\nIt turns out we can count all the appearances of graph edges and/or vertices in all the shortest paths among all the vertices in a network. These are graph centrality measures termed edge betweenness and vertex betweenness (the latter is often simply betweenness).\nIf we use these in their ‘raw’ form then the most geographically central spots in a study area may have the highest values by virtue of their location in the middle of the area. This might not be wrong — not exactly anyway — but it might lead to ignoring relatively vulnerable areas that may be heavily travelled in spite of more peripheral locations because they are at bottlenecks between parts of the study area. For this reason we calculate a base- and cost- betweenness scores. The former is the betweenness if the cost of traversing every edge in the graph was equal’ the latter is when slope and terrain are taken into account.\n\n# vertices\n# add 1 to these to make it easier to scale them if needed because a raw value 0 is possible\nbase_vb &lt;- betweenness(G)\ncost_vb &lt;- betweenness(G, weights = E(G)$cost)\nbase_vb &lt;- rescale(base_vb, to = c(1, 100))\ncost_vb &lt;- rescale(cost_vb, to = c(1, 100))\ndiff_vb &lt;- (cost_vb - base_vb) / (cost_vb + base_vb)\nrel_vb &lt;- cost_vb / base_vb\nlog_rel_vb &lt;- log(rel_vb)\n\n\n# edges\nbase_eb &lt;- edge_betweenness(G)\ncost_eb &lt;- edge_betweenness(G, weights = E(G)$cost)\nbase_eb &lt;- rescale(base_eb, to = c(1, 100))\ncost_eb &lt;- rescale(cost_eb, to = c(1, 100))\ndiff_eb &lt;- (cost_eb - base_eb) / (cost_eb + base_eb)\nrel_eb &lt;- cost_eb / base_eb\nlog_rel_eb &lt;- log(rel_eb, 10)\n\nAdd all these results to the relevant graph attributes.\n\nGsp &lt;- G |&gt; \n  set_vertex_attr(\"base_vb\", value = base_vb) |&gt;\n  set_vertex_attr(\"cost_vb\", value = cost_vb) |&gt;\n  set_vertex_attr(\"diff_vb\", value = diff_vb) |&gt;\n  set_vertex_attr(\"rel_vb\", value = rel_vb) |&gt;\n  set_vertex_attr(\"log_rel_vb\", value = log_rel_vb)|&gt; \n  set_edge_attr(\"base_eb\", value = base_eb) |&gt;\n  set_edge_attr(\"cost_eb\", value = cost_eb) |&gt;\n  set_edge_attr(\"diff_eb\", value = diff_eb) |&gt;\n  set_edge_attr(\"rel_eb\", value = rel_eb) |&gt;\n  set_edge_attr(\"log_rel_eb\", value = log_rel_eb)",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#maps-of-betweenness-centrality",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#maps-of-betweenness-centrality",
    "title": "A hiking network with better data",
    "section": "Maps of betweenness centrality",
    "text": "Maps of betweenness centrality\nFirst vertex centrality, where it is relatively simple to colour vertices by their centrality. It is useful to map all the various calculated betweenness measures to get a feel for which may be most useful.\n\nm1 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = base_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm2 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = cost_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm3 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = diff_vb), size = 0.02) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm4 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = rel_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\nm5 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(),\n          aes(colour = log_rel_vb), size = 0.02) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n  \nggarrange(plotlist = list(m1, m2, m3, m4, m5), ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nExamining these results, it appears that the the simple cost_vb betweenness may actually be the most useful in revealing routes likely to be well trafficked. The diff_vb output also appears likely to be useful.\nEdge centrality is trickier to map (because of how the linewidth aesthetic works).\n\nm1 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt;\n            mutate(lwd = base_eb / max(E(Gsp)$base_eb)),\n          aes(colour = base_eb, linewidth = lwd)) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  scale_linewidth_identity()\n\nm2 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt;\n            mutate(lwd = cost_eb / max(E(Gsp)$cost_eb)),\n          aes(colour = cost_eb, linewidth = lwd)) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  scale_linewidth_identity()\n\nm3 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = diff_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm4 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = rel_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n\nm5 &lt;- hillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer(),\n          aes(colour = log_rel_eb), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"RdBu\") \n  \nggarrange(plotlist = list(m1, m2, m3, m4, m5), ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nHere, it seems clear that the cost_eb is likely to be the most useful output.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/06-a-hiking-network-with-better-data.html#conclusions",
    "href": "notebooks/06-a-hiking-network-with-better-data.html#conclusions",
    "title": "A hiking network with better data",
    "section": "Conclusions",
    "text": "Conclusions\nTentatively, I conclude from the above that the most useful outputs are the vertex cost-based betweenness and difference outputs. The edge betweenness results are similar, harder to map, and double the computation times without adding much.\nThe next step is to investigate the impact on computation times of imposing a ‘cutoff’ time on vertex betweenness measures. Some initial exploration suggests that this greatly speeds up computation, again, without necessarily leading to very different conclusions concerning likely areas of high impact.",
    "crumbs": [
      "Notebooks",
      "A hiking network with better data"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html",
    "href": "notebooks/03-estimating-a-hiking-function.html",
    "title": "Estimating a hiking function",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-07-22\nAdded variables to hex bin filter method.\n\n\n2024-07-19\nAdded nlsLM estimation of models.\n\n\n2024-07-17\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#tldr-executive-summary",
    "href": "notebooks/03-estimating-a-hiking-function.html#tldr-executive-summary",
    "title": "Estimating a hiking function",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThis notebook provides a useful overview of the process of estimating a hiking function from the GPS data but IS SUPERSEDED by this notebook.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(here)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(minpack.lm)\n\n\nsrc_folder &lt;- str_glue(\"{here()}/_data/GPS-1516Season-MiersValley\")\ntgt_folder &lt;- str_glue(\"{here()}/_data/cleaned-gps-data\")\nall_traces &lt;- st_read(str_glue(\"{tgt_folder}/all-gps-traces.gpkg\"))\n\nReading layer `all-gps-traces' from data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/cleaned-gps-data/all-gps-traces.gpkg' \n  using driver `GPKG'\nSimple feature collection with 112216 features and 26 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 352183.3 ymin: -1254631 xmax: 364874.4 ymax: -1240066\nProjected CRS: WGS 84 / Antarctic Polar Stereographic",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#now-look-at-the-slope-speed-plots",
    "href": "notebooks/03-estimating-a-hiking-function.html#now-look-at-the-slope-speed-plots",
    "title": "Estimating a hiking function",
    "section": "Now look at the slope-speed plots",
    "text": "Now look at the slope-speed plots\nIf we first make a crude estimate using geom_smooth on all the data we can start to appreciate the challenge\n\nggplot(all_traces, aes(x = slope_h, y = speed_km_h)) +\n  geom_point(alpha = 0.25, size = 0.5) +\n  geom_smooth() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nWhile the result is somewhat bell-shaped as we might hope, there are a lot of fixes that seem unlikely to be related to movement from place to place, rather they are ‘milling about’ in place - such as at basecamp, at experiment sites, at rest stops, etc.\nWe can of course fit a curve of desired functional form. In the relevant literature, there are essentially two functional forms, exponential, in Waldo Tobler’s original formulation1, given by \\[\nv=6e^{-3.5\\|{s+0.05}\\|}\n\\] which gives hiking speed \\(v\\) in km/h for a slope (given as rise over run) \\(s\\). This function has subsequently modified by others.2\nOther work3 yields speeds essentially Gaussian with respect to slope centred at slight downward slopes, i.e.\n\\[\nv = ae^{-b(s+c)^2}\n\\]\nWe can explicitly fit such functional forms to the data using the nlsLM function from the minpack.lm package:\n\n# Tobler-like\nmod_tobler &lt;- nlsLM(speed_km_h ~ a * exp(-b * abs(slope_h + c)), data = all_traces,\n                    start = c(a = 5, b = 3, c = 0.05))\n# Gaussian\nmod_gauss &lt;- nlsLM(speed_km_h ~ a * dnorm(slope_h, m, s), data = all_traces,\n                   start = c(a = 5, m = 0, s = 0.5))\n# Student's t (for heavier tails)\nmod_students_t &lt;- nlsLM(speed_km_h ~ a * dt((slope_h + m), d), data = all_traces,\n                   start = c(a = 5, m = 0, d = 0.001))\n\nNote that we use the probability density functions dnorm and dt here for convenience of parameterisation. It is easier to revert to base R plotting to get a feel for what these look like:\n\n# a non spatial version of the data for plotting - which it is convenient to \ngps_data &lt;- all_traces |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\ns &lt;- -200:200 / 100\nslopes &lt;- data.frame(slope_h = s)\nplot(gps_data, col = \"lightgray\", cex = 0.5)\nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nThat’s the process… but we need to apply it to filtered data to get closer to a reasonable outcome. Wrap the model-building in convenience functions:\n\nget_tobler_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * exp(-b * abs(slope_h + c)), data = df,\n        start = c(a = 5, b = 3, c = 0.05))  \n}\n\nget_gaussian_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dnorm(slope_h, m, s), data = df,\n        start = c(a = 5, m = 0, s = 0.5))\n}\n\nget_students_t_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dt((slope_h + m), d), data = df,\n        start = c(a = 5, m = 0, d = 0.001))\n}",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#cleaning-the-data-to-get-a-better-hiking-function",
    "href": "notebooks/03-estimating-a-hiking-function.html#cleaning-the-data-to-get-a-better-hiking-function",
    "title": "Estimating a hiking function",
    "section": "Cleaning the data to get a better hiking function",
    "text": "Cleaning the data to get a better hiking function\nThere are a number of features in the data that might allow us to exclude such data from consideration. We examine three below:\n\nOnly consider an ‘upper bound’ on the distribution, say the 90th percentile of speed_km_h relative to a given slope\nRemove fixes where the density of fixes is high since these may relate to base camp sites, etc.\nRemove fixes with high turn angles, since these may relate to where the data suggest a lot of ‘milling about’\n\nWe look at each of these below.\n\nUpper bound of the distribution\nIf we only consider speed estimates at or close to the upper bound of the data, then we are removing all the milling about data:\n\nd90 &lt;- gps_data |&gt;\n  mutate(slope_h = round(slope_h, 2)) |&gt;\n  group_by(slope_h) |&gt;\n  summarise(speed_km_h = quantile(speed_km_h, 0.9)) |&gt;\n  ungroup()\n\nmod_tobler &lt;- get_tobler_hiking_function(d90)\nmod_gauss &lt;- get_gaussian_hiking_function(d90)\nmod_students_t &lt;- get_students_t_hiking_function(d90)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from 90th percentile speeds\")\npoints(d90, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nBased on residual sum-of-squares, the Tobler-like function is the best fit in this case (although the differences are small). In this particular case, the paramterisation is strikingly similar to Tobler’s\n\nsummary(mod_tobler)\n\n\nFormula: speed_km_h ~ a * exp(-b * abs(slope_h + c))\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 5.950260   0.141452  42.065  &lt; 2e-16 ***\nb 3.399181   0.114934  29.575  &lt; 2e-16 ***\nc 0.014756   0.004926   2.995  0.00302 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5403 on 249 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.49e-08\n\n\nGiving us \\(v = 5.95e^{-3.39\\|s+0.0147\\|}\\), which only differs much from Tobler’s proposed formula by the offset from 0 slope, which is reduced from 0.05 to 0.0147.\nNote that by choosing a different quantile than the 90th percentile we can vary the speeds predicted by a model produced in this way.\n\n\nFiltering by densely travelled locations\nThe idea here is that densely travelled locations are near base camps and similar sites, and not where scientists are ‘on the move’. The approach below uses hex binning to find densely trafficked areas and exclude fixes in those areas.\n\nhexes &lt;- all_traces |&gt;\n  st_make_grid(cellsize = 60, square = FALSE, what = \"polygons\") |&gt;\n  st_as_sf(crs = st_crs(all_traces)) |&gt;\n  rename(geom = x) |&gt;\n  st_join(all_traces |&gt; mutate(id = row_number()), left = FALSE) |&gt;\n  group_by(geom) |&gt;\n  summarise(n = n(), n_persons = n_distinct(name)) |&gt;\n  select(n, n_persons) |&gt; \n  ungroup() |&gt;\n  mutate(density = n / n_persons)\n\nMap with tmap (greater flexibility on the classification scheme)\n\ntm_shape(hexes |&gt; pivot_longer(cols = c(n, n_persons, density))) +\n  tm_fill(col = \"value\", style = \"quantile\", n = 10, palette = \"-magma\") +\n  tm_facets(by = \"name\", free.scales = TRUE)\n\n\n\n\n\n\n\n\nBut where to cut off the data? Here are boxplots (note log scales) of the number of fixes n, the number of persons, n_persons, and the number of fixes per person, density, associated with each hex.\n\nhexes |&gt; \n  pivot_longer(cols = c(n, n_persons, density)) |&gt;\n  ggplot() +\n  geom_boxplot(aes(y = value)) +\n  scale_y_log10() +\n  facet_wrap( ~ name, scales = \"free_y\", ncol = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis doesn’t really help! … and it’s not entirely clear that this yields better results!\n\nd_hexes &lt;- all_traces |&gt;\n  st_join(hexes) |&gt; \n  filter(n &lt; 26) |&gt;\n  filter(density &lt; 7.5) |&gt; \n  filter(n_persons &lt; 5) |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\nmod_tobler &lt;- get_tobler_hiking_function(d_hexes)\nmod_gauss &lt;- get_gaussian_hiking_function(d_hexes)\nmod_students_t &lt;- get_students_t_hiking_function(d_hexes)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from less densely trafficked areas\")\npoints(d_hexes, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\n\n\nRemoving fixes with large turn angles\nAgain, this is a way to potentially remove data where there is a lot of milling about. As with the density approach, it’s hard to know what is a sensible cutoff. A map of the fixes coloured by turn angle is not necessarily much help:\n\nggplot(all_traces) +\n  geom_sf(aes(colour = turn_angle), size = 0.25)\n\n\n\n\n\n\n\n\nIn fact it seems that a rather aggressive seeming filter is needed before base camp and similar areas ‘show through’:\n\nggplot(all_traces |&gt; filter(turn_angle &lt; 5)) +\n  geom_sf(size = 0.2)\n\n\n\n\n\n\n\n\n\nd_low_turn_angle &lt;- all_traces |&gt;\n  filter(turn_angle &lt; 5) |&gt;\n  st_drop_geometry() |&gt;\n  select(slope_h, speed_km_h)\n\nmod_tobler &lt;- get_tobler_hiking_function(d_low_turn_angle)\nmod_gauss &lt;- get_gaussian_hiking_function(d_low_turn_angle)\nmod_students_t &lt;- get_students_t_hiking_function(d_low_turn_angle)\n\nplot(gps_data, col = \"lightgray\", cex = 0.5,\n     main = \"Hiking functions from fixes with low turn angles\")\npoints(d_low_turn_angle, cex = 0.5) \nlines(s, predict(mod_tobler, slopes), col = \"red\", lwd = 2)\nlines(s, predict(mod_gauss, slopes), col = \"blue\", lwd = 2)\nlines(s, predict(mod_students_t, slopes), col = \"forestgreen\", lwd = 2)\nlegend(\"topright\", legend = c(\"Tobler\", \"Gaussian\", \"Student's t\"), \n       col = c(\"red\", \"blue\", \"forestgreen\"), lwd = 2, lty = 1)",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#conclusion",
    "href": "notebooks/03-estimating-a-hiking-function.html#conclusion",
    "title": "Estimating a hiking function",
    "section": "Conclusion",
    "text": "Conclusion\nThis is still a work in progress…",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/03-estimating-a-hiking-function.html#footnotes",
    "href": "notebooks/03-estimating-a-hiking-function.html#footnotes",
    "title": "Estimating a hiking function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTobler WR. 1993. Three Presentations on Geographical Analysis and Modeling: Non-Isotropic Geographic Modeling; Speculations on the Geometry of Geography; and Global Spatial Analysis. Technical Report 93–1. NCGIA Technical Reports. Santa Barbara, CA: National Center for Geographic Information and Analysis.↩︎\nSee, for example, Márquez-Pérez J, I Vallejo-Villalta and JI Álvarez-Francoso. 2017. Estimated travel time for walking trails in natural areas. Geografisk Tidsskrift-Danish Journal of Geography 117(1) 53–62. doi: 10.1080/00167223.2017.1316212↩︎\nIrmischer IJ and KC Clarke. 2018. Measuring and modeling the speed of human navigation. Cartography and Geographic Information Science 45(2). 177–186. doi: 10.1080/15230406.2017.1292150↩︎",
    "crumbs": [
      "Notebooks",
      "Estimating a hiking function"
    ]
  },
  {
    "objectID": "notebooks/07-compute-times-of-hiking-network-analysis.html",
    "href": "notebooks/07-compute-times-of-hiking-network-analysis.html",
    "title": "Compute times of hiking network analysis",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-09-04\nAdded quick analysis of timings.\n\n\n2024-08-29\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Compute times of hiking network analysis"
    ]
  },
  {
    "objectID": "notebooks/07-compute-times-of-hiking-network-analysis.html#tldr-executive-summary",
    "href": "notebooks/07-compute-times-of-hiking-network-analysis.html#tldr-executive-summary",
    "title": "Compute times of hiking network analysis",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings are:\n\nAs expected calculation of betweenness times increases with the square of the network size (in number of nodes), hence with the 4th power of the resolution.\nThis combinatorial explosion (it’s polynomial, but not in a good way…) means that restricting betweenness calculations using a cost cutoff is desirable. For a given cutoff calculation time is linear in the network size, hence it grows with the square of the resolution. For the landscape sub-areas under consideration calculation of everywhere-to-everywhere betweenness on 100m nominal resolution networks takes several hours on a stock machine (M2 Mac mini) whereas betweenness calculations with 2 or 3 hour cutoffs take manageable 10-20 minute times.\n\nThis notebook explores the impact of varying the resolution of the hiking network and cutoff times on computation times.\n\n\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\nlibrary(ggspatial)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\n\nMost of this code is in the previous notebook just wrapped in a couple of loops.\nSetup a bunch of folders and base data sets that are unchanged from run to run.\n\ndem_resolution     &lt;- \"32m\" # 10m also available, but probably not critical\nimpassable_cost    &lt;- -999  # in the cover-costs file. hat-tip to ESRI\ndata_folder        &lt;- str_glue(\"{here()}/_data\")\nbase_folder        &lt;- str_glue(\"{data_folder}/dry-valleys\")\ninput_folder       &lt;- str_glue(\"{base_folder}/common-inputs\")\noutput_folder      &lt;- str_glue(\"{base_folder}/output\")\nbasename           &lt;- str_glue(\"dry-valleys\")\ncontiguous_geology &lt;- \"05\"  # use small one '05' for testing\n\ndem_folder         &lt;- str_glue(\"{data_folder}/dem/{dem_resolution}\")\ndem_file &lt;- str_glue(\"{dem_folder}/{basename}-combined-{dem_resolution}-dem-clipped.tif\")\nextent_file        &lt;- str_glue(\"{base_folder}/contiguous-geologies/{basename}-extent-{contiguous_geology}.gpkg\")\nextent             &lt;- st_read(extent_file)\n\nterrain &lt;- rast(dem_file) |&gt;\n  crop(extent |&gt; as(\"SpatVector\")) |&gt;\n  mask(extent |&gt; as(\"SpatVector\"))\nshade &lt;- get_hillshade(terrain) |&gt;\n  as.data.frame(xy = TRUE) # this is for plotting in ggplot\n\nhillshade_basemap  &lt;- ggplot(shade) +\n  geom_raster(data = shade, aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()\n\nlandcover_file     &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")\ncover              &lt;- st_read(landcover_file)\n\n\nresolutions &lt;- c(100, 200)\ncutoffs     &lt;- c(1:5, -1)\n\nresults &lt;- list()\nindex &lt;- 1\nfor (resolution in resolutions) {\n  hex_cell_spacing &lt;- resolution * sqrt(2 / sqrt(3))\n  # check if we have already made this dataset\n  hexgrid_file &lt;- \n    str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-hex-grid-{resolution}.gpkg\")\n  if (file.exists(hexgrid_file)) {\n    xy &lt;- st_read(hexgrid_file)\n  } else {\n    xy  &lt;- extent |&gt;\n      st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n      st_as_sf() |&gt;\n      rename(geometry = x) |&gt;   # shouldsee https://github.com/r-spatial/sf/issues/2429\n      st_set_crs(st_crs(extent))\n    coords &lt;- xy |&gt; st_coordinates()\n    xy &lt;- xy |&gt;\n      mutate(x = coords[, 1], y = coords[, 2])\n    xy |&gt;\n      st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n  }\n  z &lt;- terrain |&gt; terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n  pts &lt;- xy |&gt;\n    mutate(z = z[, 2]) |&gt;\n    st_join(cover) |&gt;\n    filter(!is.na(z), cost != impassable_cost)\n  graph_file &lt;-   \n    str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-graph-hex-{resolution}.txt\") \n  if (file.exists(graph_file)) {\n    G &lt;- read_graph(graph_file) \n  } else {\n    G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n    write_graph(G, graph_file)\n  }\n  \n  # note... for now this uses a default Tobler hiking function\n  G &lt;- assign_movement_variables_to_graph(G, xyz = pts, impedances = pts$cost)\n\n  for (cutoff in cutoffs) {\n    vb_base_time &lt;- system.time(base_vb &lt;- betweenness(G, cutoff = cutoff))\n    vb_cost_time &lt;- system.time(cost_vb &lt;- betweenness(G, weights = E(G)$cost, cutoff = cutoff))\n    base_vb    &lt;- scales::rescale(base_vb, to = c(1, 100))\n    cost_vb    &lt;- scales::rescale(cost_vb, to = c(1, 100))\n    diff_vb    &lt;- (cost_vb - base_vb) / (cost_vb + base_vb)\n    rel_vb     &lt;- cost_vb / base_vb\n    log_rel_vb &lt;- log(rel_vb)\n    \n    Gsp &lt;- G |&gt; \n      set_vertex_attr(\"base_vb\", value = base_vb) |&gt;\n      set_vertex_attr(\"cost_vb\", value = cost_vb) |&gt;\n      set_vertex_attr(\"diff_vb\", value = diff_vb) |&gt;\n      set_vertex_attr(\"rel_vb\", value = rel_vb) |&gt;\n      set_vertex_attr(\"log_rel_vb\", value = log_rel_vb)\n    \n    results[[index]] &lt;- vertex.attributes(Gsp) |&gt; \n      as.data.frame() |&gt;\n      mutate(resolution = resolution, cutoff = cutoff,\n             vb_base_time = vb_base_time[3], \n             vb_cost_time = vb_cost_time[3])\n    index &lt;- index + 1\n  }\n}\nresults &lt;- bind_rows(results)\n\n\nresults &lt;- results |&gt; \n  mutate(resolution = as.factor(resolution),\n         cutoff = as.factor(cutoff)) |&gt;\n  mutate(cutoff = if_else(cutoff == -1, \"None\", cutoff))\n\n\nhillshade_basemap + \n  geom_point(data = results |&gt; filter(resolution == 100), \n             aes(x = x, y = y, colour = cost_vb), size = 0.09) + \n  scale_colour_viridis_c(option = \"A\", direction = -1) + \n  coord_equal() + \n  facet_wrap( ~ cutoff, nrow = 3, labeller = label_both) +\n  ggtitle(\"Hiking network nominal resolution: 100m\")\n\n\n\n\n\n\n\n\n\nhillshade_basemap + \n  geom_point(data = results |&gt; filter(resolution == 200), \n             aes(x = x, y = y, colour = cost_vb), size = 0.2) + \n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  coord_equal() + \n  facet_wrap( ~ cutoff, nrow = 3, labeller = label_both) +\n  ggtitle(\"Hiking network nominal resolution: 200m\")\n\n\n\n\n\n\n\n\nExport these results for interactive examination in QGIS.\n[NOT RUN]\nfor (c in cutoffs) {\n  for (r in resolutions) {\n    results |&gt; filter(cutoff == c, resolution == r) |&gt;\n      st_as_sf(coords = c(\"x\", \"y\")) |&gt; \n      st_set_crs(st_crs(extent)) |&gt; \n      st_write(\n        str_glue(\"{output_folder}/{basename}-{contiguous_geology}-{dem_resolution}-betweenness-{r}-{c}.gpkg\"),\n        delete_dsn = TRUE)\n  }\n}",
    "crumbs": [
      "Notebooks",
      "Compute times of hiking network analysis"
    ]
  },
  {
    "objectID": "notebooks/07-compute-times-of-hiking-network-analysis.html#timings",
    "href": "notebooks/07-compute-times-of-hiking-network-analysis.html#timings",
    "title": "Compute times of hiking network analysis",
    "section": "Timings",
    "text": "Timings\nThe timing for this small area of interest at these resolutions and cutoff times are tabulated below.\n\ndf &lt;- results |&gt; \n  select(resolution, vb_base_time, vb_cost_time, cutoff) |&gt;\n  distinct() |&gt;\n  pivot_longer(cols = c(vb_base_time, vb_cost_time))\n\nggplot(df) +\n  geom_col(aes(x = cutoff, y = value, group = resolution, fill = resolution), position = \"dodge\") +\n  xlab(\"Betweenness calculation est. hiking time cutoff\") +\n  ylab(\"Time, sec\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis confirms that the time to perform betweenness centrality estimation increases with the square of the number of vertices in the network. The finer resolution network (100m) has four times as many nodes, and analysis takes around 16 times longer (&gt;15 seconds vs ~ 1 second in the cutoff = 5 case).\nWhen no cutoff time is used, the betweenness calculation takes much longer. It is clear from the maps above that the no cutoff case identifies ‘arterials’ although it is unclear that this is the most relevant concern for expected environmental impacts.\nFurther analysis of larger networks (the networks for the largest contiguous region are about 10 times larger than this one) confirms this result, with comparable analyses taking ~100 times longer, which on the computer used (an M2 Mac Mini) translates to several hours compute time for the most extensive betweenness estimation at 100m resolution.\nThese results suggest that it is important to settle on a resolution and cutoff distance judged appropriate for ongoing work to avoid costly repetition of sometimes time-consuming analysis. It is almost certainly preferable to apply some cutoff estimated hiking time to the calculation in any case.",
    "crumbs": [
      "Notebooks",
      "Compute times of hiking network analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Science impacts on Antarctica",
    "section": "",
    "text": "A simple website to share ongoing project work on possible impacts of science in Antarctica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview / plan\n\n\nAntarctica Dry Valleys vulnerability to human impacts\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExploring GPS data\n\n\nChecking over GPS data for accuracy of speed, etc.\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning GPS data\n\n\nAssembling provided GPS data into cleaned files, and a single large dataset\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating a hiking function\n\n\nA work in progress…\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a hiking network\n\n\nAlso a work in progress…\n\n\n\n\n\n\n\n\nAug 6, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExploring hiking network options\n\n\nAlso a work in progress…\n\n\n\n\n\n\n\n\nAug 7, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA hiking network with better data\n\n\nHoming in on the most useful graph analysis outputs\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nCompute times of hiking network analysis\n\n\nVarying resolution and cutoff times in centrality analysis\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nTaking stock\n\n\nProgress to 4 September, 2024\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nRefining hiking functions\n\n\nRevisiting hiking function estimation with more data\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "paper/paper.html#introduction",
    "href": "paper/paper.html#introduction",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "1 Introduction",
    "text": "1 Introduction\nFRASER TO FRAME / CLARIFY MOTIVATION\nOverview of Antarctic science: when and where, its intensity etc. Background on international treaties, etc.\nRelevant findings as to human impacts in Antarctica. Note that in this environment even ‘leave only footprints’ is likely impacting the environment in significant ways.\nOverview of sections ahead."
  },
  {
    "objectID": "paper/paper.html#our-approach-and-related-work",
    "href": "paper/paper.html#our-approach-and-related-work",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "2 Our approach and related work",
    "text": "2 Our approach and related work\nWe chose to explore the question of where human impacts are likely to be strongest using an approach closely related to work on patterns of human movement in archaeology (Verhagen et al. 2019) where likely and potential movement paths of humans across landscapes have been used to infer the settlement structure and human geography of large-scale landscapes. Closely related work in biology investigates the structure and geography of animal transportation networks (Perna and Latty 2014). Both approaches rely on the idea that humans or animals move around in an environment in time or energy efficient ways. These approaches rely on hiking functions that relate speed of movement across a terrain to its slope.\nHiking functions must be applied in some context where locations across a landscape are connected to one another. Because hiking functions are asymmetric, with estimated speed of movement up slopes different than estimated speeds down the same slope, landscape must be represented in a way that allows for this asymmetry. We therefore represent terrain in our landscapes as directed graphs (or network) of connections between locations regularly distributed in planimetric space across the landscape of interest. Because the graphs are directed the costs associated with movement between two locations can be different depending on the direction of movement. Additionally, we associate with locations (i.e., vertices in the graph) the ground cover at the location, which also affects the speed at which it can be traversed. Because the ground cover in the Antarctic environments under study can be broadly categorised into only two types, moraine and rock, we use the ground cover of a location to switch between two estimated hiking functions, rather than the more widely used approach of penalising movement on different ground covers by applying cost factors. We consider previous work on hiking functions and directed graphs in more detail below.\n\n2.1 Hiking functions\nPrisner and Sui (2023) provide an overview of a variety of functions that have been used to model how hiking times and speeds vary with terrain slope. They consider longstanding rules of thumb (Naismith 1892), and later modifications thereof (Langmuir 1984), along with more recent such guidance from the Swiss and German Alpine Clubs (Winkler et al. 2010; Deutscher und Östereichischer Alpenverein 2011). These functions estimate the time taken to travel 1km, referred to as pace, based on slope expressed as rise over run, that is change in elevation divided by horizontal distance. They are all piecewise functions with sharp changes in estimated pace at specific slopes.\nAlongside these hiking pace functions Prisner and Sui (2023) also present hiking speed functions (generally referred to as simply hiking functions) from Tobler (1993 generally considered the first hiking function) and more recent, related but more firmly empirically grounded alternatives offered by Márquez-Pérez et al. (2017), Irmischer and Clarke (2018), and Campbell et al. (2019). Another hiking function not discussed by Prisner and Sui (2023) is presented by Rees (2004). These hiking functions are all continuous in the slope of the terrain so that \\(v=f(\\theta)\\), where \\(v\\) is the speed, and \\(\\theta\\) is the slope. They can all be parameterised to control the maximum speed attainable, the slope at which maximum speed is attained (expected to be a shallow downhill slope), and the rate at which speed falls off with increasing slope.\nThe functional forms of some hiking functions are shown in Table 1 and graphed in Figure 1.\n\n\n\nTable 1: Functional forms of hiking functions\n\n\n\n\n\n\n\n\n\n\nDescription\nEquation\nExamples\n\n\n\n\nExponential\n\\(ae^{b\\left|\\theta-c\\right|}\\)\nTobler (1993), Márquez-Pérez et al. (2017)\n\n\nGaussian\n\\(ae^{-b(\\theta-c)^2}\\)\nIrmischer and Clarke (2018), Campbell et al. (2019)\n\n\nLorentz\n\\(\\frac{a}{\\left[b+d(\\theta-c)^2\\right]}\\)\nCampbell et al. (2019)\n\n\nQuadratic\n\\(a+b\\theta+c\\theta^2\\)\nRees (2004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example hiking functional forms: Exponential (Tobler 1993), Gaussian (Irmischer and Clarke 2018), Lorentz (Campbell et al. 2019), and Quadratic (Rees 2004).\n\n\n\n\n\nThe parameterisations of the functions in Figure 1 have been chosen for illustrative purposes only, although the parameter values for the exponential function shown are such that \\(v=6e^{-3.5\\left|\\theta+0.05\\right|}\\) as suggested in Tobler (1993). These parameters give Tobler’s hiking function but, as has been noted elsewhere, (see Herzog 2010; Campbell et al. 2019), are based on a poorly specified fit to secondary data on hiking speeds found in Imhof (1950, 217–220). Nevertheless, these parameter values are widely applied in the literature.\nAll these hiking functional forms are somewhat ad hoc. They exhibit desirable, expected properties: a peak speed at a slope near zero, which we expect to be slightly negative (i.e., downhill), and continuously falling speeds as the slope deviates from the slope of peak speed. However, there is no theoretical basis for the specific functional forms listed in Table 1. More principled approaches might be developed based on the literature on the physiology of human movement (see, e.g., Minetti et al. 2002). In general, approaches based on minimising energy use yield similar results to empirical speed-slope functions, although it is worth noting that they more reliably generate zig-zag or ‘switchback’ movement behaviour on steep slopes (Llobera and Sluckin 2007). However, these are hard to implement, and we have adopted empirically-derived and locally-specific hiking functions following Márquez-Pérez et al. (2017), Irmischer and Clarke (2018), and Campbell et al. (2019). This choice is based on available data and the goals of our study, where the relative cost of different potential routes in the landscape is more important than exact prediction of routes. In practice, almost any function with the peaked form of those shown in Figure 1 is suited to our requirements.\nIt is commonplace in many applications to also incorporate a penalty on movement contingent on land cover, especially for off-track or off-road movement. For example the speed attainable off-track in forested terrain might be only half that attainable in grasslands. Unsurprisingly, there are no widely agreed land cover penalties, but see for example, those compiled by Herzog (2020). A hiking function derived by Wood et al. (2023) includes the local gradient of the terrain (i.e., the maximum slope at each location, not the slope in the direction of movement) as a covariate in the estimated function. It is possible that this kind of approach including other spatial covariates, such as land cover (which would be a categorical variable in most cases) in the estimation of complex hiking functions might be more widely applied in future work. In our application, because there are only two kinds of navigable land cover—moraine (or gravel) and rock—we chose instead to estimate two hiking functions, one for each land cover, and estimate movement costs conditional on the land cover at each location. This also has the advantage of allowing for differing effects of slope on speed due to land cover, where for example, gravel might allow more rapid movement on the level surfaces, but more rapidly reduce speed on slopes. Details of this approach are reported and discussed in Section 4.1.\n\n\n2.2 Representing the landscape\nA has been noted, hiking functions are usually asymmetric, with the highest attainable speed at a slight downhill slope. This asymmetry means that unless analysis is focused on assessing movement cost from a single origin or to a single destination, it is necessary to represent the landscape in a way that can accommodate asymmetry. We therefore represent the landscape as a directed graph \\(G(V,E)\\) of vertices and directed edges connecting them. In this representation, graph vertices \\(v_i\\) are locations with associated elevation and land cover. Vertices minimally have spatial coordinates \\((x_i,y_i)\\), an elevation \\(z_i\\), and a land cover, \\(C_i\\):\n\\[\nv_i=\\left(x_i,y_i,z_i,C_i\\right)\n\\tag{1}\\]\nGraph edges \\(e_{ij}=\\left(v_i,v_j\\right)\\) are directed connections between vertices for which a change in elevation between the start and end vertex can be calculated, and a slope derived, based on the elevation difference, and the horizontal distance between the vertices. Thus the slope \\(\\theta_{ij}\\) of edge \\(e_ij\\) is given by\n\\[\n\\theta_{ij}=\\frac{z_j-z_i}{\\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}}\n\\tag{2}\\]\nThe obvious way to derive such a graph from spatial data is to assign each cell in a raster digital elevation model (DEM) to a graph∏ vertex, so that graph vertices are arranged in a regular square grid or lattice, and this approach has been widely adopted. CITATIONS NEEDED HERE However, this still requires a decision about how to define graph edges, which can be thought of in terms of allowed moves between vertices. Three possibilities on a square lattice—Rook’s, Queen’s, and Knight’s moves—are shown in Figure 2. Another option shown in the figure is to lay out a regular hexagonal grid of vertex locations and link each vertex to its six nearest neighbours.\n\n\n\n\n\n\n\n\nFigure 2: Possible graph lattices.\n\n\n\n\n\nThe advantage of a square lattice is that is east to derive vertex elevations assuming a DEM at the desired lattice spacing is available. A hexagonal lattice on the other hand usually requires that elevation values be interpolated from a DEM to the lattice locations.\nBoth square and hexagonal lattices lead to geometric artifacts when the allowed moves are used to determine contours of equal travel time or isochrones from an origin point on a flat surface with uniform movement speeds. In this situation, the Rook’s move produces diamond-shaped isochrones, while the Queen’s move produces octagons. In general isochrones will be polygons with as many sides as the number of neighbours of each vertex. While the 16-gons of the Knight’s move lattice are likely to be close enough to circular for many purposes, the resulting graph is denser than the alternatives. Etherington (2012) suggests that by combining results from randomly generated planar graphs such geometric artifacts can be removed, but this substantially increases the computational requirements. Another computationally intensive approach might draw on methods for estimating geodesics on complex triangular meshes (Martínez et al. 2005). Because these geometric effects of graph structure are masked when we introduce varying movement speeds due to the slope of each edge, we do not consider these more complex approaches necessary in our application. Nevertheless, more circular base isochrone shapes are to be preferred, all other things equal.\nA further complication when a lattice includes edges of varying length as in the Queen’s and Knight’s move lattices, is bias in the estimation of slopes leading to lower estimated movement costs for longer edges. This is because longer edges (e.g., the knight’s moves) may ‘jump’ across intervening segments of varying slope, smoothing them to a single slope estimate. Such smoothing will usually result in shorter edge traversal times along such edges than those that would accumulate along intervening sections of varying slope. This problem is discussed in a slightly different context by Campbell et al. (2019, 96–98).\nAny choice of graph structure is necessarily a compromise, and estimates of movement rates are always an approximation (Goodchild 2020). We consider the differing edge lengths introduced by all the square lattices other than the Rook’s case to be problematic, and favour the hexagonal lattice structure over the simple square lattice because a base hexagonal isochrone is preferable to a diamond shape.\nHaving settled on a hexagonal lattice for the graph structure, at a chosen resolution we set out a regular hexagonal grid of locations across the study area, and assign to each point an elevation by interpolation from a DEM. Because the DEM is at finer resolution than the hexagonal lattice, the choice of interpolation method is not a major concern. Our approach applies bilinear interpolation based on elevation values in the DEM cell the graph vertex falls in and its four orthogonal neighbours. Based on the difference in elevation of the vertices at each end of each edge we estimate a slope using Equation 2, and also traversal times using our estimated hiking function. The important point here is that our graph is directed so that different traversal times \\(t_{ij}\\) and \\(t_{ji}\\) are estimated for moving between vertex \\(v_i\\) and \\(v_j\\) depending on the direction of travel.\nAs noted in the previous section, we estimate two hiking functions, one for moraine and one for rock land cover. When an edge connects locations of two different land cover types the estimated traversal time is the mean of the traversal times for each land cover."
  },
  {
    "objectID": "paper/paper.html#data-sources",
    "href": "paper/paper.html#data-sources",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "3 Data sources",
    "text": "3 Data sources\n\n3.1 Antarctic geospatial data\nGeospatial data for Antarctica were obtained from sources referenced in Cox et al. (2023b), Cox et al. (2023a), and Felden et al. (2023). The study area was defined to be the Skelton and Dry Valleys basins, as defined by the NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) project (Mouginot and University Of California Irvine 2017) and shown in Figure 3(a). The Skelton basin was included because while the expedition GPS data was ostensibly collected in the McMurdo Dry Valleys, it actually extends into that basin as shown in Figure 3(b). Elevation data from the Reference Elevation Model of Antarctica (REMA) project Howat et al. (2022), and geology from GeoMAP Cox et al. (2023b) are shown in Figure 3(c). The five largest areas of contiguous non-ice surface geology across the study area shown in Figure 3(d) were chosen to be the specific sites for more detailed exploration using the methods set out in this paper. These range in size from around 320 to 2600 square kilometres.\n\n\n\n\n\n\n\n\nFigure 3: The study area: (a) Study area location in Antarctica; (b) Skelton and Dry Valleys basins; (c) Study area elevation (hillshade) and surface geology; and (d) Five sub-regions of contiguous surface geology.\n\n\n\n\n\n\n\n3.2 GPS data from an expedition\nFRASER: Timeline, devices used, and associated protocols for scientists while on site.\nGPS data were processed to make them better suited for use in the estimation of hiking functions.\nThe first processing step was to confirm the plausibility of the data, particularly the device-generated speed distance between fixes, and elevations associated with fixes. The challenges of post-processing GPS data are well documented and relate to issues with GPS drift which can lead to estimated non-zero movement speeds as a result of noise in the signal. The raw GPS data included distance since last fix, speed, and elevation estimates and it was determined in all cases that the device generated results for these measurements were likely to be more reliable than post-processing the raw latitude-longitude fixes to calculate the values.\nThe second processing step was to remove fixes associated with faster movement on other modes of transport than walking. Wood et al. (2023) cite a number of previous works that base detection of trip segments based on recorded speeds. This method was trivially applicable to our data to a limited degree as scientists arrive at the expedition base camp and occasionally also travel on helicopters on trips to more remote experimental sites.\nThe third, more challenging processing step was to deal with sequences of fixes associated with non-purposeful movement when scientists were in or around base camp, at rest stops, or at experimental sites. Crude filters removed fixes with low recorded distances between fixes (less than 2.5 metres), high turn angles at the fix location (greater than 150°), and fixes recorded on ice-covered terrain, but this didi not clean the data sufficiently for further analysis. An additional filtering step was to count fixes (across all scientists) in square grid cells and remove all fixes in grid cells with more than 50 fixes.\nThis left one persistent concern: an over-representation of consecutive fixes recorded at exactly the same elevation, resulting in many fixes with estimated slopes of exactly 0, and leading to a clearly evident dip in estimated movement speeds at 0 slope (Figure 4(a)). It is likely that these fixes are associated with GPS device drift, so a it was decided to remove all fixes where estimated slope was exactly 0. Figure 4(b) shows the improvement in even a crudely estimated hiking function derived from local scatterplot (LOESS) smoothing. Note that such functions are likely overfitted and not used further in our analysis where we favour more easily parameterised functions such as those discussed in Section 2.1.\n\n\n\n\n\n\n\n\nFigure 4: GPS data and crudely estimated hiking functions before and after filtering the to remove fixes associated with non-purposive movement: (a) Boxplots by slope of speed, with smoothed estimated hiking function showing a ‘dip’ due to over-representation of 0 slope fixes; and (b) After filtering the estimated hiking function no longer has a dip."
  },
  {
    "objectID": "paper/paper.html#methods-and-results",
    "href": "paper/paper.html#methods-and-results",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "4 Methods and results",
    "text": "4 Methods and results\n\n4.1 Hiking functions\nWe fit three alternative functional forms to the cleaned GPS data: exponential (Tobler 1993), Gaussian (following Irmischer and Clarke 2018), and Lorentz (following Campbell et al. 2019) using the Levenburg-Marquardt algorithm (Moré 1978) as provided by the nlsLM function in the minpack.lm R package (Elzhov et al. 2022). The raw data and fitted curves are shown in Figure 5.\n\n\n\n\n\n\n\n\nFigure 5: Three possible hiking functions applied to GPS data split by land cover.\n\n\n\n\n\nThe Lorentz function offers a marginal improvement in the model fit in comparison with the Gaussian function, while both are clearly better than the exponential form. However, the improvement offered by the Lorentz function over the Gaussian is marginal: residual standard error 1.489 vs 1.491 on Moraine, and 1.487 vs 1.488 on Rock, and inspection of the curves shows that estimated hiking speeds for the Gaussian functions are much closer to a plausible zero on very steep slopes. We therefore chose to adopt Gaussian hiking functions for the remainder of the present work.\nIn previous work researchers have applied a ground cover penalty cost to a base hiking function to estimate traversal times. We instead, as shown, estimate different hiking functions for the two ground cover types present. The peak speed on rock is attained on steeper downhill slopes than on moraines, perhaps indicative of the greater care required on downhill gravel slopes. Meanwhile the highest speeds on level terrain are attained on moraines.\nPlotting both hiking functions along with an additional model fitted to all the data on the same axes confirms that the fitted functions are sufficiently different to retain separate models for each ground cover (see Figure 6). Plotting both functions in the same graph makes clearer the difference in maximum speed and slope at maximum speed associated with each ground cover.\n\n\n\n\n\n\n\n\nFigure 6: The hiking functions for All, Moraine and Rock ground covers compared, including 95% confidence intervals derived by Monte-Carlo simulation.\n\n\n\n\n\nThe estimated hiking functions associated with the two land covers are \\[\n\\begin{array}{rcl}\nv_{\\mathrm{moraine}} & = & 4.17\\,\\exp\\left[{-\\frac{(\\theta+0.0526)^2}{0.236}}\\right] \\\\\nv_{\\mathrm{rock}}    & = & 3.76\\,\\exp\\left[{-\\frac{(\\theta+0.119)^2}{0.365}}\\right]\n\\end{array}\n\\tag{3}\\]\nwhere the different maximum speeds (in kilometres per hour) and slopes of maximum speed are apparent.\n\n\n4.2 Landscapes as graphs\nWe developed R code (R Core Team 2024) to build graphs (i.e. networks) with hexagonal lattice structure and estimated traversal times for graph edges derived from our hiking functions. Graphs are manipulated as igraph package (Csárdi and Nepusz 2006; Csárdi et al. 2024) graph objects for further analysis. An important decision in constructing graphs is choice of the spacing of the hexagonal lattice, and also of the underlying DEM from which graph vertex elevations are derived. Given the extent of the study sites (see Figure 3(d)) it was decided that a hexagonal lattice (see Figure 2) with hexagons equivalent in area to 100 metre square cells as appropriate. The hexagon centre to centre spacing of this lattice is given by \\[\n100\\sqrt{\\frac{2}{\\sqrt{3}}}\\approxeq 107.5\\,\\mathrm{metres}\n\\tag{4}\\] Given this lattice resolution we interpolated vertex elevations from a 32m resolution DEM from the REMA project (Howat et al. 2022) by bilinear interpolation using the R terra package (Hijmans 2024). It would be straightforward to derive vertex elevations from a more detailed DEM if required.\nEdge weights (i.e. estimated traversal costs) are assigned by calculating the slope of each directed edge, the estimated hiking speed for that slope, and thus finding how long it should take for an edge to be traversed. If the estimated traversal time of an edge in the nominal 100 metre lattice is greater than 30 minutes then it is removed from the graph, along with its ‘twin’ edge in the opposite direction. Removing edges in both directions is partly a practical matter as it simplifies the operation of many graph algorithms, but can also be justified on the basis that a slope steep enough to be a barrier to ascent is unlikely to be traversed when descending.\nAfter removal of all such edges only the largest connected graph component is retained so that the resulting hiking network representation is fully connected with no isolated vertices unreachable from elsewhere in the network remaining. A map of the fifth largest study area’s hiking network is shown in Figure 7. This network includes 30,697 vertices and 174,798 directed edges. The largest of the five study sites (see Figure 3(d)) results in a network containing almost a quarter of a million vertices and over 1.4 million directed edges. A hiking network can be used to explore many connectivity properties of the environment. For example, for a chosen origin point, a shortest path tree can be derived showing the route (Figure 7(c)).\n\n\n\n\n\n\n\n\nFigure 7: A hiking network derived for the fifth largest study site shown in Figure 3(d): (a) Map of a hiking network coloured by estimated traversal times of edges; (b) Red-outlined area zoomed in revealing edges removed from graph due to steep slopes; (c) Shortest path tree of a hiking network from the indicated origin; and (d) Red-outlined area zoomed in view of the shortest path tree.\n\n\n\n\n\n\n\n4.3 Betweenness centrality\nThe connectivity properties of a hiking graph can be described using a range of measures of graph structure and used to reveal the relative likelihood of different parts of a terrain being frequently traversed. The particular structural measure we focus on is betweenness centrality, which is explained below.\nIn a graph \\(G=(V,E)\\) a path \\(P\\) is an ordered sequence of vertices, \\(P=\\left(v_1,v_2,\\ldots,v_n\\right)\\) such that each consecutive pair of vertices \\(v_i\\) and \\(v_{i+1}\\) is adjacent, i.e. connected by a directed edge \\(e_{i,j}\\). The length of a path is the number of edges it contains. The weighted length or cost of a path is the sum over its edges of a weight or cost associated with each edge, which we here denote \\(w_{i,j}\\), i.e., the length \\(L\\) of a path \\(P\\) is given by \\[\nL(P)=\\sum_{i=1}^{n-1} w_{i,i+1}\n\\tag{5}\\]\nThe shortest path from \\(v_i\\) to \\(v_j\\) is the path \\(P=\\left(v_i,\\dots,v_*,\\ldots,v_j\\right)\\) starting at\\(v_i\\) and ending at \\(v_i\\) passing through intervening vertices \\(\\left(v_*\\right)\\) such that \\(L(P)\\) is minimised. In a regular lattice such as our hiking networks there are many shortest paths of equal length if the weight associated with each edge is equal, as it would be if we based it solely on the length of the edge between each pair of vertices. When we consider edge traversal costs derived from the slope of each edge and a hiking function, then the shortest path will be unique, or one of only a small number of possibilities of equal total cost.\nShortest paths are used to develop many measures of vertex centrality in graphs (Freeman 1978). One such measure of vertex centrality is betweenness centrality. The betweenness centrality of a vertex is the total number of times that vertex is on shortest paths between every other pair of vertices in the network. If we denote the number of shortest paths or geodesics between two vertices \\(v_j\\) and \\(v_k\\) by \\(g_{jk}\\), then each appearance of a vertex \\(v_i\\) on the shortest path between those vertices only contributes \\(1/g_{jk}\\) to the betweenness centrality of \\(v_i\\). Formally, if we denote the number of times vertex \\(v_i\\) appears on shortest paths between \\(v_j\\) and \\(v_k\\) by \\(g_{jk}(v_i)\\), then its betweenness \\(b_i\\) is given by \\[\nb_i=\\sum_{k=1}^n\\sum_{j=1}^n\\frac{g_{jk}(v_i)}{g_{jk}}\\forall i\\neq j\\neq k\n\\tag{6}\\] Betweenness centrality is particularly relevant to applications where we are interested in how important vertices are to movement across the graph. An edge betweenness centrality measure can also be calculated on similar principles, but is not considered further here, in part because visualization is challenging given the potential for different scores for the two directed edges between each pair of vertices. Vertex betweenness on the other hand yields a single value for each vertex.\nAs might be expected betweenness centralities are computationally demanding to calculate. The time complexity of early algorithms was \\(\\mathcal{O}(n^3)\\), where \\(n\\) is the number of vertices in the graph (Brandes 2001). An implementation of Brandes’s algorithm (2001) with time complexity \\(\\mathcal{O}(nm + n^2\\log n)\\), where \\(m\\) is the number of edges in the graph, is provided in the igraph package (Csárdi et al. 2024). Even with this improvement in performance, in our application such computational complexity is a strong motivation for working at a nominal 100 metre resolution. Halving the resolution to 50 metres would increase the number of graph vertices four-fold, and lead to a substantial increases in the times taken to calculate betweenness centralities.\nResults of betweenness centrality for our example hiking network are shown in Figure 8. ‘All-paths’ betweenness centralities can be standardised relative to a theoretical maximum of \\((n-1)(n-2)\\), no straightforward standardisation is possible for the radius-limited case. Since we are primarily interested in betweenness as a measure of the relative vulnerability of different locations to human impacts, we linearly rescaled betweenness scores in all cases with respect to the range of scores across the graph being analysed. The rescaled betweenness, \\(b_i^\\prime\\) for vertex \\(v_i\\) is given by \\[\nb_i^\\prime=\\frac{b_i-b_{\\min}}{b_{\\max}-b_{\\min}}\n\\tag{7}\\] This approach is also applicable to radius-limited betweenness scores (see Section 4.4).\nWhen all-paths betweenness is calculated, bottlenecks between large sub-areas are strongly highlighted as is the case for the ‘inner bend’ of the relatively narrow neck of navigable terrain that connects the east and west sub-areas of this site. The other feature of this map is the identification of a distinctive structure of ‘arterial’ routes.\n\n\n\n\n\n\n\n\nFigure 8: Relative vertex betweenness of vertices in the graph from Figure 7.\n\n\n\n\n\n\n\n4.4 Betweenness centrality limited by radius\nThe igraph implementation of betweenness centrality provides an option to radius limit betweenness calculations, meaning that only paths shorter than a specified radius (expressed in cost units, here of time) are considered in counting the appearance of vertices on shortest paths. This approach is particularly useful for large graphs where the time complexity of calculating radius limited betweenness scales more favourably than cited above, because the number of shortest paths that must be identified is greatly reduced. We informally confirmed the intuition that for a given radius limit the time take to compute betweenness scales approximately linearly with the number of vertices in the graph, since the number of shortest paths local to each vertex, and on which it might be found is similar.\nResults for a series of radius limits are shown in Figure 9. The results here are interesting. At very short radii (the 30 minutes case) many locally important paths are identified as potentially relatively heavily trafficked. As the radius increases the pattern emphasizes potential paths that are important across wider areas, eventually tending toward the arterial structure of the all-paths case. We discuss these results more fully in Section 5.\n\n\n\n\n\n\n\n\nFigure 9: Vertex betweenness maps based on different radius limits. As in Figure 8 the visualization colouring is based on linear scaling of raw betweenness scores.\n\n\n\n\n\n\n\n4.5 Impact minimizing networks\nIn this section we explore developing the betweenness centrality for a limited set of locations, which might represent a base camp, rest sites, and experimental sites for a season’s expedition.\nThis is relatively straightforward and like radius limited betweenness involves limiting the shortest paths considered to only those among a chosen subset of vertices in the graph. One slight simplification we make for performance reasons is to weight every appearance of a vertex on a geodesic as equal, even if multiple equal length shortest paths exist between two vertices. In practice, this is unlikely to have much effect on the resulting estimates of relative betweenness, since with real-valued edge costs two paths of exactly equal cost are unlikely. An example result is shown in Figure 10. One refinement shown in the figure is that sites have been waited by relative importance so that a site such as an expedition base camp can be expected to originate more trips than other sites.\n\n\n\n\n\n\n\n\nFigure 10: The network of shortest paths among a weighted set of expedition sites.\n\n\n\n\n\nA further analytical possibility is to use the hiking network to develop a set of routes among the sites that might minimise total impact, by confining movement to a limited set of paths. This approach is consistent with work in ecology and park and wildlife management which suggests that path networks can reduce the impact of human movement in many environments (Cole 1995a; Cole 1995b; Kidd et al. 2015; Marion et al. 2016; Piscová et al. 2023; Tomczyk and Ewertowski 2023).\nThe approach taken is first to first determine the lengths of all the shortest paths among all the sites. Then a minimum spanning tree for the sites is calculated, which is a graph with minimum total edge length that connects all the sites (Prim 1957). One refinement here, which we do not account for is that algorithms for finding the minimum spanning tree of a directed graph (referred to as a minimum-cost arborescence or optimal branching) are more complex (Korte and Vygen 2018), and not at present supported in igraph. While there is some asymmetry in paths between locations depending on the direction of travel, it is not extreme, and so we approximate a solution by finding the minimum spanning tree links (i.e. direct connections) among sites treating our network as undirected, and then determine paths over the ground to make the identified connections between sites based on the directed hiking network. The resulting path network for the same set of sites as in Figure 10 is shown in Figure 11.\n\n\n\n\n\n\n\n\nFigure 11: A potential impact minimising route network among sites."
  },
  {
    "objectID": "paper/paper.html#discussion",
    "href": "paper/paper.html#discussion",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "5 Discussion",
    "text": "5 Discussion\nBlah"
  },
  {
    "objectID": "paper/paper.html#conclusions",
    "href": "paper/paper.html#conclusions",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "6 Conclusions",
    "text": "6 Conclusions\n\n\n\nFigure 1: Example hiking functional forms: Exponential (Tobler 1993), Gaussian (Irmischer and Clarke 2018), Lorentz (Campbell et al. 2019), and Quadratic (Rees 2004).\nFigure 2: Possible graph lattices.\nFigure 3 (a): Study area location in Antarctica\nFigure 3 (b): Skelton and Dry Valleys basins\nFigure 3 (c): Study area elevation (hillshade) and surface geology\nFigure 3 (d): Five sub-regions of contiguous surface geology\nFigure 4 (a): Boxplots by slope of speed, with smoothed estimated hiking function showing a ‘dip’ due to over-representation of 0 slope fixes\nFigure 4 (b): After filtering the estimated hiking function no longer has a dip.\nFigure 5: Three possible hiking functions applied to GPS data split by land cover.\nFigure 6: The hiking functions for All, Moraine and Rock ground covers compared, including 95% confidence intervals derived by Monte-Carlo simulation.\nFigure 7 (a): Map of a hiking network coloured by estimated traversal times of edges\nFigure 7 (b): Red-outlined area zoomed in revealing edges removed from graph due to steep slopes\nFigure 7 (c): Shortest path tree of a hiking network from the indicated origin\nFigure 7 (d): Red-outlined area zoomed in view of the shortest path tree\nFigure 8: Relative vertex betweenness of vertices in the graph from Figure 7.\nFigure 9: Vertex betweenness maps based on different radius limits. As in Figure 8 the visualization colouring is based on linear scaling of raw betweenness scores.\nFigure 10: The network of shortest paths among a weighted set of expedition sites.\nFigure 11: A potential impact minimising route network among sites."
  },
  {
    "objectID": "notebooks/00-notes.html",
    "href": "notebooks/00-notes.html",
    "title": "Overview / plan",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-07-18\nReprioritised items.\n\n\n2024-07-18\nInitial post.\nWill keep this page updated as we go.",
    "crumbs": [
      "Notebooks",
      "Overview / plan"
    ]
  },
  {
    "objectID": "notebooks/00-notes.html#data-sources",
    "href": "notebooks/00-notes.html#data-sources",
    "title": "Overview / plan",
    "section": "Data sources",
    "text": "Data sources\n\nElevation data\nHowat, Ian, et al., 2022, “The Reference Elevation Model of Antarctica – Mosaics, Version 2”, https://doi.org/10.7910/DVN/EBW8UC, Harvard Dataverse, V1, Accessed: 28-29 August 2024. Downloaded from ftp.data.pgc.umn.edu at 10 and 32m resolutions.\n\n\nRock outcrops\nGerrish, L. (2020). Automatically extracted rock outcrop dataset for Antarctica (7.3) [Data set]. UK Polar Data Centre, Natural Environment Research Council, UK Research & Innovation. https://doi.org/10.5285/178ec50d-1ffb-42a4-a4a3-1145419da2bb\n\n\nGeology\nFrom GNS GeoMAP:\nCox SC, B Smith Lyttle, S Elkind, CS Smith Siddoway, P Morin, G Capponi, T Abu-Alam, M Ballinger, L Bamber, B Kitchener, L Lelli, JF Mawson, A Millikin, N Dal Seno, L Whitburn, T White, A Burton-Johnson, L Crispini, D Elliot, S Elvevold, JW Goodge, JA Halpin, J Jacobs, E Mikhalsky, AP Martin, F Morgan, J Smellie, P Scadden, and GS Wilson. 2023. The GeoMAP (v.2022-08) continent-wide detailed geological dataset of Antarctica.PANGAEA. doi: 10.1594/PANGAEA.951482.\nDownloaded from: https://download.pangaea.de/dataset/951482/files/ATA_SCAR_GeoMAP_v2022_08_QGIS.zip\n\n\nLakes\nGerrish, L., Fretwell, P., & Cooper, P. (2020). High resolution Antarctic lakes dataset (7.3) [Data set]. UK Polar Data Centre, Natural Environment Research Council, UK Research & Innovation. https://doi.org/10.5285/6a27ab9e-1258-49b1-bd2c-fcbed310ab45",
    "crumbs": [
      "Notebooks",
      "Overview / plan"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html",
    "href": "notebooks/04-building-a-hiking-network.html",
    "title": "Building a hiking network",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-08-06\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#tldr-executive-summary",
    "href": "notebooks/04-building-a-hiking-network.html#tldr-executive-summary",
    "title": "Building a hiking network",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings are:\n\nEssential steps in building a hiking network are set out and remain valid.\nThe interpolation step in Making an estimated travel time map is overkill: it is sufficient for mapping purposes to display a point map of the graph vertices.\nAn additional necessary step became apparent in later work where very steep edges remain in the networks. These may not matter much in practice as shortest path network approaches are unlikely to highlight them as plausible routes (some very steep edges end up with multi-hour traversal time estimates!). However, it makes sense to remove such edges completely from consideration, and if that makes some regions completely inaccessible this is probably a more realistic outcome.\n\n\nThis notebook explores creation of hiking networks across a terrain where movement rates may depend on the direction of movement (because going uphill is different than going downhill…).\nFirst we set up some folders and filenames.\n\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\nbase_folder &lt;-    str_glue(\"{here()}/_data/testing\")\ninput_folder &lt;-   str_glue(\"{base_folder}/input\")\noutput_folder &lt;-  str_glue(\"{base_folder}/output\")\nbasename &lt;-       \"dry-valleys-10m\"\ndem_file &lt;-       str_glue(\"{input_folder}/{basename}.tif\")\nimagery_file &lt;-   str_glue(\"{input_folder}/{basename}-imagery.tif\")\nlandcover_file &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")\nextent_file &lt;-    str_glue(\"{input_folder}/{basename}-extent.gpkg\")\norigins_file &lt;-   str_glue(\"{input_folder}/{basename}-origins.gpkg\")\n\nNow read in the DEM and an extent, and imagery (which we might not use…).\n\nterrain &lt;- rast(dem_file)\n# and for visualization\nshade &lt;- get_hillshade(terrain)\nimagery &lt;- rast(imagery_file)\nextent &lt;- st_read(extent_file)\n\n# make a focal area for convenience of plotting in some situations\ncentre_of_extent &lt;- extent |&gt; \n  st_centroid() # we use this later...\nc_vec &lt;- centre_of_extent |&gt;\n  st_coordinates() |&gt;\n  as.vector()\nfocal_area &lt;- ((extent$geom[1] - c_vec) * diag(0.025, 2, 2) + c_vec) |&gt;\n  st_sfc(crs = st_crs(extent)) |&gt;\n  st_as_sf() |&gt;\n  rename(geom = x)\n\nSetup a resolution for use in building networks (i.e. determining which cells are adjacent to one another in vector GIS space). Although we have a DEM at 10m resolution, at least for now we’ll use a coarser grain for construction of the network. It is in any case unclear what an appropriate resolution might be given the various approximations in play.\n\n# assume square pixels: 50m on a side\nresolution &lt;- res(terrain)[1] * 5 \n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- 2 * resolution / sqrt(2 * sqrt(3))\n\nhexgrid_file &lt;- str_glue(\"{input_folder}/{basename}-hex-grid-{resolution}.gpkg\")\ngraph_file &lt;-   str_glue(\"{output_folder}/{basename}-graph-hex-{resolution}.txt\") \n\nhex_resolution is a hexagon spacing for use in st_make_grid that yields the same area cell as square cells with edge length given by resolution",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#making-a-hex-grid-across-the-terrain",
    "href": "notebooks/04-building-a-hiking-network.html#making-a-hex-grid-across-the-terrain",
    "title": "Building a hiking network",
    "section": "Making a hex grid across the terrain",
    "text": "Making a hex grid across the terrain\nWe build the network based on hex spaced points, not a square grid. This is helpful in making all edges the same length, as opposed to in a square grid where diagonal neighbour cell centres are further apart than edge neighbouring cells.\nWe load a hex grid file if one has already been made, otherwise build it.\nIMPORTANT NOTE: Don’t forget to remake this grid if changing any of the code prior to this point in the workflow in ways that affect the spacing of the hex grid.\n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    st_set_crs(st_crs(extent))\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nMake a plot to make sure things are the right scale, in particular that the hexes we get are the same area as the simple square grid cells would be (this isn’t critical, but it’s nice as a standardisation of what ‘resolution’ means for a hex grid).\n\nsquare_grid &lt;- focal_area |&gt;\n  st_make_grid(cellsize = resolution)\n\nggplot(square_grid) +\n  geom_sf(colour = \"white\") +\n  geom_sf(data = xy |&gt; st_filter(focal_area), colour = \"red\", size = 3) +\n  theme_void()\n\n\n\n\n\n\n\n\nBy visual inspection these are similar. We can confirm that the hexagon area in a grid at this spacing is\n\nfocal_area |&gt; \n  st_make_grid(cellsize = hex_cell_spacing, square = FALSE) |&gt; \n  head(1) |&gt; \n  st_area()\n\n2500 [m^2]\n\n\nwhich is the same as the square grid\n\nsquare_grid |&gt; head(1) |&gt; st_area()\n\n2500 [m^2]",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#retrieve-elevations-from-the-terrain",
    "href": "notebooks/04-building-a-hiking-network.html#retrieve-elevations-from-the-terrain",
    "title": "Building a hiking network",
    "section": "Retrieve elevations from the terrain",
    "text": "Retrieve elevations from the terrain\nAlthough we do the modelling of hiking functions using elevations in the GPS data, we don’t have complete elevation data for the area on that basis, so we retrieve these from the DEM.\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n# make pts and remove any NAs that might result from interpolation\n# NAs are around the edges of the study area\npts &lt;- xy |&gt;\n  mutate(z = z[, 2]) |&gt;\n  filter(!is.na(z))\n\nAgain a map is helpful, although again we use only the focal area so we can see what’s going on—and for speed of plotting.\n\nggplot(pts |&gt; st_filter(focal_area)) +\n  geom_sf(aes(colour = z), size = 20) +\n  scale_colour_distiller(palette = \"Oranges\") +\n  theme_void()",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#make-a-graph-i.e.-network-connecting-these-elevation-points",
    "href": "notebooks/04-building-a-hiking-network.html#make-a-graph-i.e.-network-connecting-these-elevation-points",
    "title": "Building a hiking network",
    "section": "Make a graph (i.e. network) connecting these elevation points",
    "text": "Make a graph (i.e. network) connecting these elevation points\nThe graph_from_points function in raster-to-graph-functions.R creates an igraph::graph object from an (x, y) point set. IMPORTANT NOTE: Don’t forget to remake the graph if messing with any of the code above here that might affect the spacing of the hex grid.\n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\nBefore we can visualize this easily, we have to attach various data to its elements (at the moment it is just a set of vertices without any attributes). The required data include a landcover layer with relative impedances. Note that for now this cover layer is entirely made up – pending consultation with someone more knowledgeable about conditions on the ground. As far as I can tell from imagery the high cost areas are ‘ice’ which I’ve arbitrarily assigned a relative cost of 5.\n\ncover &lt;- st_read(landcover_file)\n\nggplot() +\n  geom_sf(data = cover, aes(fill = as.factor(cost)), linewidth = 0) +\n  scale_fill_manual(values = c(\"white\", \"red\"), name = \"Relative cost\") +\n  # some ggnewscale shenanigans required to plot a second layer with fill aesthetic\n  new_scale_fill() +\n  geom_raster(data = shade |&gt; as.data.frame(xy = TRUE), \n              aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\nNext add attributes to the graph using the assignment_movement_variables_to_graph function in raster-to-graph-functions.R\n\nxyz &lt;- extract_xyz_from_points(pts)\n\ncosts &lt;- pts |&gt;\n  st_join(cover, .predicate = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  select(cost)\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz, impedances = costs$cost)\n\nNow we have a graph with coordinates assigned to its vertices, we can map it (after transforming to a sf dataset)\n\nggplot(G |&gt; get_graph_as_line_layer() |&gt; \n  st_filter(focal_area)) +\n  geom_sf(aes(colour = cost)) +\n  theme_void()\n\n\n\n\n\n\n\n\nIt’s not clear above that this is a bidirectional network, but we can see by inspection, e.g. the first edge in the graph is between vertices 1 and 166. We can access these (in igraph’s rather abstruse way), as follows:\n\nG |&gt; \n  edge_attr(index = get.edge.ids(G, c(1, 166, 166, 1))) |&gt;\n  as_tibble()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlength_xyz\nlength_xy\nz_diff\ngradient\nimpedance\ntobler_speed\ncost\n\n\n\n\n53.72997\n53.7285\n-0.3975143\n-0.0073986\n1\n3101.323\n0.0173249\n\n\n53.72997\n53.7285\n0.3975143\n0.0073986\n1\n2944.794\n0.0182457\n\n\n\n\n\n\nThe lengths of the edges are identical, their height differences and gradients are opposite and the speed (metres per hour) and cost (time to traverse in hours) are different due to the slope.",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/04-building-a-hiking-network.html#make-an-estimated-travel-time-map",
    "href": "notebooks/04-building-a-hiking-network.html#make-an-estimated-travel-time-map",
    "title": "Building a hiking network",
    "section": "Make an estimated travel time map",
    "text": "Make an estimated travel time map\nUsing the graph representation we can estimate travel times from any start location to every other location. For the sake of illustration we’ll make such a map from the graph vertex closest to the centre of the study area.\n\norigin_i &lt;- centre_of_extent |&gt;\n  st_nearest_feature(pts)\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), weights = edge_attr(G, \"cost\")) |&gt; t()\n\n# assemble results into a DF\ndf &lt;- data.frame(x = V(G)$x, y = V(G)$y, z = V(G)$z, \n                 time_hrs = V(G)$time_hrs, \n                 cost = costs$cost)\n# write this out to a shapefile, which Whitebox Tools needs\nshp_fname &lt;- str_glue(\"{here()}/_temp/{basename}-{format(origin_i, scientific = FALSE)}-hex.shp\")\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# note that interpolation is required because we are using hexagonal grid cells\n# do interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\nwbt_natural_neighbour_interpolation(shp_fname, field = \"time_hrs\",\n                                    output = tif_fname, \n                                    base = str_glue(dem_file))\n\nReading the exported raster layer made by whitebox::wbt_natural_neighbour_interpolation back in we get a map of estimated travel times.\n\nrast(tif_fname) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_distiller(palette = \"Spectral\", name = \"Est. time hrs\") +\n  geom_contour(aes(z = travel_time), linewidth = 0.5) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nIn practice we’ll use other graph functionalities than one origin to all destination shortest paths, but this gives the general idea.",
    "crumbs": [
      "Notebooks",
      "Building a hiking network"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html",
    "href": "notebooks/01-exploring-gps-data.html",
    "title": "Exploring GPS data",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-11-15\nChanged DEM data source. Looked more closely at the slope from GPS vs slope from DEM comparison.\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-07-19\nUpdated to add turn angle calculation and source gps cleaning function from gps-data-util.R.\n\n\n2024-07-16\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#tldr-executive-summary",
    "href": "notebooks/01-exploring-gps-data.html#tldr-executive-summary",
    "title": "Exploring GPS data",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings are:\n\nGPS reported speed and height data are reasonable choices to use in the estimation of hiking functions.\nHowever, some cleanup is required: speeds of up to 200km/h (helicopter flights), very long elapsed distances, and some negative elevations appear in the raw data and should be removed.\nNot all the issues with the GPS data are uncovered in this notebook (it is an initial exploration), so some additional filtering is applied in subsequent notebooks as deemed necessary. In particular, see this notebook\nThe bulk of the GPS data cleaning operations are carried out on the basis of the list compiled at the end of this notebook, detailed in this section and implemented in this script\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(here)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(zoo) # useful for serial data\nlibrary(R.utils)\nlibrary(hrbrthemes)\n\ntheme_set(theme_ipsum_rc())\n\nsource(str_glue(\"{here()}/scripts/gps-data-utils.R\"))\n\nThese notes explore the usefulness, accuracy etc. of the provided GPS data.\nOf particular interest is the asymmetry or not of movement speeds with respect to slope, given the salience of this for ‘hiking functions’. It is possible in a largely ‘unpathed’ environment that the asymmetry is limited (see e.g. Rees 20041)\nWe use just one of the datasets to explore general characteristics. Remove a meaningless column x and do a little bit of cleanup on the rcr and date and time columns.\n\ngps_data &lt;- get_gps_data_as_sf(str_glue(\n  \"{here()}/_data/GPS-1516Season-MiersValley/Fraser-FINAL.csv\")) |&gt;\n    dplyr::select(-x) |&gt;\n    mutate(rcr = as.logical(rcr),\n           date_time = ymd_hms(paste(date, time)))",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#exploration-of-the-data",
    "href": "notebooks/01-exploring-gps-data.html#exploration-of-the-data",
    "title": "Exploring GPS data",
    "section": "Exploration of the data",
    "text": "Exploration of the data\nA summary of the data suggests that there are some data rows that should be ignored.\n\nsummary(gps_data)\n\n     index        rcr              date               time          \n Min.   :   1   Mode:logical   Length:7323        Length:7323       \n 1st Qu.:1832   TRUE:7304      Class :character   Class :character  \n Median :3662   NA's:19        Mode  :character   Mode  :character  \n Mean   :3662                                                       \n 3rd Qu.:5492                                                       \n Max.   :7323                                                       \n    valid              latitude          n_s              longitude    \n Length:7323        Min.   :-78.12   Length:7323        Min.   :163.7  \n Class :character   1st Qu.:-78.11   Class :character   1st Qu.:163.8  \n Mode  :character   Median :-78.10   Mode  :character   Median :163.8  \n                    Mean   :-78.10                      Mean   :163.9  \n                    3rd Qu.:-78.10                      3rd Qu.:163.9  \n                    Max.   :-78.09                      Max.   :164.2  \n     e_w               height_m       speed_km_h           pdop      \n Length:7323        Min.   :-23.1   Min.   :  0.000   Min.   : 0.98  \n Class :character   1st Qu.:102.1   1st Qu.:  0.255   1st Qu.: 1.14  \n Mode  :character   Median :110.8   Median :  0.496   Median : 1.19  \n                    Mean   :180.1   Mean   :  1.604   Mean   : 1.27  \n                    3rd Qu.:165.7   3rd Qu.:  2.241   3rd Qu.: 1.24  \n                    Max.   :636.9   Max.   :209.555   Max.   :13.76  \n      hdop              vdop        nsat_used_view       distance_m      \n Min.   : 0.6100   Min.   :0.7600   Length:7323        Min.   :    0.00  \n 1st Qu.: 0.7400   1st Qu.:0.8600   Class :character   1st Qu.:    1.38  \n Median : 0.7900   Median :0.8900   Mode  :character   Median :    2.78  \n Mean   : 0.7977   Mean   :0.9756                      Mean   :   21.73  \n 3rd Qu.: 0.8400   3rd Qu.:0.9200                      3rd Qu.:   14.99  \n Max.   :13.7200   Max.   :4.7100                      Max.   :74486.45  \n          geometry      date_time                     \n POINT        :7323   Min.   :2016-01-13 16:28:31.07  \n epsg:3031    :   0   1st Qu.:2016-01-15 16:43:27.00  \n +proj=ster...:   0   Median :2016-01-19 12:46:32.00  \n                      Mean   :2016-01-19 01:37:06.72  \n                      3rd Qu.:2016-01-22 08:39:45.00  \n                      Max.   :2016-01-23 15:16:47.00  \n\n\nA speed of &gt; 200km/h was clearly not achieved on foot! Similarly an elapsed distance between 1/30Hz fixes of 74km was not on foot either.\nThe GPS dilution of precision measures for horizontal and vertical position (hdop and vdop) are mostly within the ‘good’ range (&lt;1) with some outliers. The 3D positions (pdop) are not as good, but are also generally considered reliable (&lt;5). Inspection of a quick map of the data suggests helicopter movement or similar for some ‘singleton’ isolated observations:\n\nggplot(gps_data) +\n  geom_sf(size = 0.25)\n\n\n\n\n\n\n\n\nThis being the case it seems reasonable to remove observations with clearly unreasonable speed estimates, assuming that these result from use of other forms of transport. However, before doing so, we augment the data with additional estimates of speed, distance, turn angle, etc. calculated from consecutive fixes (it is not advisable to remove any fixes before making these estimates).",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#sanity-checking-the-gps-estimates-of-speed-distance-and-height",
    "href": "notebooks/01-exploring-gps-data.html#sanity-checking-the-gps-estimates-of-speed-distance-and-height",
    "title": "Exploring GPS data",
    "section": "Sanity checking the GPS estimates of speed, distance, and height",
    "text": "Sanity checking the GPS estimates of speed, distance, and height\nBefore applying that potentially simplistic cleaning of the data, it is also reasonable to sanity check the speed, distance, and height estimates in the GPS data. First we obtain height data from a 10m DEM.\n\nxy &lt;- gps_data |&gt;\n  st_coordinates() |&gt;\n  as_tibble()\n\ndem &lt;- rast(str_glue(\"{here()}/_data/dem/10m/dem-10m-gps-extent.tif\"))\n\nheights_from_dem &lt;- dem |&gt;\n  terra::extract(xy, method = \"bilinear\") |&gt;\n  dplyr::select(2)\n\nTRI_from_dem &lt;- dem |&gt; \n  terra::terrain(\"TRI\") |&gt;\n  terra::extract(xy, method = \"bilinear\") |&gt;\n  select(2)\n\nslope_from_dem &lt;- dem |&gt; \n  terra::terrain(\"slope\") |&gt;\n  terra::extract(xy, method = \"bilinear\") |&gt;\n  select(2)\n\nxy &lt;- xy |&gt;\n  bind_cols(heights_from_dem) |&gt;\n  bind_cols(TRI_from_dem) |&gt;\n  bind_cols(slope_from_dem) |&gt;\n  rename(Z = `dem-10m-gps-extent`,\n         dem_TRI = TRI,\n         dem_slope = slope)\n\ngps_data_plus &lt;- gps_data |&gt;\n  bind_cols(xy)\n\nNow calculate estimates of distance, change in elevation, and slope between observations, from these numbers and from the GPS data.\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  mutate(\n    dx1 = as.numeric(diff(as.zoo(X), na.pad = TRUE)), \n    dy1 = as.numeric(diff(as.zoo(Y), na.pad = TRUE)),\n    dx2 = as.numeric(diff(as.zoo(X), lag = -1, na.pad = TRUE)),\n    dy2 = as.numeric(diff(as.zoo(Y), lag = -1, na.pad = TRUE)),\n    dist_xy = sqrt(dx1^ 2 + dy1 ^ 2), # not useful - path followed is not straight\n    turn_angle = get_turn_angle(dx1, dy1, dx2, dy2),\n    dt = as.numeric(diff(as.zoo(date_time), na.pad = TRUE)),\n    dh = as.numeric(diff(as.zoo(height_m), na.pad = TRUE)),\n    dz = as.numeric(diff(as.zoo(Z), na.pad = TRUE))) |&gt;\nrename(\n  z_gps = height_m,\n  z_dem = Z)\n\nNow some comparisons.\n\nDistance estimates\nThe distance_m estimates in the provided data are generally higher than the manually calculated point to point straight line distances between fixes, which is unsurprising. However, examination of sequences of data show the two time series are well aligned (below). Differences may be due to filtering applied by the GPS unit, although this is not well documented.\n\ngps_data_plus |&gt;\n  filter(dt == 30) |&gt;\n  slice(2:151) |&gt;\n  pivot_longer(cols = c(distance_m, dist_xy)) |&gt;\n  ggplot() +\n  geom_line(aes(x = date_time, y = value, colour = name, group = name)) +\n  scale_colour_manual(values = c(\"red\", \"blue\"), labels = c(\"From XY data\", \"GPS estimate\"), name = \"Source\") +\n  xlab(\"Date/time\") + ylab(\"Estimated distance\")\n\n\n\n\n\n\n\n\nIn any case, the GPS distance_m variable seems safe for use in further analysis.\nTherefore we’ll remove the dist_xy alternative and rename distance_m simply to distance.\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  select(-dist_xy, -dx1, -dx2, -dy1, -dy2, -X, -Y) |&gt; \n  rename(distance = distance_m)\n\n\n\nSpeed estimates\nGiven the distance attribute we can estimate speeds for comparison with the GPS reported speed_km_h value.\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  mutate(derived_speed = distance / dt / 1000 * 3600)\n\nA comparison\n\ngps_data_plus |&gt; \n  filter(dt == 30) |&gt;\n  slice(2:151) |&gt;\n  pivot_longer(cols = c(speed_km_h, derived_speed)) |&gt;\n  ggplot() + \n  geom_line(aes(x = date_time, y = value, colour = name, group = name)) +\n  scale_colour_manual(values = c(\"red\", \"blue\"), labels = c(\"Derived\", \"GPS estimate\"), name = \"Source\") +\n  xlab(\"Date/time\") + ylab(\"Estimated speed\")\n\n\n\n\n\n\n\n\nThe GPS provided estimates are generally higher although there is some alignment between the time series, insofar as extended periods of lower speeds tend to line up.\nThe likely reason for the differences is that in a 30 second period (the usual interval between fixes) an individual may move in many directions so that the direct point to point distance between fixes is less than might be expected based on the recorded speed. So, for example, in the above chart between 16:30 and 16:40 the person was moving not in straight lines, where in the following 10 minutes they were moving in relatively straight lines. We can test this theory, at least qualitatively, by calculating a sinuosity estimate between fixes.\n\nsinuosity &lt;- gps_data_plus |&gt;\n  filter(dt == 30) |&gt;\n  slice(2:61) |&gt;\n  mutate(sinuosity = speed_km_h / 120 / distance,\n         date_time_start = lag(date_time)) \n\nspeeds &lt;- sinuosity |&gt;\n  pivot_longer(cols = c(speed_km_h, derived_speed))\n\nggplot(speeds) +\n  geom_step(aes(x = date_time, y = value, group = name, linetype = name), \n            direction = \"vh\", lwd = 0.5) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\"), labels = c(\"Derived\", \"GPS est.\"), name = \"Source\") +\n  geom_segment(data = sinuosity, \n               aes(x = date_time_start, xend = date_time, \n                   y = speed_km_h, colour = rank(-sinuosity)), lwd = 2) +\n  scale_colour_distiller(palette = \"Reds\", direction = 1, name = \"Rank of sinuosity\") +\n  xlab(\"Date/time\") + ylab(\"Estimated speed\")\n\n\n\n\n\n\n\n\nSince the estimated sinuosity here is based only on the GPS estimated speed and point to point distances, that it is generally highest (lower rank, paler reds) when the difference between the GPS reported speed speed_km_h and the manually estimated speed derived_speed is largest supports the assumption that the GPS is reporting speed using satellite carrier signal doppler effects, and not elapsed distance and time.\nIt therefore makes sense to use the GPS estimated speed and delete the derived speed.\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  select(-derived_speed) |&gt; \n  rename(speed = speed_km_h)\n\n\n\nGPS height and elevation from the DEM\nNow we consider the GPS estimated height and DEM derived elevations.\nAgain, there are differences between heights recorded by the GPS units, and heights extracted from the 10m DEM. It is difficult to know why these might occur, although there is a clear correlation between the two. However, there is no obvious correlation with, e.g., surface roughness (TRI), for example, or with the vertical dilution of precision (vdop) in the differences between the two:\n\nggplot(gps_data_plus |&gt; \n         filter(dt == 30, distance &lt; 100, z_gps &gt; 0, z_dem &gt; 0) |&gt;\n         mutate(z_error = (2 * (z_gps - z_dem) / (z_gps + z_dem)))) +\n  geom_point(aes(x = dem_TRI, y = z_error, colour = vdop), alpha = 0.5, size = 1) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\n\n\n\n\n\n\n\nSome reassurance that the GPS height estimates are suitably aligned is obtained by plotting each days set of observations of the two as time series. On days where much of the activity occurred at the same elevation or across a very limited elevation range there are obvious deviations, but these are relatively small in magnitude; on days when more elevation chance is observed, the overall profile of the day’s activity matches well.\n\ngps_data_plus |&gt; \n  filter(dt == 30) |&gt;\n  pivot_longer(cols = c(z_gps, z_dem)) |&gt;\n  ggplot() + \n  geom_line(aes(x = date_time, y = value, colour = name, group = name), alpha = 0.5) +\n  xlab(\"Date/time\") + ylab(\"Estimated elevation\") + \n  facet_wrap(~ date, scales = \"free\", ncol = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt’s not clear which is to be preferred. So add slope variables to the data:\n\ngps_data_plus &lt;- gps_data_plus |&gt;\n  select(-dem_TRI, -dem_slope) |&gt;\n  mutate(\n    slope_gps = dh / distance,\n    slope_dem = dz / distance)\n\nWe can eyeball how the choice of height measures affects the slope - speed relationship we are interested in for hiking function estimation.\n\ndf &lt;- bind_rows(\n  gps_data_plus |&gt; mutate(slope = slope_gps, slope_src = \"GPS\"),\n  gps_data_plus |&gt; mutate(slope = slope_dem, slope_src = \"DEM\")\n)\n\nggplot(df |&gt; filter(dt == 30, speed &lt; 10)) +\n  geom_point(aes(x = slope, y = speed), size = 0.1, alpha = 0.5) +\n  # scale_colour_viridis_c(option = \"A\", direction = -1) +\n  # guides(colour = guide_legend(override.aes = list(size = 5))) +  \n  geom_smooth(aes(x = slope, y = speed)) +\n  facet_wrap( ~ slope_src, labeller = \"label_both\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis result strongly suggests that it is a safer option to use the GPS estimates of height, although it is really not clear that it is ‘better’ or more accurate.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#a-hiking-function",
    "href": "notebooks/01-exploring-gps-data.html#a-hiking-function",
    "title": "Exploring GPS data",
    "section": "A hiking function",
    "text": "A hiking function\nA hiking function relates slope of the land to speed at which it is traversed. Using the GPS unit reported speed and slope derived from GPS unit reported distance and height, locally estimated scatter plot smoothing yields an approximately Gaussian curve with the peak speed at or close to 0 slope. This gives some confidence that we can use these data to estimate hiking functions. The approach implied in this plot is insufficient, as the data contain many fixes that don’t give much information about ‘hiking’ as such. The preponderance of higher turn angle fixes in the less ‘mobile’ parts of the data (i.e. at lower speeds) suggests that it may be possible to filter out those data points.\nBetter estimation of a hiking function is left to later notebooks (see Estimating a hiking function and Refining hiking functions).\n\n# gps_data_plus |&gt;\n#   # here's a possible filter to apply to the data to improve the estimation\n#   filter(dt == 30, speed_km_h &lt; 10, !is.na(slope_h), distance_m &gt; 2.5, turn_angle &lt; 150) |&gt;\n#   ggplot(aes(x = slope_h, y = speed_km_h, colour = turn_angle)) +\n#   geom_point(size = 0.25) +\n#   geom_smooth(aes(x = slope_h, y = speed_km_h))\n# \n# model &lt;- loess(speed_km_h ~ slope_h, data = gps_data_plus)\n\ngps_data_plus |&gt;\n  # here's a possible filter to apply to the data to improve the estimation\n  filter(dt == 30, speed &lt; 10, !is.na(slope_gps), distance &gt; 2.5, turn_angle &lt; 150) |&gt;\n  ggplot(aes(x = slope_gps, y = speed, colour = turn_angle)) +\n  geom_point(size = 0.25) +\n  geom_smooth(aes(x = slope_gps, y = speed))\n\n\n\n\n\n\n\nmodel &lt;- loess(speed ~ slope_gps, data = gps_data_plus)\n\nMany data in the above plot are probably from ‘milling about’ in breaks, at camp, or around experiments, rather than while on the move. The best way to remove these sections is probably based on turn angles.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#conclusion-a-reasonable-gps-file-cleaning-function",
    "href": "notebooks/01-exploring-gps-data.html#conclusion-a-reasonable-gps-file-cleaning-function",
    "title": "Exploring GPS data",
    "section": "Conclusion: a reasonable GPS file cleaning function",
    "text": "Conclusion: a reasonable GPS file cleaning function\nBased on this exploration, GPS data need to be processed as follows (note that no initial conversion to a spatial format is required at this stage, since none of the added spatial data introduced above is required).\n\nRead raw data from CSV.\nClean names with janitor::clean_names.\nRemove meaningless variable x.\nAdd a name reflecting the id of the person whose data is.\nAugment data with dt (time interval between fixes), dh (height change between fixes), slope_h (estimated slope between fixes), and turn_angle (change in direction at this fix).\nRemove data outside expected latitudinal range (&gt; -78 latitude).\nRemove fixes with high estimated speeds (&gt;~ 10km/h).\nRemove fixes with long estimated distances (&gt;~ 60m).\nIdentify breaks in the series where dt != 30 and tag subsets of the data accordingly.\nCount number of fixes in each subset – consider removing small ones…\nDrop NAs and any rows where dt != 30.\n\nThis is the basis for a function get_cleaned_gps_data() in the script gps-data-utils.R used for cleaning up / augmenting the raw CSV files.",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/01-exploring-gps-data.html#footnotes",
    "href": "notebooks/01-exploring-gps-data.html#footnotes",
    "title": "Exploring GPS data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRees WG. 2004. Least-cost paths in mountainous terrain. Computers & Geosciences 30(3) 203–209. doi: 10.1016/j.cageo.2003.11.001.↩︎",
    "crumbs": [
      "Notebooks",
      "Exploring GPS data"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html",
    "href": "notebooks/09-refining-hiking-functions.html",
    "title": "Refining hiking functions",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-11-15\nCorrected minor error in transcribing final hiking function constants.\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-09-23\nFinalised the hiking function for use.\n\n\n2024-09-19\nDetailed the commentary.\n\n\n2024-09-16\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#tldr-executive-summary",
    "href": "notebooks/09-refining-hiking-functions.html#tldr-executive-summary",
    "title": "Refining hiking functions",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings are:\n\nGPS data must be filtered to support estimation of reasonable hiking functions. In particular: stopping locations (base camp, experimental sites, rest stops) must be filtered out. This filtering is performed by counting fixes in hexbins at 100m spacing and removing any fixes that fall in dense areas (50 fixes per bin was arbitrarily chosen after some experimentation). An alternative might be to obtain detailed information about the base camp and experimental sites for this season’s fieldwork.\nAlso removed: observations associated with a slope estimate slope_h == 0. There are an anomalously large number of these and they depress estimated hiking speeds on the flat. Exploration of the data turned up no obvious reason for the existence of these fixes, so the pragmatic decision was taken to remove them. Many slope estimates close to 0 are present in the data in any case.\nAlso removed: high turn angles (≥150°) and low reported GPS distances (≤2.5m) as these are both suggestive of ‘loitering’ not purposive movement.\nHiking function estimation is readily carried out based on the complete remaining dataset. This seems preferable to an earlier approach using a smoothed version of the data derived from median speeds at a series of slope intervals. We have many data points, let’s use them!\nA Gaussian functional form seems to fit the data best.\nThe differences between the hiking function derived for moraine and rock surfaces are plausible enough and strong enough to retain both functions and apply them separately to the setup of the hiking network.\nThere are differences between individuals but since the purpose of this research is to estimate collective impacts across populations of scientists, there is no obvious way to incorporate such differences into the project.\n\n\n\nCode\nlibrary(here)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(hrbrthemes)\nlibrary(ggnewscale)\nlibrary(ggspatial)\nlibrary(minpack.lm)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#organise-the-data",
    "href": "notebooks/09-refining-hiking-functions.html#organise-the-data",
    "title": "Refining hiking functions",
    "section": "Organise the data",
    "text": "Organise the data\nNow read the GPS data and associate with it different geology types. Contrary to the more detailed geology types shown in this notebook we use a column from the geology layer that distinguishes only moraine, rock, and ice. More detail than this is not readily validated, and also the GPS data available only crosses 8 of the 13 geology classes in the more detailed classification—and hence cannot be used to parameterise the more detailed classification anyway.\n\n\nCode\n# gps data and its bbox\ngps_data &lt;- st_read(str_glue(\"{here()}/_data/cleaned-gps-data/all-gps-traces.gpkg\"))\nbb &lt;- gps_data |&gt; st_bbox() # this is to allow plotting restricted to GPS extents\n\n# geologies data\ngeologies &lt;- \n  st_read(str_glue(\"{here()}/_data/ata-scar-geomap-geology-v2022-08-clipped.gpkg\")) |&gt; \n  dplyr::select(POLYGTYPE) |&gt;\n  st_filter(bb |&gt; st_as_sfc()) |&gt; \n  mutate(cover = factor(POLYGTYPE))\n\n# join geologies to the gps data\ngps_geol &lt;- gps_data |&gt;\n  st_join(geologies) |&gt;\n  dplyr::select(-POLYGTYPE) |&gt; \n  # we do not further consider (in this notebook) 'ice' terrain, nor\n  # observations at negative elevations(!)\n  filter(cover != \"ice\", height_m &gt; 0) |&gt;\n  mutate(slope_h_round = round(slope_h * 10) / 10) |&gt;\n  drop_na()\n\n\n\nSuppressing observations with slope_h == 0\nOne anomaly remains in the GPS data, which is hard to explain, but affects the estimation of models as carried out in the remainder of this notebook, namely a large number of observations with estimated slope_h of 0. This is apparent in a scatterplot of the data\n\n\nCode\nggplot(gps_geol) +\n  geom_point(aes(x = slope_h, y = speed_km_h), size = 0.1, alpha = 0.1, pch = 19) +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nEven a cursory examination of a local smoothing of the speed of movement relative to slope demonstrates the effect of these observations on any estimation of rates of movement.\n\n\nCode\nggplot(gps_geol) +\n  geom_boxplot(aes(x = round(slope_h, 1), y = speed_km_h, group = round(slope_h, 1)), \n               outlier.size = 0.2, colour = \"grey\", linewidth = 0.5) +\n  stat_smooth(aes(x = slope_h, y = speed_km_h)) +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nThis concentration of observations at a single value seems likely to be an artifact of data collection, and so we avoid further difficulties by removing such observations as one part of the filtering discussed in the next section.",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#filter-out-areas-of-non-purposive-movement",
    "href": "notebooks/09-refining-hiking-functions.html#filter-out-areas-of-non-purposive-movement",
    "title": "Refining hiking functions",
    "section": "Filter out areas of non-purposive movement",
    "text": "Filter out areas of non-purposive movement\nEstimating a hiking function depends in part on only fitting functions to data recorded when subjects were purposefully moving.\nOne option is to filter on the density of GPS fixes - there are necessarily many more GPS fixes in and around the base camp than elsewhere. We can use hex binning to estimate density of fixes.\n\n\nCode\nhexes &lt;- gps_geol |&gt; \n  st_make_grid(cellsize = 100, square = FALSE, what = \"polygons\") |&gt;\n  st_as_sf(crs = st_crs(gps_geol)) |&gt;\n  rename(geom = x) |&gt;\n  st_join(gps_geol |&gt; mutate(id = row_number()), left = FALSE) |&gt;\n  group_by(geom) |&gt;\n  summarise(n = n(), n_persons = n_distinct(name)) |&gt;\n  ungroup()\n\n\nWe use this in conjunction with some other characteristics of the traces to filter data to fixes most likely to be associated with periods of purposive movement. Choice of cutoff values is fairly arbitrary. Experimentation with settings shows that exact values don’t matter greatly, the important thing is to exclude data in or around base camp, experimental sites, rests stops, etc. In any case, doing so (as shown in the plot which follows below, makes it difficult to argue for a meaningful difference in estimated hiking speeds vs. slope on the different geologies available in the GPS data).\n\n\nCode\ngps_geol_purposive &lt;- gps_geol |&gt; \n  # see above\n  filter(slope_h != 0) |&gt;\n  # these two remove 'dithering'\n  filter(turn_angle &lt; 150) |&gt;\n  filter(distance_m &gt; 2.5) |&gt;\n  # remove fixes in densely trafficked areas\n  st_join(hexes) |&gt;\n    filter(n &lt;= 50)\n\n\nA map to show the effect.\n\n\nCode\nggplot() +\n  geom_sf(data = geologies, aes(fill = cover), linewidth = 0.1, colour = \"white\") +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  new_scale_fill() +\n  geom_sf(data = hexes |&gt; filter(n &lt;= 200), aes(fill = n), linewidth = 0) +\n  scale_fill_viridis_c(option = \"A\", direction = 1) +\n  geom_sf(data = gps_geol_purposive, size = 0.005, pch = 19, colour = \"white\") +\n  coord_sf(xlim = bb[c(1, 3)], ylim = bb[c(2, 4)]) +\n  annotation_scale(height = unit(0.1, \"cm\")) +\n  theme_ipsum_rc()",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#estimate-models",
    "href": "notebooks/09-refining-hiking-functions.html#estimate-models",
    "title": "Refining hiking functions",
    "section": "Estimate models",
    "text": "Estimate models\n\nFunctions to estimate models\n\nget_gaussian_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dnorm(slope_h, m, s), data = df,\n        start = c(a = 5, m = 0, s = 0.5))\n}\n\nget_tobler_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * exp(-b * abs(slope_h + c)), data = df,\n        start = c(a = 5, b = 3, c = 0.05))  \n}\n\nget_students_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a * dt((slope_h + m), d), data = df,\n                   start = c(a = 5, m = 0, d = 0.001))  \n}\n\nget_lorentz_hiking_function &lt;- function(df) {\n  nlsLM(speed_km_h ~ a /(pi * b * (1 + ((slope_h - c) / b) ^ 2)), \n        data = df, start = c(a = 5, b = 1, c = -0.05))\n}\n\nIt is also convenient to have a function that returns estimated speeds as a slope-speed dataframe.\n\n# makes a prediction data frame with x, y values\n# slopes is a DF with one column called slope_h\nget_model_prediction_df &lt;- function(m, slopes) {\n  data.frame(x = slopes, y = predict(m, data.frame(slope_h = slopes$slope_h)))\n}\n\nNow estimate hiking functions split by geology.\n\n\nCode\nslopes &lt;- data.frame(slope_h = -150:150 / 100)\n\nhiking_functions &lt;- list(\n  gaussian = get_gaussian_hiking_function,\n  tobler = get_tobler_hiking_function,\n  # students = get_students_hiking_function\n  lorentz = get_lorentz_hiking_function\n)\ncovers &lt;- c(\"moraine\", \"rock\")\n\nnbins &lt;- 10\n\nmodels &lt;- list()\npredictions &lt;- list()\ninputs &lt;- list()\ni &lt;- 1\nfor (name in names(hiking_functions)) {\n  for (cover_type in covers) {\n    df &lt;- gps_geol_purposive |&gt; \n      filter(cover == cover_type) \n    # |&gt;\n    #   get_percentile_summary(percentile = pc, num_bins = nbins)\n    m &lt;- hiking_functions[[name]](df)\n    models[[cover_type]][[name]] &lt;- m\n    predictions[[i]] &lt;- get_model_prediction_df(m, slopes) |&gt;\n      mutate(cover = cover_type, model = name)\n    inputs[[i]] &lt;- df |&gt;\n      mutate(cover = cover_type, model = name)\n    i &lt;- i + 1\n  }\n}\nmodel_predictions &lt;- bind_rows(predictions)\ninput_data &lt;- bind_rows(inputs)",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#inspection-of-initial-models",
    "href": "notebooks/09-refining-hiking-functions.html#inspection-of-initial-models",
    "title": "Refining hiking functions",
    "section": "Inspection of initial models",
    "text": "Inspection of initial models\n\n\nCode\nggplot() +\n  geom_boxplot(data = gps_geol_purposive,\n               aes(x = slope_h_round, y = speed_km_h, group = slope_h_round),\n               outlier.size = 0.35, colour = \"grey\", linewidth = 0.5) +\n  # geom_point(data = input_data, aes(x = slope_h, y = speed_km_h)) + \n  geom_line(data = model_predictions, aes(x = slope_h, y = y, colour = model)) +\n  xlab(\"Slope\") + ylab(\"Speed, km/h\") +\n  facet_wrap( ~ cover, ncol = 3) +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nBased on these results (and also some prior exploration):\n\nAlthough models can be estimated for ‘ice’ we consider ice impassable, as it is clear from mapping the GPS data that any supposed movement across ‘ice’ is due to inaccuracies in the data. Hence this geology cover has been removed from consideration.\nIt appears that the top speed attainable across ‘moraine’ is slightly higher than on ‘rock’, which seems reasonable.\nIt may be that the speed attained across ‘moraine’ falls away more rapidly with slope than on ‘rock’. This is not implausible: ‘moraine’ terrain might be slippy and somewhat treacherous to cross on steeper slopes.\n\nThe Gaussian functional forms of the hiking function have the best ‘fit’ based on the residual sum of squares statistic (and AIC and log-likelihood).",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#are-the-hiking-functions-on-the-two-terrains-different-enough-to-use-both",
    "href": "notebooks/09-refining-hiking-functions.html#are-the-hiking-functions-on-the-two-terrains-different-enough-to-use-both",
    "title": "Refining hiking functions",
    "section": "Are the hiking functions on the two terrains different enough to use both?",
    "text": "Are the hiking functions on the two terrains different enough to use both?\nBut should we use different models for the two surface covers? The differences above are not substantial. We can explore their overlap by Monte-Carlo simulation of the fitted models. This unfortunately is not something models generated by minpack.lm::nlsLM can do using the base R predict.nls functions. The following function, obtained from this post has been used for this purpose and appears to work satisfactorily.\n\n\nCode\npredictNLS &lt;- function(object, newdata, level = 0.95, nsim = 10000, ...) {\n  require(MASS, quietly = TRUE)\n  ## get right-hand side of formula\n  RHS &lt;- as.list(object$call$formula)[[3]]\n  EXPR &lt;- as.expression(RHS)\n  ## all variables in model\n  VARS &lt;- all.vars(EXPR)\n  ## coefficients\n  COEF &lt;- coef(object)\n  ## extract predictor variable    \n  predNAME &lt;- setdiff(VARS, names(COEF))  \n  ## take fitted values, if 'newdata' is missing\n  if (missing(newdata)) {\n    newdata &lt;- eval(object$data)[predNAME]\n    colnames(newdata) &lt;- predNAME\n  }\n  ## check that 'newdata' has same name as predVAR\n  if (names(newdata)[1] != predNAME) stop(\"newdata should have name '\", predNAME, \"'!\")\n  ## get parameter coefficients\n  COEF &lt;- coef(object)\n  ## get variance-covariance matrix\n  VCOV &lt;- vcov(object)\n  ## augment variance-covariance matrix for 'mvrnorm' \n  ## by adding a column/row for 'error in x'\n  NCOL &lt;- ncol(VCOV)\n  ADD1 &lt;- c(rep(0, NCOL))\n  ADD1 &lt;- matrix(ADD1, ncol = 1)\n  colnames(ADD1) &lt;- predNAME\n  VCOV &lt;- cbind(VCOV, ADD1)\n  ADD2 &lt;- c(rep(0, NCOL + 1))\n  ADD2 &lt;- matrix(ADD2, nrow = 1)\n  rownames(ADD2) &lt;- predNAME\n  VCOV &lt;- rbind(VCOV, ADD2) \n  ## iterate over all entries in 'newdata' as in usual 'predict.' functions\n  NR &lt;- nrow(newdata)\n  respVEC &lt;- numeric(NR)\n  seVEC &lt;- numeric(NR)\n  varPLACE &lt;- ncol(VCOV)   \n  ## define counter function\n  counter &lt;- function (i) {\n    if (i%%10 == 0) \n      cat(i)\n    else cat(\".\")\n    if (i%%50 == 0) \n      cat(\"\\n\")\n    flush.console()\n  }\n  outMAT &lt;- NULL \n  for (i in 1:NR) {\n    counter(i)\n    ## get predictor values and optional errors\n    predVAL &lt;- newdata[i, 1]\n    if (ncol(newdata) == 2) predERROR &lt;- newdata[i, 2] else predERROR &lt;- 0\n    names(predVAL) &lt;- predNAME  \n    names(predERROR) &lt;- predNAME  \n    ## create mean vector for 'mvrnorm'\n    MU &lt;- c(COEF, predVAL)\n    ## create variance-covariance matrix for 'mvrnorm'\n    ## by putting error^2 in lower-right position of VCOV\n    newVCOV &lt;- VCOV\n    newVCOV[varPLACE, varPLACE] &lt;- predERROR^2\n    ## create MC simulation matrix\n    simMAT &lt;- mvrnorm(n = nsim, mu = MU, Sigma = newVCOV, empirical = TRUE)\n    ## evaluate expression on rows of simMAT\n    EVAL &lt;- try(eval(EXPR, envir = as.data.frame(simMAT)), silent = TRUE)\n    if (inherits(EVAL, \"try-error\")) stop(\"There was an error evaluating the simulations!\")\n    ## collect statistics\n    PRED &lt;- data.frame(predVAL)\n    colnames(PRED) &lt;- predNAME   \n    FITTED &lt;- predict(object, newdata = data.frame(PRED))\n    MEAN.sim &lt;- mean(EVAL, na.rm = TRUE)\n    SD.sim &lt;- sd(EVAL, na.rm = TRUE)\n    MEDIAN.sim &lt;- median(EVAL, na.rm = TRUE)\n    MAD.sim &lt;- mad(EVAL, na.rm = TRUE)\n    QUANT &lt;- quantile(EVAL, c((1 - level)/2, level + (1 - level)/2))\n    RES &lt;- c(FITTED, MEAN.sim, SD.sim, MEDIAN.sim, MAD.sim, QUANT[1], QUANT[2])\n    outMAT &lt;- rbind(outMAT, RES)\n  }\n  colnames(outMAT) &lt;- c(\"fit\", \"mean\", \"sd\", \"median\", \"mad\", names(QUANT[1]), names(QUANT[2]))\n  rownames(outMAT) &lt;- NULL\n  cat(\"\\n\")\n  return(outMAT)  \n}\n\n\nBefore applying this, we also estimate a model from all the data\n\n\nCode\ndf &lt;- gps_geol_purposive \nm3 &lt;- get_gaussian_hiking_function(df)\nmodels[[\"all\"]][[\"gaussian\"]] &lt;- m\nmodel_predictions &lt;- model_predictions |&gt;\n  bind_rows(get_model_prediction_df(m, slopes) |&gt; \n              mutate(cover = \"all\", model = \"gaussian\"))\ninput_data &lt;- input_data |&gt;\n  bind_rows(df |&gt; mutate(cover = \"all\", model = \"gaussian\"))\n\n\nAnd now determine confidence intervals across all three models.\n\n\nCode\nconf &lt;- 0.95\npred1 &lt;- predictNLS(models[[\"moraine\"]][[\"gaussian\"]], newdata = slopes, level = conf) |&gt;\n  as.data.frame() |&gt;\n  mutate(x = slopes$slope_h, cover = \"moraine\")\n\n\n.........10.........20.........30.........40.........50\n.........60.........70.........80.........90.........100\n.........110.........120.........130.........140.........150\n.........160.........170.........180.........190.........200\n.........210.........220.........230.........240.........250\n.........260.........270.........280.........290.........300\n.\n\n\nCode\npred2 &lt;- predictNLS(models[[\"rock\"]][[\"gaussian\"]], newdata = slopes, level = conf) |&gt;\n  as.data.frame() |&gt;\n  mutate(x = slopes$slope_h, cover = \"rock\")\n\n\n.........10.........20.........30.........40.........50\n.........60.........70.........80.........90.........100\n.........110.........120.........130.........140.........150\n.........160.........170.........180.........190.........200\n.........210.........220.........230.........240.........250\n.........260.........270.........280.........290.........300\n.\n\n\nCode\npred3 &lt;- predictNLS(m3, newdata = slopes, level = conf) |&gt;\n  as.data.frame() |&gt;\n  mutate(x = slopes$slope_h, cover = \"all\")\n\n\n.........10.........20.........30.........40.........50\n.........60.........70.........80.........90.........100\n.........110.........120.........130.........140.........150\n.........160.........170.........180.........190.........200\n.........210.........220.........230.........240.........250\n.........260.........270.........280.........290.........300\n.\n\n\nCode\npredictions &lt;- bind_rows(pred1, pred2, pred3)\n\n\nAnd now we can overplot the models showing their 95% confidence intervals.\n\n\nCode\nggplot() +\n  geom_ribbon(data = predictions |&gt; filter(cover != \"all\"), \n              aes(x = x, ymin = `2.5%`, ymax = `97.5%`, group = cover, fill = cover), \n              alpha = 0.35, linewidth = 0) + \n  scale_fill_brewer(palette = \"Set1\", name = \"Terrain\") +\n  geom_ribbon(data = predictions |&gt; filter(cover == \"all\"), \n              aes(x = x, ymin = `2.5%`, ymax = `97.5%`), \n              colour = \"black\", lty = \"dashed\", fill = \"#00000000\", linewidth = 0.35) +\n  geom_line(data = predictions |&gt; filter(cover == \"all\"), aes(x = x, y = mean)) +\n  xlab(\"Slope\") + ylab(\"Speed, km/h\") +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nThere is a clear difference between the moraine and rock estimates, both in the maximum speeds attained, and interestingly (and convincingly) in the slope at which this maximum speed is attained. Both terrains admit faster movement downhill, but a steeper downhill is where the fastest speed is attainable on rocky surfaces, and in fact downhill movement on rocky surfaces is faster at most slope angles. Uphill movement is similar on both terrain types. This makes sense when we consider that moraines consist of loose gravels which can be treacherous especially when moving downhill. The ‘all’ terrain model (black lines) matches quite closely the moraine model, but is a poor fit to the rocky terrain model.",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#two-estimated-hiking-functions",
    "href": "notebooks/09-refining-hiking-functions.html#two-estimated-hiking-functions",
    "title": "Refining hiking functions",
    "section": "Two estimated hiking functions",
    "text": "Two estimated hiking functions\nThe two functions we are left with are\n\n\nCode\nggplot() + \n  geom_boxplot(data = gps_geol_purposive,\n               aes(x = slope_h_round, y = speed_km_h, group = slope_h_round),\n               outlier.size = 0.35, colour = \"grey\", linewidth = 0.5) +\n  geom_ribbon(data = predictions |&gt; filter(cover != \"all\"), \n            aes(x = x, ymin = `2.5%`, ymax = `97.5%`, fill = cover), alpha = 0.35) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Terrain\") +\n  geom_line(data = predictions |&gt; filter(cover != \"all\"), \n            aes(x = x, y = mean, colour = cover), linewidth = 0.75) +\n  scale_colour_brewer(palette = \"Set1\", name = \"Terrain\") +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\nThe equations for these function given the model summaries below\n\n\nCode\nmodels[[\"moraine\"]][[\"gaussian\"]]\n\n\nNonlinear regression model\n  model: speed_km_h ~ a * dnorm(slope_h, m, s)\n   data: df\n       a        m        s \n 3.59025 -0.05259  0.34348 \n residual sum-of-squares: 17728\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 1.49e-08\n\n\n\n\nCode\nmodels[[\"rock\"]][[\"gaussian\"]]\n\n\nNonlinear regression model\n  model: speed_km_h ~ a * dnorm(slope_h, m, s)\n   data: df\n      a       m       s \n 4.0302 -0.1188  0.4272 \n residual sum-of-squares: 8050\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 1.49e-08\n\n\nnoting that the Gaussian form is \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\), calculating out all the constants, are given by\n\\[\n\\begin{eqnarray}\nv_{\\mathrm{moraine}} & = & 4.169969\\,e^{-\\frac{(s+0.05259)^2}{0.235957}} \\\\\nv_{\\mathrm{rock}}    & = & 3.763575\\,e^{-\\frac{(s+0.1186)^2}{0.3653757}}\n\\end{eqnarray}\n\\]\nAnd here is a function implementing this\n\nest_hiking_function &lt;- function(slope, terrain = \"moraine\") {\n  if (terrain == \"moraine\") {\n    4.166969 * exp(-(slope + 0.05259) ^ 2 / 0.235957)\n  } else {\n    3.763575 * exp(-(slope + 0.1186) ^ 2 / 0.3653757)\n  }\n}\n\nOverall, it is unlikely that these details will much affect overall outcomes of the analysis, although it seems worth making the most of the available data, which this approach will do.\nThe most impactful aspect of changes in the top speed of the hiking function will be on the choice of ‘cutoff’ cost for the betweenness measures. However, this choice is subjective and indicative only, so it is difficult to develop any systematic rules around calibration of the hiking function, or the choice of betweenness cutoff cost.",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#addendum-what-about-variation-between-hikers",
    "href": "notebooks/09-refining-hiking-functions.html#addendum-what-about-variation-between-hikers",
    "title": "Refining hiking functions",
    "section": "Addendum: what about variation between hikers",
    "text": "Addendum: what about variation between hikers\nWhile there are differences between hikers (as shown below), since the purpose of this work is to determine a mean impact on the land aggregated over many different hikers, there seems no sensible way to include this information in the hiking function applied.\nFor simplicity here, we form estimates based on all the data not differentiated by terrain.\n\nnames &lt;- gps_geol_purposive$name |&gt; unique()\nmodels &lt;- list()\nresults &lt;- list()\ni &lt;- 1\nfor (the_name in names) {\n  df &lt;- gps_geol_purposive |&gt; \n    filter(name == the_name)\n  m &lt;- df |&gt; get_gaussian_hiking_function()\n  models[[the_name]] &lt;- m\n  results[[i]] &lt;- m |&gt; \n    get_model_prediction_df(slopes) |&gt;\n    mutate(name = the_name)\n  i &lt;- i + 1\n}\nmodel_predictions &lt;- results |&gt; \n  bind_rows()\n\nggplot() +\n  geom_boxplot(data = gps_geol_purposive,\n               aes(x = slope_h_round, y = speed_km_h, group = slope_h_round),\n               outlier.size = 0.25, colour = \"darkgrey\") +\n  geom_line(data = model_predictions, aes(x = slope_h, y = y)) +\n  xlab(\"Slope\") + ylab(\"Speed, km/h\") +\n  facet_wrap( ~ name) +\n  theme_ipsum_rc()",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/09-refining-hiking-functions.html#superseded",
    "href": "notebooks/09-refining-hiking-functions.html#superseded",
    "title": "Refining hiking functions",
    "section": "SUPERSEDED",
    "text": "SUPERSEDED\n\nSmoothed speed-slope profiles\nIn earlier work, hiking function estimation was based on median speeds at a small number of slope intervals. This appears to be unnecessary but code is included below for completeness.\nIt is helpful to instead summarise the data at some chosen percentile of the distribution of recorded speeds within in slope interval bins. The following function does this and the result is illustrated.\n\n# helper function to reduce data to a more smoothed form\nget_percentile_summary &lt;- function(df, percentile, num_bins) {\n  df |&gt; mutate(slope_bin = cut_interval(slope_h, num_bins)) |&gt;\n    group_by(slope_bin) |&gt;\n    summarise(\n      slope_h = mean(slope_h),\n      speed_km_h = quantile(speed_km_h, percentile),\n      percentile = percentile) |&gt;\n    ungroup()\n}\n\nWe can plot these to get a feel for what might be a sensible percentile to apply.\n\nsummary_speed_by_slope_df &lt;- (1:19 / 20) |&gt;\n  as.list() |&gt;\n  lapply(get_percentile_summary, df = gps_geol_purposive, num_bins = 10) |&gt;\n  bind_rows()\n\nggplot() +\n  geom_point(data = gps_geol_purposive, aes(x = slope_h, y = speed_km_h), colour = \"black\", size = 0.25, alpha = 0.1) +\n  geom_line(data = summary_speed_by_slope_df, aes(x = slope_h, y = speed_km_h, colour = percentile, group = percentile)) +\n  scale_colour_distiller(palette = \"Spectral\") +\n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\nBased on this plot, the median (percentile = 0.5) choice seems the most reasonable, as it is less affected by occasional high outlier high speed movement.",
    "crumbs": [
      "Notebooks",
      "Refining hiking functions"
    ]
  },
  {
    "objectID": "notebooks/11-refining-hiking-network-construction.html",
    "href": "notebooks/11-refining-hiking-network-construction.html",
    "title": "Refining hiking network construction",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nInitial post.\nThis notebook supersedes some previous work because it handles ‘too steep’ (actually too slow) edges in the network by removing them, and also makes use of the recalculated estimated hiking functions.\nRemoval of too slow edges is based on an arbitrary threshold such that on a network of nominal resolution 100m (where edges are 107.5m long) if the traversal time is greater than half an hour it is considered impassable and removed from the network. There’s a chicken and egg problem where we have to build the network and add all the vertex and edge attributes before determining which edges to remove. Meanwhile we are unable to store the network reliably due to some issues with the igraph::write_graph functions (note that in general storing graphs is challenging with few reliably implemented and agreed standards—a better approach might be to save vertex and edge data frames. Something to look into).\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(hrbrthemes)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\nlibrary(ggspatial)\nlibrary(scales)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\ntheme_set(theme_ipsum_rc())",
    "crumbs": [
      "Notebooks",
      "Refining hiking network construction"
    ]
  },
  {
    "objectID": "notebooks/11-refining-hiking-network-construction.html#set-up-folders-and-some-base-parameters",
    "href": "notebooks/11-refining-hiking-network-construction.html#set-up-folders-and-some-base-parameters",
    "title": "Refining hiking network construction",
    "section": "Set up folders and some base parameters",
    "text": "Set up folders and some base parameters\nThe set up of data subfolders under the top level _data folder is largely as before but we now read in the geology (not a cover costs):\n\n\n\n\n\n\n\nFolder\nContents\n\n\n\n\ndry-valleys/output\nOutputs - primarily the graph and elevation data (as points) with relative movement costs\n\n\ncontiguous-geologies\nGPKG files dry-valleys-extent-??.gpkg with a single contiguous geology polygon per file\n\n\ndry-valleys/common-inputs\nVarious input layers as needed. Primarily relative landcover costs *-cover-costs.gpkg\n\n\ndem/10m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-10m-dem-clipped.tif\n\n\ndem/32m\nDEM datasets in raw and clipped (to the Dry Valleys and Skelton basins) and a mosaiced and clipped DEM file dry-valleys-combined-32m-dem-clipped.tif\n\n\n_data\nthe geology layer ata-scar-geomap-geology-v2022-08-clipped.gpkg\n\n\n\nAdditionally, the basename for all files is dry-valleys, with DEM resolution used (32m or 10m) and the nominal hiking network resolution (without the m), which is likely to be 100 included in various output files as appropriate.\n\n\nCode\ndem_resolution     &lt;- \"32m\" # 10m also available, but probably not critical\nresolution         &lt;- 100   # 100 is high res for rapid iterating: use 200 while testing\nimpassable_geology &lt;- \"ice\"\n\ndata_folder        &lt;- str_glue(\"{here()}/_data\")\nbase_folder        &lt;- str_glue(\"{data_folder}/dry-valleys\")\ninput_folder       &lt;- str_glue(\"{base_folder}/common-inputs\")\noutput_folder      &lt;- str_glue(\"{base_folder}/output\")\ndem_folder         &lt;- str_glue(\"{data_folder}/dem/{dem_resolution}\")\n\nbasename           &lt;- str_glue(\"dry-valleys\")\ndem_file           &lt;- str_glue(\"{dem_folder}/{basename}-combined-{dem_resolution}-dem-clipped.tif\")\ngeologies_file     &lt;- str_glue(\"{data_folder}/ata-scar-geomap-geology-v2022-08-clipped.gpkg\")",
    "crumbs": [
      "Notebooks",
      "Refining hiking network construction"
    ]
  },
  {
    "objectID": "notebooks/11-refining-hiking-network-construction.html#load-the-terrain-and-crop-to-the-extent",
    "href": "notebooks/11-refining-hiking-network-construction.html#load-the-terrain-and-crop-to-the-extent",
    "title": "Refining hiking network construction",
    "section": "Load the terrain and crop to the extent",
    "text": "Load the terrain and crop to the extent\nThe extent in each case is a single area of ‘contiguous geology’ derived by dissolving all polygons in the GNS geomap data. These files are named *01.gpkg, *02.gpkg, etc., where the sequence number is from the largest to the smallest by area.\n\n\nCode\ncontiguous_geology &lt;- \"05\"  # small one for testing\nextent_file        &lt;- str_glue(\"{base_folder}/contiguous-geologies/{basename}-extent-{contiguous_geology}.gpkg\")\nextent             &lt;- st_read(extent_file)\n\nterrain &lt;- rast(dem_file) |&gt;\n  crop(extent |&gt; as(\"SpatVector\")) |&gt;\n  mask(extent |&gt; as(\"SpatVector\"))\n# and for visualization\nshade &lt;- get_hillshade(terrain) |&gt;\n  as.data.frame(xy = TRUE) # this is for plotting in ggplot\n\n\nIt’s handy to make a basemap here\n\nhillshade_basemap &lt;- ggplot(shade) +\n  geom_raster(data = shade, aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  annotation_scale(plot_unit = \"m\", height = unit(0.1, \"cm\")) +\n  theme_void()\n\nMake a hex lattice of points or retrieve one made previously.\n\n\nCode\n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- resolution * sqrt(2 / sqrt(3))\n\n# check if we have already made this dataset\nhexgrid_file &lt;- \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-hex-grid-{resolution}.gpkg\")\n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    rename(geometry = x) |&gt;   # shouldsee https://github.com/r-spatial/sf/issues/2429\n    st_set_crs(st_crs(extent))\n  coords &lt;- xy |&gt; st_coordinates()\n  xy &lt;- xy |&gt;\n    mutate(x = coords[, 1], y = coords[, 2])\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\n\nReading layer `dry-valleys-05-32m-hex-grid-100' from data source \n  `/Users/david/Documents/work/mwlr-tpm-antarctica/antarctica/_data/dry-valleys/output/dry-valleys-05-32m-hex-grid-100.gpkg' \n  using driver `GPKG'\nSimple feature collection with 82530 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 424655.2 ymin: -1262612 xmax: 452755.2 ymax: -1233391\nProjected CRS: WGS 84 / Antarctic Polar Stereographic\n\n\nAttach elevation values to the points from the terrain data.\n\n\nCode\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n\npts &lt;- xy |&gt;\n  mutate(z = z[, 2])\n\n\nUp to here things are more or less unchanged. But now we attach geologies which are used to determine which variant of the estimated hiking function to apply.\n\n# remove any NAs that might result from interpolation or impassable terrain\n# NAs tend to occur around the edge of study area\ncover &lt;- st_read(geologies_file) |&gt;\n  dplyr::select(POLYGTYPE) |&gt; \n  rename(terrain = POLYGTYPE)\n\npts &lt;- pts |&gt;\n  st_join(cover) |&gt;\n  filter(!is.na(z), terrain != impassable_geology)\n\nNow we make the graph by connecting pairs of points within range of one another (note 1.1 factor to catch floating-point near misses).\nThis is similar to before but involves a post-graph construction step where too-costly edges are removed, and the corresponding points in the pts dataset are also removed.\nTODO: figure out how to store these sensibly.\n\ngraph_file &lt;-   \n  str_glue(\"{base_folder}/output/{basename}-{contiguous_geology}-{dem_resolution}-graph-hex-{resolution}.txt\") \n\n# note that the saved graph file does not store graph variables\n# and also has not been trimmed to remove too slow/steep edges...\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\nG &lt;- assign_movement_variables_to_graph_2(G, xyz = pts, terrain = pts$terrain)\nV(G)$id &lt;- 1:length(G)\n# now simplify by removing all edges that have est. costs &gt; 30 min for 100m res edges\n# scaled appropriately i.e. if resolution is coarser then that cutoff should be longer\n# and extracting the strong largest component (connected both directions)\ncost_cutoff &lt;- 0.5 * resolution / 100\nG &lt;- delete_edges(G, E(G)[which(E(G)$cost &gt; cost_cutoff)]) |&gt;\n  largest_component(mode = \"strong\")\n\nremaining_nodes &lt;- V(G)$id\npts &lt;- pts |&gt; slice(remaining_nodes)",
    "crumbs": [
      "Notebooks",
      "Refining hiking network construction"
    ]
  },
  {
    "objectID": "notebooks/11-refining-hiking-network-construction.html#now-we-have-a-graph",
    "href": "notebooks/11-refining-hiking-network-construction.html#now-we-have-a-graph",
    "title": "Refining hiking network construction",
    "section": "Now we have a graph",
    "text": "Now we have a graph\nSo let’s take a look… first, at a map of the network for reassurance - noting that since costs are different in each direction there’s no easy way to show this unambiguously. Some ‘holes’ in the network in steep areas are now apparent.\n\n\nCode\nhillshade_basemap +\n  geom_sf(data = G |&gt; get_graph_as_line_layer(), \n          aes(colour = cost), linewidth = 0.1) +\n  scale_colour_viridis_c(option = \"A\", direction = -1)\n\n\n\n\n\n\n\n\n\nAnd just for assurance, here’s an estimated travel time map.\n\n\nCode\n# pick a random point as origin point for some examples\norigin_i &lt;- sample(seq_along(pts$geom), 1)[1]\norigin &lt;- c(pts$x[origin_i], pts$y[origin_i])\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), \n                           weights = edge_attr(G, \"cost\")) |&gt; t()\ndf &lt;- vertex.attributes(G) |&gt; as.data.frame()\n\nhillshade_basemap +\n  geom_point(data = df, aes(x = x, y = y, colour = time_hrs), size = 0.2) +\n  scale_colour_viridis_c(option = \"A\", direction = -1) +\n  annotate(\"point\", x = origin[1], y = origin[2], pch = 4) +\n  coord_equal()",
    "crumbs": [
      "Notebooks",
      "Refining hiking network construction"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html",
    "href": "notebooks/05-exploring-hiking-network-options.html",
    "title": "Exploring hiking network options",
    "section": "",
    "text": "Update History\n\n\n\n\n\n\n\n\nDate\nChanges\n\n\n\n\n2024-09-26\nAdded an executive summary.\n\n\n2024-08-07\nInitial post.",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html#tldr-executive-summary",
    "href": "notebooks/05-exploring-hiking-network-options.html#tldr-executive-summary",
    "title": "Exploring hiking network options",
    "section": "TL;DR Executive summary",
    "text": "TL;DR Executive summary\nThe key findings are:\n\nEstimated travel time maps are easily produced, but only show travel time from a single origin. This might have use for a base camp location, but otherwise seems to have limited value.\nSome graph derivatives are interesting but unlikely to be especially useful, e.g. shortest path tree, minimum spanning tree.\nThe most promising approach is centrality measures on the graph. Among the options available the most promising class of measures are betweennees measures since these focus attention on graph edges and vertices most often traversed along the shortest routes among vertices.\nBoth vertex and edge betweenness are available options. Only the latter is explored here, but vertex betweenness is more readily visualized and in later notebooks the two approaches lead to essentially the same conclusions.\n\n\nPer the previous notebook we’ll make a small network (lower resolution) so we can explore the possibilities of this graph representation.\n\n\nCode\nlibrary(here)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(whitebox)\nlibrary(ggpubr)\n\nsource(str_glue(\"{here()}/scripts/raster-to-graph-functions.R\"))\n\nbase_folder &lt;-    str_glue(\"{here()}/_data/testing\")\ninput_folder &lt;-   str_glue(\"{base_folder}/input\")\noutput_folder &lt;-  str_glue(\"{base_folder}/output\")\nbasename &lt;-       \"dry-valleys-10m\"\ndem_file &lt;-       str_glue(\"{input_folder}/{basename}.tif\")\nimagery_file &lt;-   str_glue(\"{input_folder}/{basename}-imagery.tif\")\nlandcover_file &lt;- str_glue(\"{input_folder}/{basename}-cover-costs.gpkg\")\nextent_file &lt;-    str_glue(\"{input_folder}/{basename}-extent.gpkg\")\norigins_file &lt;-   str_glue(\"{input_folder}/{basename}-origins.gpkg\")\n\nterrain &lt;- rast(dem_file)\n# and for visualization\nshade &lt;- get_hillshade(terrain)\nimagery &lt;- rast(imagery_file)\nextent &lt;- st_read(extent_file)\n\n# make a focal area for convenience of plotting in some situations\ncentre_of_extent &lt;- extent |&gt; \n  st_centroid() # we use this later...\nc_vec &lt;- centre_of_extent |&gt;\n  st_coordinates() |&gt;\n  as.vector()\nfocal_area &lt;- ((extent$geom[1] - c_vec) * diag(0.025, 2, 2) + c_vec) |&gt;\n  st_sfc(crs = st_crs(extent)) |&gt;\n  st_as_sf() |&gt;\n  rename(geom = x)\n\nresolution &lt;- res(terrain)[1] * 20 \n# calculate hexagon spacing of equivalent area\nhex_cell_spacing &lt;- 2 * resolution / sqrt(2 * sqrt(3))\n\nhexgrid_file &lt;- str_glue(\"{input_folder}/{basename}-hex-grid-{resolution}.gpkg\")\ngraph_file &lt;-   str_glue(\"{output_folder}/{basename}-graph-hex-{resolution}.txt\") \n\nif (file.exists(hexgrid_file)) {\n  xy &lt;- st_read(hexgrid_file)\n} else {\n  xy  &lt;- extent |&gt;\n    st_make_grid(cellsize = hex_cell_spacing, what = \"centers\", square = FALSE) |&gt;\n    st_as_sf() |&gt;\n    st_set_crs(st_crs(extent))\n  xy |&gt;\n    st_write(str_glue(hexgrid_file), delete_dsn = TRUE)\n}\n\nz &lt;- terrain |&gt; \n  terra::extract(xy |&gt; as(\"SpatVector\"), method = \"bilinear\")\n# make pts and remove any NAs that might result from interpolation\n# NAs are around the edges of the study area\npts &lt;- xy |&gt;\n  mutate(z = z[, 2]) |&gt;\n  filter(!is.na(z))\n\nif (file.exists(graph_file)) {\n  G &lt;- read_graph(graph_file) \n} else {\n  G &lt;- graph_from_points(pts, hex_cell_spacing * 1.1)\n  write_graph(G, graph_file)\n}\n\ncover &lt;- st_read(landcover_file)\n\nxyz &lt;- extract_xyz_from_points(pts)\n\ncosts &lt;- pts |&gt;\n  st_join(cover, .predicate = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  select(cost)\n\n# note... for now this uses a default Tobler hiking function\nG &lt;- assign_movement_variables_to_graph(G, xyz, impedances = costs$cost)",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html#now-we-have-a-graph",
    "href": "notebooks/05-exploring-hiking-network-options.html#now-we-have-a-graph",
    "title": "Exploring hiking network options",
    "section": "Now we have a graph",
    "text": "Now we have a graph\nSo let’s take a look…\n\n# make a hillshade 'basemap'\nhillshade_basemap &lt;- ggplot() +\n  geom_raster(data = shade |&gt; as.data.frame(xy = TRUE), \n              aes(x = x, y = y, fill = hillshade), alpha = 0.5) + \n  scale_fill_distiller(palette = \"Greys\", direction = 1) +\n  guides(fill = \"none\") +\n  theme_void()\n\nhillshade_basemap +\n  geom_sf(data = G |&gt; get_graph_as_line_layer(), \n          aes(colour = cost), linewidth = 0.1) +\n  scale_colour_distiller(palette = \"Spectral\") +\n  theme_void()\n\n\n\n\n\n\n\n\nAnd just for assurance, here’s the estimated travel time map.\n\norigin_i &lt;- centre_of_extent |&gt;\n  st_nearest_feature(pts)\n\nV(G)$time_hrs &lt;- distances(G, v = c(V(G)[origin_i]), weights = edge_attr(G, \"cost\")) |&gt; t()\n\n# assemble results into a DF\ndf &lt;- data.frame(x = V(G)$x, y = V(G)$y, z = V(G)$z, \n                 time_hrs = V(G)$time_hrs, \n                 cost = costs$cost)\n# write this out to a shapefile, which Whitebox Tools needs\nshp_fname &lt;- str_glue(\"{here()}/_temp/{basename}-{format(origin_i, scientific = FALSE)}-hex.shp\")\ndf |&gt; st_as_sf(coords = c(\"x\", \"y\"), crs = crs(extent)) |&gt;\n  st_write(shp_fname, delete_dsn = TRUE)\n\n# do the interpolation using Sibley's natural neighbours since we have a regular\n# and relatively dense array and save it to a tif\ntif_fname &lt;- str_glue(\"{here()}/_temp/travel_time.tif\")\nwbt_natural_neighbour_interpolation(shp_fname, field = \"time_hrs\",\n                                    output = tif_fname, \n                                    base = str_glue(dem_file))\n\nReading the exported raster layer made by whitebox::wbt_natural_neighbour_interpolation back in we get a map of estimated travel times.\n\nrast(tif_fname) |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = travel_time)) +\n  scale_fill_distiller(palette = \"Spectral\", name = \"Est. time hrs\") +\n  geom_contour(aes(z = travel_time), linewidth = 0.5) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nIt may be worth noting that this map made with a 200m resolution network doesn’t differ greatly from the one at 50m resolution in the previous notebook.",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "notebooks/05-exploring-hiking-network-options.html#ok-so-what-can-we-do-with-these-things",
    "href": "notebooks/05-exploring-hiking-network-options.html#ok-so-what-can-we-do-with-these-things",
    "title": "Exploring hiking network options",
    "section": "OK… so what can we do with these things?!",
    "text": "OK… so what can we do with these things?!\nThe network is stored as an igraph object G. This admits many different network analysis methods.\n\nMinimum spanning tree\nThis is the shortest total path length (expressed here in the estimated traversal time variable cost) to reach all graph nodes.\n\nMST &lt;- G |&gt; \n  mst(weights = E(G)$cost) |&gt;\n  get_graph_as_line_layer() \n\nWe can see what this looks like\n\nhillshade_basemap + \n  geom_sf(data = MST, linewidth = 0.1)\n\n\n\n\n\n\n\n\nThe hillshade basemap highlights how the total shortest path prioritises paths that follow contours since the lowest cost way in general to get to a particular vertex is from a neighbouring vertex at similar elevation.\n\n\nShortest path tree\nStarting from a given vertex we can make a tree graph which shows all the shortest paths from that site to every other site in the vertex. This is not a built-in igraph function. A function in raster-to-graph-functions.R builds one for us.\n\nSPT &lt;- G |&gt;\n  get_shortest_path_tree(origin_i) |&gt;\n  get_graph_as_line_layer()\n\n\nhillshade_basemap + \n  geom_sf(data = SPT, linewidth = 0.1) +\n  geom_sf(data = pts |&gt; slice(origin_i), pch = 4)\n\n\n\n\n\n\n\n\nThis shows all the potential most efficient pathways out from the root vertex to every other vertex.\n\n\nEdge betweenness\nIt turns out we can count all the appearances of graph edges and/or vertices in all the shortest paths among all the vertices in a network. This are graph centrality measures termed edge betweenness and vertex betweenness (the latter is often simply betweenness).\n\n# scale relative to betweenness if all edges are equal cost to offset edge effects\nbase_eb &lt;- edge_betweenness(G)\ncost_eb &lt;- edge_betweenness(G, weights = E(G)$cost)\ndiff_eb &lt;- (cost_eb - base_eb) / (cost_eb + base_eb)\nrel_eb &lt;- cost_eb / base_eb\nlog_rel_eb &lt;- log(rel_eb, 10)\nGsp &lt;- G |&gt; \n  set_edge_attr(\"base_eb\", value = base_eb) |&gt;\n  set_edge_attr(\"cost_eb\", value = cost_eb) |&gt;\n  set_edge_attr(\"diff_eb\", value = diff_eb) |&gt;\n  set_edge_attr(\"rel_eb\", value = rel_eb) |&gt;\n  set_edge_attr(\"log_rel_eb\", value = log_rel_eb)\n\n# add 1 to these to make it easier to scale them if needed because a raw value 0 is possible\nbase_vb &lt;- betweenness(G) + 1\ncost_vb &lt;- betweenness(G, weights = E(G)$cost) + 1\ndiff_vb &lt;- (cost_vb - base_vb) / (cost_vb + base_vb)\nrel_vb &lt;- cost_vb / base_vb\nlog_rel_vb &lt;- log(rel_vb)\nGsp &lt;- Gsp |&gt; \n  set_vertex_attr(\"base_vb\", value = base_vb) |&gt;\n  set_vertex_attr(\"cost_vb\", value = cost_vb) |&gt;\n  set_vertex_attr(\"diff_vb\", value = diff_vb) |&gt;\n  set_vertex_attr(\"rel_vb\", value = rel_vb) |&gt;\n  set_vertex_attr(\"log_rel_vb\", value = log_rel_vb)\n\nMaps of these…\nFirst vertex centrality, where it is relatively simple to colour vertices by their centrality.\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer() |&gt; filter(log_rel_vb &gt; 0), \n          aes(colour = rel_vb), size = 0.5) +\n  scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\n\nEdge centrality is fiddlier to map (because of how the linewidth aesthetic works).\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_line_layer() |&gt; filter(log_rel_eb &gt; 0) |&gt; mutate(lwd = rel_eb / 100),\n          aes(linewidth = lwd)) +\n  scale_linewidth_identity(breaks = 0:5 * 50, labels = format(0:5 * 50, scientific = FALSE),\n                           guide = \"legend\", name = \"Edge betweenness\")\n\n\n\n\n\n\n\n\nFrom which we deduce… there is a ‘shortcut’ around the eastern (to the left…) end of the northernmost (bottom…) ridge which the shortest path function is finding.\nTODO: Check if this is an appropriate study area…\n\n\nRestricting the length of shortest paths\nIt is possible to restrict shortest paths to a maximum length in terms of the cost attribute used to calculate path lengths, which in our case is hours. Intuitively if the longest hike ‘segments’ undertaken are (say) 3 hours, then restricting shortest paths in this way might make sense. So…\n\nvb &lt;- betweenness(G, weights = E(G)$cost, cutoff = 3) + 1\nGsp &lt;- G |&gt;\n  set_vertex_attr(\"betweenness\", value = vb)\n\n\nhillshade_basemap +\n  geom_sf(data = Gsp |&gt; get_graph_as_point_layer(), aes(colour = betweenness), size = 0.5) +\n  scale_colour_distiller(palette = \"Spectral\", guide = \"none\")\n\n\n\n\n\n\n\n\nThis more local focus identifies more vertices as being on shortest paths",
    "crumbs": [
      "Notebooks",
      "Exploring hiking network options"
    ]
  },
  {
    "objectID": "paper/paper.html",
    "href": "paper/paper.html",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(cols4all)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(igraph)\nlibrary(sf)\nlibrary(akima)"
  },
  {
    "objectID": "paper/paper.html#discussion-and-conclusions",
    "href": "paper/paper.html#discussion-and-conclusions",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "5 Discussion and conclusions",
    "text": "5 Discussion and conclusions\n\n\n\nFigure 1: Example hiking functional forms: Exponential (Tobler 1993), Gaussian (Irmischer and Clarke 2018), Lorentz (Campbell et al. 2019), and Quadratic (Rees 2004).\nFigure 2: Possible graph lattices.\nFigure 3 (a): Study area location in Antarctica\nFigure 3 (b): Skelton and Dry Valleys basins\nFigure 3 (c): Study area elevation (hillshade) and surface geology\nFigure 3 (d): Five sub-regions of contiguous surface geology\nFigure 4 (a): Boxplots by slope of speed, with smoothed estimated hiking function showing a ‘dip’ due to over-representation of 0 slope fixes\nFigure 4 (b): After filtering the estimated hiking function no longer has a dip.\nFigure 5: Three possible hiking functions applied to GPS data split by land cover.\nFigure 6: The hiking functions for All, Moraine and Rock ground covers compared, including 95% confidence intervals derived by Monte-Carlo simulation.\nFigure 7 (a): Map of a hiking network coloured by estimated traversal times of edges\nFigure 7 (b): Red-outlined area zoomed in revealing edges removed from graph due to steep slopes\nFigure 7 (c): Shortest path tree of a hiking network from the indicated origin\nFigure 7 (d): Red-outlined area zoomed in view of the shortest path tree\nFigure 8: Relative vertex betweenness of vertices in the graph from Figure 7.\nFigure 9: Vertex betweenness maps based on different radius limits. As in Figure 8 the visualization colouring is based on linear scaling of raw betweenness scores."
  },
  {
    "objectID": "paper/paper.html#sec-discussion",
    "href": "paper/paper.html#sec-discussion",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "5 Discussion",
    "text": "5 Discussion\nTo cover:\n\nFigure of the largest study site?\nLimitations of the method:\n\nData concerns, esp. opportunistic nature of the GPS data\nWould higher resolution network be useful?\nPossible limitations of a regularly spaced lattice: extend to a lattice based on e.g., TIN?\nReliance of shortest paths as a guide to behaviour\nSome minor issues regarding the difference between directed and undirected graphs\n\nRelevance of radius limits on betweenness: the implicit model of movement in this approach (short horizons, no existing paths, ‘keep going for half an hour in this general direction…’)\nQuestion of embedding paths, i.e., locking in more damage to some parts of the landscape vs others"
  },
  {
    "objectID": "paper/paper.html#sec-conclusions",
    "href": "paper/paper.html#sec-conclusions",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "6 Conclusions",
    "text": "6 Conclusions\nTo cover:\n\nNovelty of fitting land cover specific hiking functions\nA method for exploring potential impacts in a novel environment\nA method for proposing less damaging plans for expeditions\nOther?"
  },
  {
    "objectID": "talk/index.html#overview",
    "href": "talk/index.html#overview",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Overview",
    "text": "Overview\nStudy area and data\nHiking networks\nSome outputs\nConclusions\n\n\nImage: Miers Valley, from commons.wikimedia.org"
  },
  {
    "objectID": "talk/index.html#data",
    "href": "talk/index.html#data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "talk/index.html#hiking-functions",
    "href": "talk/index.html#hiking-functions",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Hiking functions",
    "text": "Hiking functions"
  },
  {
    "objectID": "talk/index.html#hiking-networks",
    "href": "talk/index.html#hiking-networks",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Hiking networks",
    "text": "Hiking networks"
  },
  {
    "objectID": "talk/index.html#results",
    "href": "talk/index.html#results",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "talk/index.html#conclusion",
    "href": "talk/index.html#conclusion",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "talk/index.html#antarctica",
    "href": "talk/index.html#antarctica",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Antarctica",
    "text": "Antarctica"
  },
  {
    "objectID": "talk/index.html#gps-data",
    "href": "talk/index.html#gps-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "GPS data",
    "text": "GPS data\n\n\n\n\nCollected using QStarz Q1100P GPS Tracking Recorders between 2016 and 2018\n40+ hour battery life and simple use\nLocation recorded every 30 seconds\nScientists turned devices on when leaving base camp in the morning and off on return at the end of the day"
  },
  {
    "objectID": "talk/index.html#study-area",
    "href": "talk/index.html#study-area",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Study area",
    "text": "Study area"
  },
  {
    "objectID": "talk/index.html#up-neq-down",
    "href": "talk/index.html#up-neq-down",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Up \\(\\neq\\) down",
    "text": "Up \\(\\neq\\) down\nSlope is critical to movement, so attach heights to nodes and calculate slope in each direction along lattice edges\nThen apply a hiking function to estimate traversal times\n\n\nPeople heading uphill in the Klondike gold rush 1897 National Park Service, Klondike Gold Rush National Historical Park, Candy Waugaman Collection, KLGO Library SS-126-8831"
  },
  {
    "objectID": "talk/index.html#building-a-hiking-network",
    "href": "talk/index.html#building-a-hiking-network",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Building a hiking network",
    "text": "Building a hiking network\nStart with regular lattice across study area\nMany topologies could be used\n\nWe went with hexagonal at ~107m spacing"
  },
  {
    "objectID": "talk/index.html#derived-from-data",
    "href": "talk/index.html#derived-from-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived from data",
    "text": "Derived from data"
  },
  {
    "objectID": "talk/index.html#derived-from-raw-data",
    "href": "talk/index.html#derived-from-raw-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived from raw data",
    "text": "Derived from raw data"
  },
  {
    "objectID": "talk/index.html#derived-from-cleaned-data",
    "href": "talk/index.html#derived-from-cleaned-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived from cleaned data",
    "text": "Derived from cleaned data"
  },
  {
    "objectID": "talk/index.html#compared-with-a-combined-model",
    "href": "talk/index.html#compared-with-a-combined-model",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Compared with a combined model",
    "text": "Compared with a combined model"
  },
  {
    "objectID": "talk/index.html#derived-from-smoothed-data",
    "href": "talk/index.html#derived-from-smoothed-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived from smoothed data",
    "text": "Derived from smoothed data"
  },
  {
    "objectID": "talk/index.html#derived-by-smoothing-gps-data",
    "href": "talk/index.html#derived-by-smoothing-gps-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived by smoothing GPS data",
    "text": "Derived by smoothing GPS data"
  },
  {
    "objectID": "talk/index.html#derived-by-curve-fittingh-to-cleaned-data",
    "href": "talk/index.html#derived-by-curve-fittingh-to-cleaned-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Derived by curve fittingh to cleaned data",
    "text": "Derived by curve fittingh to cleaned data"
  },
  {
    "objectID": "talk/index.html#smoothing-gps-data",
    "href": "talk/index.html#smoothing-gps-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Smoothing GPS data",
    "text": "Smoothing GPS data"
  },
  {
    "objectID": "talk/index.html#curve-fittingh-to-cleaned-data",
    "href": "talk/index.html#curve-fittingh-to-cleaned-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Curve fittingh to cleaned data",
    "text": "Curve fittingh to cleaned data"
  },
  {
    "objectID": "talk/index.html#the-dry-valleys",
    "href": "talk/index.html#the-dry-valleys",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "The Dry Valleys",
    "text": "The Dry Valleys\nBing maps aerial imagery"
  },
  {
    "objectID": "talk/index.html#curve-fitting-to-cleaned-data",
    "href": "talk/index.html#curve-fitting-to-cleaned-data",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Curve fitting to cleaned data",
    "text": "Curve fitting to cleaned data"
  },
  {
    "objectID": "talk/index.html#and-on-the-ground",
    "href": "talk/index.html#and-on-the-ground",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "And on the ground",
    "text": "And on the ground\nYup! There’s streetview imagery"
  },
  {
    "objectID": "talk/index.html#terrain-dependent-vs-combined",
    "href": "talk/index.html#terrain-dependent-vs-combined",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Terrain-dependent vs combined",
    "text": "Terrain-dependent vs combined"
  },
  {
    "objectID": "talk/index.html#completing-and-applying-the-network",
    "href": "talk/index.html#completing-and-applying-the-network",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Completing and applying the network",
    "text": "Completing and applying the network\nAssign edges estimated traversal times based on slope, the hiking function, and land cover (moraine or rock)\nMake the nodes and edges into a directed graph\nThen use various graph algorithms to find, e.g., everywhere-to-everywhere shortest paths"
  },
  {
    "objectID": "talk/index.html#section-1",
    "href": "talk/index.html#section-1",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "",
    "text": "Collected using QStarz Q1100P GPS Tracking Recorders between 2016 and 2018\n40+ hour battery life and simple use\nLocation recorded every 30 seconds\nScientists turned devices on when leaving base camp in the morning and off on return at the end of the day"
  },
  {
    "objectID": "talk/index.html#betweenness-centrality",
    "href": "talk/index.html#betweenness-centrality",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Betweenness centrality",
    "text": "Betweenness centrality\nAfter some exploration, we landed on betweenness centrality as a useful metric\nCounts how often each node appears on shortest paths between every other pair of nodes\n \n→ Indicator of relative likelihood of locations being visited"
  },
  {
    "objectID": "talk/index.html#radius-limited-betweenness",
    "href": "talk/index.html#radius-limited-betweenness",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Radius-limited betweenness",
    "text": "Radius-limited betweenness\nRestrict betweenness centrality to nodes no more than some cost (i.e., time) apart\nMuch faster to calculate\nAlso… it may have more value: perhaps relevant to how people navigate in such environments"
  },
  {
    "objectID": "talk/index.html#planning-a-path-network",
    "href": "talk/index.html#planning-a-path-network",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Planning a path network?",
    "text": "Planning a path network?\nA way to reduce impact might be to plan paths\nExperimental at this stage\nBased on a minimum spanning tree approximation to an arborescence\n\n\nA minimum spanning tree is a subgraph with minimum total cost that reaches all the nodes in a network. An arborescence is the equivalent in a directed graph."
  },
  {
    "objectID": "talk/index.html#building-a-hiking-network-1",
    "href": "talk/index.html#building-a-hiking-network-1",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Building a hiking network",
    "text": "Building a hiking network\nStart with a regular lattice across the study area\nMany topologies could be used\n\nWe went with hexagonal at ~100m spacing"
  },
  {
    "objectID": "talk/index.html#shortest-paths-for-a-planned-expedition",
    "href": "talk/index.html#shortest-paths-for-a-planned-expedition",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Shortest paths for a planned expedition",
    "text": "Shortest paths for a planned expedition"
  },
  {
    "objectID": "talk/index.html#reduced-to-a-minimum-spanning-tree",
    "href": "talk/index.html#reduced-to-a-minimum-spanning-tree",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Reduced to a minimum spanning tree",
    "text": "Reduced to a minimum spanning tree"
  },
  {
    "objectID": "talk/index.html#section-7",
    "href": "talk/index.html#section-7",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "",
    "text": "Terrain differentiated fitted hiking functions are a novelty\nPotential wider application of radius-limited betweenness centrality?\nManuscript almost finished\nFor more: + On this project: dosull.github.io/antarctica + About me at Geospatial Stuff: dosull.github.io"
  },
  {
    "objectID": "talk/index.html#conclusion-1",
    "href": "talk/index.html#conclusion-1",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Conclusion",
    "text": "Conclusion\nTerrain differentiated fitted hiking functions are a novelty\nPotential wider application of radius-limited betweenness centrality?\nManuscript almost finished\nFor more: + On this project: dosull.github.io/antarctica + About me at Geospatial Stuff: dosull.github.io"
  },
  {
    "objectID": "talk/index.html#conclusions",
    "href": "talk/index.html#conclusions",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Conclusions",
    "text": "Conclusions\nTerrain differentiated fitted hiking functions are a novelty\nPotential wider application of radius-limited betweenness centrality?\nA manuscript almost finished\n \nFor more\n\nOn this project: dosull.github.io/antarctica\nAbout Geospatial Stuff: dosull.github.io"
  },
  {
    "objectID": "talk/index.html#for-more",
    "href": "talk/index.html#for-more",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "For more",
    "text": "For more\n\nOn this project: dosull.github.io/antarctica\nAbout Geospatial Stuff: dosull.github.io"
  },
  {
    "objectID": "talk/index.html#conclusions-1",
    "href": "talk/index.html#conclusions-1",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "Conclusions",
    "text": "Conclusions\n\nTerrain-differentiated data-fitted hiking functions are a novelty\nPotential wider application of radius-limited betweenness centrality?\nManuscript in preparation\nStart a conversation with Antarctic scientists about planning\n\n\n\nImage: Beacon Valley, from reddit.com/rHumanForScale"
  },
  {
    "objectID": "talk/index.html#gps-data-1",
    "href": "talk/index.html#gps-data-1",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "GPS data",
    "text": "GPS data\n\n\n\n\nCollected using QStarz Q1100P GPS Tracking Recorders between 2016 and 2018\n40+ hour battery life and simple use\nLocation recorded every 30 seconds\nScientists turned devices on when leaving base camp in the morning and off on return at the end of the day"
  },
  {
    "objectID": "talk/index.html#conclusionsstyletext-shadow0px-0px-8px-rgba102000.5-styletext-shadow0px-0px-4px-rgba102001-background-imagebeacon-valley.jpg",
    "href": "talk/index.html#conclusionsstyletext-shadow0px-0px-8px-rgba102000.5-styletext-shadow0px-0px-4px-rgba102001-background-imagebeacon-valley.jpg",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "[Conclusions]{style=“text-shadow:0px 0px 8px rgba(102,0,0,0.5); style=”text-shadow:0px 0px 4px rgba(102,0,0,1);““} {background-image=”beacon-valley.jpg”}",
    "text": "[Conclusions]{style=“text-shadow:0px 0px 8px rgba(102,0,0,0.5); style=”text-shadow:0px 0px 4px rgba(102,0,0,1);““} {background-image=”beacon-valley.jpg”}\nTerrain-differentiated data-fitted hiking functions are a novelty\nPotential wider application of radius-limited betweenness centrality?\n \nFor more\n\nOn this project: dosull.github.io/antarctica\nAbout Geospatial Stuff: dosull.github.io\n\n\n\nImage: Beacon Valley, from reddit.com/rHumanForScale"
  },
  {
    "objectID": "talk/index.html#section",
    "href": "talk/index.html#section",
    "title": "Modelling potential environmental impacts of science activity in Antarctica",
    "section": "",
    "text": "…"
  }
]